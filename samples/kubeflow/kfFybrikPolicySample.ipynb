{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Fybrik\n",
    "\n",
    "Training data set is synthetic bank transaction data: https://www.kaggle.com/ntnu-testimon/paysim1/data\n",
    "\n",
    "Notebook: https://www.kaggle.com/arjunjoshua/predicting-fraud-in-financial-payment-services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --user pandas seaborn sklearn pyarrow\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/.local/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ML dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance, to_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from Fybrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.flight as fl\n",
    "import json\n",
    "client = fl.connect(\"grpc://<arrow-flight-module-service>.<arrow-flight-module-ns>.svc.cluster.local:80\")\n",
    "request = {\n",
    "    \"asset\": \"<bucket-name>/<file-name>.csv\",\n",
    "    \"columns\": [\"step\", \"type\", \"amount\", \"nameOrig\", \"oldbalanceOrg\", \"newbalanceOrig\", \"nameDest\", \"oldbalanceDest\", \"newbalanceDest\", \"isFraud\", \"isFlaggedFraud\"]\n",
    "}\n",
    "info: fl.FlightInfo = client.get_flight_info(\n",
    "        fl.FlightDescriptor.for_command(json.dumps(request)))\n",
    "    \n",
    "result = client.do_get(info.endpoints[0].ticket)\n",
    "\n",
    "df: pd.DataFrame = result.read_pandas()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \\\n",
    "                     'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values in DataFrame\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine types of transactions that are fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n The types of fraudulent transactions are {}'.format(\\\n",
    "list(df.loc[df.isFraud == 1].type.drop_duplicates().values))) # only 'CASH_OUT' \n",
    "                                                             # & 'TRANSFER'\n",
    "\n",
    "dfFraudTransfer = df.loc[(df.isFraud == 1) & (df.type == 'TRANSFER')]\n",
    "dfFraudCashout = df.loc[(df.isFraud == 1) & (df.type == 'CASH_OUT')]\n",
    "\n",
    "print ('\\n The number of fraudulent TRANSFERs = {}'.\\\n",
    "       format(len(dfFraudTransfer))) # 4097\n",
    "\n",
    "print ('\\n The number of fraudulent CASH_OUTs = {}'.\\\n",
    "       format(len(dfFraudCashout))) # 4116\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine when isFlaggedFraud gets set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nThe type of transactions in which isFlaggedFraud is set: \\\n",
    "{}'.format(list(df.loc[df.isFlaggedFraud == 1].type.drop_duplicates()))) \n",
    "                                                            # only 'TRANSFER'\n",
    "\n",
    "dfTransfer = df.loc[df.type == 'TRANSFER']\n",
    "dfFlagged = df.loc[df.isFlaggedFraud == 1]\n",
    "dfNotFlagged = df.loc[df.isFlaggedFraud == 0]\n",
    "\n",
    "print('\\nMin amount transacted when isFlaggedFraud is set= {}'\\\n",
    "                                  .format(dfFlagged.amount.min())) # 353874.22\n",
    "\n",
    "print('\\nMax amount transacted in a TRANSFER where isFlaggedFraud is not set=\\\n",
    " {}'.format(dfTransfer.loc[dfTransfer.isFlaggedFraud == 0].amount.max())) # 9\n",
    "\n",
    "print('\\nThe number of TRANSFERs where isFlaggedFraud = 0, yet oldBalanceDest = 0 and\\\n",
    " newBalanceDest = 0: {}'.\\\n",
    "format(len(dfTransfer.loc[(dfTransfer.isFlaggedFraud == 0) & \\\n",
    "(dfTransfer.oldBalanceDest == 0) & (dfTransfer.newBalanceDest == 0)]))) # 4158\n",
    "\n",
    "print('\\nMin, Max of oldBalanceOrig for isFlaggedFraud = 1 TRANSFERs: {}'.\\\n",
    "format([round(dfFlagged.oldBalanceOrig.min()), round(dfFlagged.oldBalanceOrig.max())]))\n",
    "\n",
    "print('\\nMin, Max of oldBalanceOrig for isFlaggedFraud = 0 TRANSFERs where \\\n",
    "oldBalanceOrig = \\\n",
    "newBalanceOrig: {}'.format(\\\n",
    "[dfTransfer.loc[(dfTransfer.isFlaggedFraud == 0) & (dfTransfer.oldBalanceOrig \\\n",
    "== dfTransfer.newBalanceOrig)].oldBalanceOrig.min(), \\\n",
    "round(dfTransfer.loc[(dfTransfer.isFlaggedFraud == 0) & (dfTransfer.oldBalanceOrig \\\n",
    "               == dfTransfer.newBalanceOrig)].oldBalanceOrig.max())]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "Although isFraud is always set when isFlaggedFraud is set, since isFlaggedFraud is set just 16 times in a seemingly meaningless way, we can treat this feature as insignificant and discard it in the dataset without loosing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "From the exploratory data analysis, we know that fraud only occurs in 'TRANSFER's and 'CASH_OUT's. So we assemble only the corresponding data in X for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[(df.type == 'TRANSFER') | (df.type == 'CASH_OUT')]\n",
    "\n",
    "randomState = 5\n",
    "np.random.seed(randomState)\n",
    "\n",
    "#X = X.loc[np.random.choice(X.index, 100000, replace = False)]\n",
    "\n",
    "Y = X['isFraud']\n",
    "del X['isFraud']\n",
    "\n",
    "# Eliminate columns shown to be irrelevant for analysis in the EDA and columns with redacted data\n",
    "X = X.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)\n",
    "\n",
    "# Binary-encoding of labelled data in 'type'\n",
    "X.loc[X.type == 'TRANSFER', 'type'] = 0\n",
    "X.loc[X.type == 'CASH_OUT', 'type'] = 1\n",
    "X.type = X.type.astype(int) # convert dtype('O') to dtype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of Latent Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfraud = X.loc[Y == 1]\n",
    "XnonFraud = X.loc[Y == 0]\n",
    "print('\\nThe fraction of fraudulent transactions with \\'oldBalanceDest\\' = \\\n",
    "\\'newBalanceDest\\' = 0 although the transacted \\'amount\\' is non-zero is: {}'.\\\n",
    "format(len(Xfraud.loc[(Xfraud.oldBalanceDest == 0) & \\\n",
    "(Xfraud.newBalanceDest == 0) & (Xfraud.amount)]) / (1.0 * len(Xfraud))))\n",
    "\n",
    "print('\\nThe fraction of genuine transactions with \\'oldBalanceDest\\' = \\\n",
    "newBalanceDest\\' = 0 although the transacted \\'amount\\' is non-zero is: {}'.\\\n",
    "format(len(XnonFraud.loc[(XnonFraud.oldBalanceDest == 0) & \\\n",
    "(XnonFraud.newBalanceDest == 0) & (XnonFraud.amount)]) / (1.0 * len(XnonFraud))))\n",
    "\n",
    "#Since the destination account balances being zero is a strong indicator of fraud, we replace the value of 0 with -1\n",
    "X.loc[(X.oldBalanceDest == 0) & (X.newBalanceDest == 0) & (X.amount != 0), \\\n",
    "      ['oldBalanceDest', 'newBalanceDest']] = - 1\n",
    "\n",
    "X.loc[(X.oldBalanceOrig == 0) & (X.newBalanceOrig == 0) & (X.amount != 0), \\\n",
    "      ['oldBalanceOrig', 'newBalanceOrig']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Create 2 new features (columns) recording errors in the originating and destination accounts for each transaction. These new features turn out to be important in obtaining the best performance from the ML algorithm that we will finally use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['errorBalanceOrig'] = X.newBalanceOrig + X.amount - X.oldBalanceOrig\n",
    "X['errorBalanceDest'] = X.oldBalanceDest + X.amount - X.newBalanceDest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Visualize the differences between fraudulent and genuine transactions to confirm that an ML algorithm can make strong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = len(X)\n",
    "\n",
    "def plotStrip(x, y, hue, figsize = (14, 9)):\n",
    "    \n",
    "    fig = plt.figure(figsize = figsize)\n",
    "    colours = plt.cm.tab10(np.linspace(0, 1, 9))\n",
    "    with sns.axes_style('ticks'):\n",
    "        ax = sns.stripplot(x, y, \\\n",
    "             hue = hue, jitter = 0.4, marker = '.', \\\n",
    "             size = 4, palette = colours)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_xticklabels(['genuine', 'fraudulent'], size = 16)\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(2)\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        plt.legend(handles, ['Transfer', 'Cash out'], bbox_to_anchor=(1, 1), \\\n",
    "               loc=2, borderaxespad=0, fontsize = 16);\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Over Time \n",
    "This plot shows how the fraudulent and genuine transactions yield different figerprints when their dispersion is views over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plotStrip(Y[:limit], X.step[:limit], X.type[:limit])\n",
    "ax.set_ylabel('time [hour]', size = 16)\n",
    "ax.set_title('Striped vs. homogenous fingerprints of genuine and fraudulent \\\n",
    "transactions over time', size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Over Amount\n",
    "The new `errorBalanceDest` feature is more effective at making a distinction than the original `amount` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = len(X)\n",
    "ax = plotStrip(Y[:limit], X.amount[:limit], X.type[:limit], figsize = (14, 9))\n",
    "ax.set_ylabel('amount', size = 16)\n",
    "ax.set_title('Same-signed fingerprints of genuine \\\n",
    "and fraudulent transactions over amount', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Over Error in Balance in Destination Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = len(X)\n",
    "ax = plotStrip(Y[:limit], - X.errorBalanceDest[:limit], X.type[:limit], \\\n",
    "              figsize = (14, 9))\n",
    "ax.set_ylabel('- errorBalanceDest', size = 16)\n",
    "ax.set_title('Opposite polarity fingerprints over the error in \\\n",
    "destination account balances', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Out Genuine From Fraudulent Transactions\n",
    "The 3D plot below distinguishes best between fraud and non-fraud data by using both of the engineered error-based features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long computation in this cell (~2.5 minutes)\n",
    "x = 'errorBalanceDest'\n",
    "y = 'step'\n",
    "z = 'errorBalanceOrig'\n",
    "zOffset = 0.02\n",
    "limit = len(X)\n",
    "\n",
    "sns.reset_orig() # prevent seaborn from over-riding mplot3d defaults\n",
    "\n",
    "fig = plt.figure(figsize = (10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X.loc[Y == 0, x][:limit], X.loc[Y == 0, y][:limit], \\\n",
    "  -np.log10(X.loc[Y == 0, z][:limit] + zOffset), c = 'g', marker = '.', \\\n",
    "  s = 1, label = 'genuine')\n",
    "    \n",
    "ax.scatter(X.loc[Y == 1, x][:limit], X.loc[Y == 1, y][:limit], \\\n",
    "  -np.log10(X.loc[Y == 1, z][:limit] + zOffset), c = 'r', marker = '.', \\\n",
    "  s = 1, label = 'fraudulent')\n",
    "\n",
    "ax.set_xlabel(x, size = 16); \n",
    "ax.set_ylabel(y + ' [hour]', size = 16); \n",
    "ax.set_zlabel('- log$_{10}$ (' + z + ')', size = 16)\n",
    "ax.set_title('Error-based features separate out genuine and fraudulent \\\n",
    "transactions', size = 20)\n",
    "\n",
    "plt.axis('tight')\n",
    "ax.grid(1)\n",
    "\n",
    "noFraudMarker = mlines.Line2D([], [], linewidth = 0, color='g', marker='.',\n",
    "                          markersize = 10, label='genuine')\n",
    "fraudMarker = mlines.Line2D([], [], linewidth = 0, color='r', marker='.',\n",
    "                          markersize = 10, label='fraudulent')\n",
    "\n",
    "plt.legend(handles = [noFraudMarker, fraudMarker], \\\n",
    "           bbox_to_anchor = (1.20, 0.38 ), frameon = False, prop={'size': 16});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fingerprints of Genuine and Fraudulent Transactions\n",
    "Smoking gun and comprehensive evidence embedded in the dataset of the difference between fraudulent and genuine transactions is obtained by examining their respective correlations in the heatmaps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfraud = X.loc[Y == 1] # update Xfraud & XnonFraud with cleaned data\n",
    "XnonFraud = X.loc[Y == 0]\n",
    "                  \n",
    "correlationNonFraud = XnonFraud.loc[:, X.columns != 'step'].corr()\n",
    "mask = np.zeros_like(correlationNonFraud)\n",
    "indices = np.triu_indices_from(correlationNonFraud)\n",
    "mask[indices] = True\n",
    "\n",
    "grid_kws = {\"width_ratios\": (.9, .9, .05), \"wspace\": 0.2}\n",
    "f, (ax1, ax2, cbar_ax) = plt.subplots(1, 3, gridspec_kw=grid_kws, \\\n",
    "                                     figsize = (14, 9))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 8, as_cmap=True)\n",
    "ax1 =sns.heatmap(correlationNonFraud, ax = ax1, vmin = -1, vmax = 1, \\\n",
    "    cmap = cmap, square = False, linewidths = 0.5, mask = mask, cbar = False)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), size = 16); \n",
    "ax1.set_yticklabels(ax1.get_yticklabels(), size = 16); \n",
    "ax1.set_title('Genuine \\n transactions', size = 20)\n",
    "\n",
    "correlationFraud = Xfraud.loc[:, X.columns != 'step'].corr()\n",
    "ax2 = sns.heatmap(correlationFraud, vmin = -1, vmax = 1, cmap = cmap, \\\n",
    " ax = ax2, square = False, linewidths = 0.5, mask = mask, yticklabels = False, \\\n",
    "    cbar_ax = cbar_ax, cbar_kws={'orientation': 'vertical', \\\n",
    "                                 'ticks': [-1, -0.5, 0, 0.5, 1]})\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), size = 16); \n",
    "ax2.set_title('Fraudulent \\n transactions', size = 20);\n",
    "\n",
    "cbar_ax.set_yticklabels(cbar_ax.get_yticklabels(), size = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning to Detect Fraud in Skewed Data\n",
    "Having obtained evidence from the plots above that the data now contains features that make fraudulent transactions clearly detectable, the remaining obstacle for training a robust ML model is the highly imbalanced nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('skew = {}'.format( len(Xfraud) / float(len(X)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets in a 80:20 ratio\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.2, \\\n",
    "                                                random_state = randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long computation in this cell (~1.8 minutes)\n",
    "weights = (Y == 0).sum() / (1.0 * (Y == 1).sum())\n",
    "clf = XGBClassifier(max_depth = 3, scale_pos_weight = weights, \\\n",
    "                n_jobs = 4)\n",
    "probabilities = clf.fit(trainX, trainY).predict_proba(testX)\n",
    "print('AUPRC = {}'.format(average_precision_score(testY, \\\n",
    "                                              probabilities[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features for the ML Model\n",
    "The figure below shows that the new feature errorBalanceOrig that we created is the most relevant feature for the model. The features are ordered based on the number of samples affected by splits on those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "colours = plt.cm.Set1(np.linspace(0, 1, 9))\n",
    "\n",
    "ax = plot_importance(clf, height = 1, color = colours, grid = False, \\\n",
    "                     show_values = False, importance_type = 'cover', ax = ax);\n",
    "for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(2)\n",
    "        \n",
    "ax.set_xlabel('importance score', size = 16);\n",
    "ax.set_ylabel('features', size = 16);\n",
    "ax.set_yticklabels(ax.get_yticklabels(), size = 12);\n",
    "ax.set_title('Ordering of features by importance to the model learnt', size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of ML Model\n",
    "The root node in the decision tree visualized below is indeed the feature errorBalanceOrig, as would be expected from its high significance to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_graphviz(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "The model we have learnt has a degree of bias and is slighly underfit. This is indicated by the levelling in AUPRC as the size of the training set is increased in the cross-validation curve below. The easiest way to improve the performance of the model still further is to increase the max_depth parameter of the XGBClassifier at the expense of the longer time spent learning the model. Other parameters of the classifier that can be adjusted to correct for the effect of the modest underfitting include decreasing min_child_weight and decreasing reg_lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long computation in this cell (~6 minutes)\n",
    "\n",
    "trainSizes, trainScores, crossValScores = learning_curve(\\\n",
    "XGBClassifier(max_depth = 3, scale_pos_weight = weights, n_jobs = 4), trainX,\\\n",
    "                                         trainY, scoring = 'average_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScoresMean = np.mean(trainScores, axis=1)\n",
    "trainScoresStd = np.std(trainScores, axis=1)\n",
    "crossValScoresMean = np.mean(crossValScores, axis=1)\n",
    "crossValScoresStd = np.std(crossValScores, axis=1)\n",
    "\n",
    "colours = plt.cm.tab10(np.linspace(0, 1, 9))\n",
    "\n",
    "fig = plt.figure(figsize = (14, 9))\n",
    "plt.fill_between(trainSizes, trainScoresMean - trainScoresStd,\n",
    "    trainScoresMean + trainScoresStd, alpha=0.1, color=colours[0])\n",
    "plt.fill_between(trainSizes, crossValScoresMean - crossValScoresStd,\n",
    "    crossValScoresMean + crossValScoresStd, alpha=0.1, color=colours[1])\n",
    "plt.plot(trainSizes, trainScores.mean(axis = 1), 'o-', label = 'train', \\\n",
    "         color = colours[0])\n",
    "plt.plot(trainSizes, crossValScores.mean(axis = 1), 'o-', label = 'cross-val', \\\n",
    "         color = colours[1])\n",
    "\n",
    "ax = plt.gca()\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth(2)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles, ['train', 'cross-val'], bbox_to_anchor=(0.8, 0.15), \\\n",
    "               loc=2, borderaxespad=0, fontsize = 16);\n",
    "plt.xlabel('training set size', size = 16); \n",
    "plt.ylabel('AUPRC', size = 16)\n",
    "plt.title('Learning curves indicate slightly underfit model', size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We thoroughly interrogated the data at the outset to gain insight into which features could be discarded and those which could be valuably engineered. The plots provided visual confirmation that the data could be indeed be discriminated with the aid of the new features. To deal with the large skew in the data, we chose an appropriate metric and used an ML algorithm based on an ensemble of decision trees which works best with strongly imbalanced classes. The method used in this kernel should therefore be broadly applicable to a range of such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
