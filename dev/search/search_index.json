{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":"python","separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"","title":"About"},{"location":"concepts/architecture.html","text":"Architecture Mesh for Data takes a modular approach to provide an open platform for controlling and securing the use of data across an organization. The figure below showcases the current architecture of the Mesh for Data platform, running on top of Kubernetes. The storage systems shown in the lower half of the figure are merely an example. The core parts of Mesh for Data are based on Kubernetes operators and custom resource definitions in order to define its work items and reconcile the state of them. The primary interaction object for a data user is the M4DApplication CRD where a user defines which data should be used for which purpose. The following chart and description describe the architecture and components of Mesh for Data relative to when they are used. Before the data user can perform any actions a data operator has to install the Mesh for Data and modules. Modules are an extensible mechanism for processing data. Modules can provide read/write access or produce implicit copies that serve as lower latency caches of remote assets. Modules also enforce policies defined for data assets. Mesh for Data connects to external services to receive policy engine decisions, available data and credentials. Policies, assets and access credentials to the assets have to be defined before the user can run an application. The current abstraction supports 2 different connectors : one for data catalog and one for policy manager. It's designed in an open way so that multiple different catalog and policy frameworks of all kinds of cloud and on-prem systems can be supported. The data steward configures policies in an external policy manager over assets defined in an external data catalog. Dataset credentials are retrieved from Vault by using Vault API . Vault uses a custom secret engine implemented with HashiCorp Vault plugins system to retrieve the credentials from where they are stored (data catalog for example). Once a developer submits an M4DApplication CRD to Kubernetes the ApplicationController will make sure that all the specs are fulfilled and will make sure that the data is made accessible to the user according to the previously defined policies. The M4DApplication holds metadata about the application such the data assets required by the application, the processing purpose and the method of access the user wishes (protocol e.g. S3 or Arrow flight). It uses this information to check with external systems (4) if access or copy is allowed and whether restrictive policies such as masking or hashing have to be applied. It compiles blueprints based on on the policy decisions received via the connectors and chooses the modules (5) which are best fit for the requirements that the user specified regarding the access protocol and availability. As data assets may reside in different systems the blueprints are compiled in a Plotter CRD (6) that specifies which blueprints have to be executed in which cluster. Depending on the setup the PlotterController will use various methods to distribute the blueprints. In a multi cluster setup the default distribution implementation is using Razee to control remote blueprints, but several multi-cloud tools could be used as a replacement. The PlotterController also collects statuses and distributes updates of said blueprints. Once all the blueprints on all clusters are ready the plotter is marked as ready. A single blueprint contains the specification of all assets that shall be accessed in a single cluster by a single application. The BlueprintController makes sure that a blueprint can deploy all needed modules (8) and (9) and tracks their status (10). Once e.g. an implicit-copy module finishes the copy the blueprint is also in a ready state. A read or write module is in ready state as soon as the proxy service such as the arrow-flight module is running. In this example an implicit-copy module copies data from a remote postgres database into a S3 compatible ceph instance. The arrow-flight module then locally serves the data to the user via the Arrow flight protocol. Credentials are handled by the modules (11) and are never exposed to the user. The application reads from and writes data to allowed targets. Requests are handled by M4DModule instances(12). The application can not interact with unauthorized targets.","title":"Architecture"},{"location":"concepts/architecture.html#architecture","text":"Mesh for Data takes a modular approach to provide an open platform for controlling and securing the use of data across an organization. The figure below showcases the current architecture of the Mesh for Data platform, running on top of Kubernetes. The storage systems shown in the lower half of the figure are merely an example. The core parts of Mesh for Data are based on Kubernetes operators and custom resource definitions in order to define its work items and reconcile the state of them. The primary interaction object for a data user is the M4DApplication CRD where a user defines which data should be used for which purpose. The following chart and description describe the architecture and components of Mesh for Data relative to when they are used. Before the data user can perform any actions a data operator has to install the Mesh for Data and modules. Modules are an extensible mechanism for processing data. Modules can provide read/write access or produce implicit copies that serve as lower latency caches of remote assets. Modules also enforce policies defined for data assets. Mesh for Data connects to external services to receive policy engine decisions, available data and credentials. Policies, assets and access credentials to the assets have to be defined before the user can run an application. The current abstraction supports 2 different connectors : one for data catalog and one for policy manager. It's designed in an open way so that multiple different catalog and policy frameworks of all kinds of cloud and on-prem systems can be supported. The data steward configures policies in an external policy manager over assets defined in an external data catalog. Dataset credentials are retrieved from Vault by using Vault API . Vault uses a custom secret engine implemented with HashiCorp Vault plugins system to retrieve the credentials from where they are stored (data catalog for example). Once a developer submits an M4DApplication CRD to Kubernetes the ApplicationController will make sure that all the specs are fulfilled and will make sure that the data is made accessible to the user according to the previously defined policies. The M4DApplication holds metadata about the application such the data assets required by the application, the processing purpose and the method of access the user wishes (protocol e.g. S3 or Arrow flight). It uses this information to check with external systems (4) if access or copy is allowed and whether restrictive policies such as masking or hashing have to be applied. It compiles blueprints based on on the policy decisions received via the connectors and chooses the modules (5) which are best fit for the requirements that the user specified regarding the access protocol and availability. As data assets may reside in different systems the blueprints are compiled in a Plotter CRD (6) that specifies which blueprints have to be executed in which cluster. Depending on the setup the PlotterController will use various methods to distribute the blueprints. In a multi cluster setup the default distribution implementation is using Razee to control remote blueprints, but several multi-cloud tools could be used as a replacement. The PlotterController also collects statuses and distributes updates of said blueprints. Once all the blueprints on all clusters are ready the plotter is marked as ready. A single blueprint contains the specification of all assets that shall be accessed in a single cluster by a single application. The BlueprintController makes sure that a blueprint can deploy all needed modules (8) and (9) and tracks their status (10). Once e.g. an implicit-copy module finishes the copy the blueprint is also in a ready state. A read or write module is in ready state as soon as the proxy service such as the arrow-flight module is running. In this example an implicit-copy module copies data from a remote postgres database into a S3 compatible ceph instance. The arrow-flight module then locally serves the data to the user via the Arrow flight protocol. Credentials are handled by the modules (11) and are never exposed to the user. The application reads from and writes data to allowed targets. Requests are handled by M4DModule instances(12). The application can not interact with unauthorized targets.","title":"Architecture"},{"location":"concepts/connectors.html","text":"Connectors The project currently has two extension mechanisms, namely connectors and modules. This page describes what connectors are and what connectors are installed using the default Mesh for Data installation. What are connectors? Connectors are GRPC services that the Mesh for Data control plane uses to connect to external systems. Specifically, the control plane needs connectors to data catalog and policy manager. These connector GRPC services are deployed alongside the control plane. Can I write my own connectors? Yes. Mesh for Data provides some default connectors described in this page but anyone can develop their own connectors. A connector needs to implement one or more of the GRPC interfaces described in the API documentation , depending on the connector type. Note that a single Kubernetes service can implement all GRPC interfaces if the system it connects to supports the required functionality, but it can also be different services. In addition, to benefit from the control plane security feature ensure that the Pods of your connector: 1. Have a m4d.ibm.com/componentType: connector label 1. Have a sidecar.istio.io/inject: \"true\" annotation Connector types Data catalog Mesh for Data assumes the use of an enterprise data catalog. For example, to reference a required data asset in a M4DApplication resource, you provide a link to the asset in the catalog. The catalog provides metadata about the asset such as security tags. It also provides connection information to describe how to connect to the data source to consume the data. Mesh for Data uses the metadata provided by the catalog both to enable seamless connectivity to the data and as input to making policy decisions. The data user is not concerned with any of it and just selects the data that it needs regardless of where the data resides. Mesh for Data is not a data catalog. Instead, it links to existing data catalogs using connectors. The default installation of Mesh for Data installs Katalog , a built-in data catalog using Kubernetes CRDs used for evaluation. A connector to ODPi Egeria is also available. Policy manager Enforcing data governance policies requires a Policy Decision Point (PDP) that dictates what enforcement actions need to take place. Mesh for Data supports a wide and extendible set of enforcement actions to perform on data read, write or copy. These include transformation of data, verification of the data, and various restrictions on the external activity of an application that can acceess the data. A PDP returns a list of enforcement actions given a set of policies and specific context about the application and the data it uses. Mesh for Data includes a PDP that is powered by Open Policy Agent (OPA). However, the PDP can also use external policy managers via connectors, to cover some or even all policy types. Policies are therefore defined externally in the policy manager of choice. Mesh for Data provides a package to help writing data policies in OPA. Otherwise, data stewards are expected to keep using the policy manager that they already use, as long as there is a connector to it.","title":"Connectors"},{"location":"concepts/connectors.html#connectors","text":"The project currently has two extension mechanisms, namely connectors and modules. This page describes what connectors are and what connectors are installed using the default Mesh for Data installation.","title":"Connectors"},{"location":"concepts/connectors.html#what-are-connectors","text":"Connectors are GRPC services that the Mesh for Data control plane uses to connect to external systems. Specifically, the control plane needs connectors to data catalog and policy manager. These connector GRPC services are deployed alongside the control plane.","title":"What are connectors?"},{"location":"concepts/connectors.html#can-i-write-my-own-connectors","text":"Yes. Mesh for Data provides some default connectors described in this page but anyone can develop their own connectors. A connector needs to implement one or more of the GRPC interfaces described in the API documentation , depending on the connector type. Note that a single Kubernetes service can implement all GRPC interfaces if the system it connects to supports the required functionality, but it can also be different services. In addition, to benefit from the control plane security feature ensure that the Pods of your connector: 1. Have a m4d.ibm.com/componentType: connector label 1. Have a sidecar.istio.io/inject: \"true\" annotation","title":"Can I write my own connectors?"},{"location":"concepts/connectors.html#connector-types","text":"","title":"Connector types"},{"location":"concepts/connectors.html#data-catalog","text":"Mesh for Data assumes the use of an enterprise data catalog. For example, to reference a required data asset in a M4DApplication resource, you provide a link to the asset in the catalog. The catalog provides metadata about the asset such as security tags. It also provides connection information to describe how to connect to the data source to consume the data. Mesh for Data uses the metadata provided by the catalog both to enable seamless connectivity to the data and as input to making policy decisions. The data user is not concerned with any of it and just selects the data that it needs regardless of where the data resides. Mesh for Data is not a data catalog. Instead, it links to existing data catalogs using connectors. The default installation of Mesh for Data installs Katalog , a built-in data catalog using Kubernetes CRDs used for evaluation. A connector to ODPi Egeria is also available.","title":"Data catalog"},{"location":"concepts/connectors.html#policy-manager","text":"Enforcing data governance policies requires a Policy Decision Point (PDP) that dictates what enforcement actions need to take place. Mesh for Data supports a wide and extendible set of enforcement actions to perform on data read, write or copy. These include transformation of data, verification of the data, and various restrictions on the external activity of an application that can acceess the data. A PDP returns a list of enforcement actions given a set of policies and specific context about the application and the data it uses. Mesh for Data includes a PDP that is powered by Open Policy Agent (OPA). However, the PDP can also use external policy managers via connectors, to cover some or even all policy types. Policies are therefore defined externally in the policy manager of choice. Mesh for Data provides a package to help writing data policies in OPA. Otherwise, data stewards are expected to keep using the policy manager that they already use, as long as there is a connector to it.","title":"Policy manager"},{"location":"concepts/introduction.html","text":"Introduction Mesh for Data is a cloud native platform to unify data access and governance, enabling business agility while securing enterprise data. By providing access and use of data only via the platform, Mesh for Data brings together access and governance for data, greatly reducing risk of data loss. Mesh for Data allows: Data users to use data in a self-service model without manual processes and without dealing with credentials. Use common tools and frameworks for reading from and exporting data to data lakes or data warehouses. Data stewards to control data access and data usage by applications. Use the organization's policy manager and data catalog of choice and let Mesh for Data enforce data usage policies even after data is accessed. Data operators to automate data lifecycle mangement via implicit data copies, eliminating the need for manual versioning and copying of data. How does it work? The inputs to Mesh for Data are declarative definitions with separation of aspects: Data stewards input definitions related to data governance and security Data users input definitions related to data usage in the business logic of their applications Data operators input definitions related to infrastructure and available resources Upon creation or change of any definition, Mesh for Data compiles together relevant inputs into blueprints of the data path (per application). The blueprint augments the application workload and data sources with additional services and functions packed as pluggable modules. This creates a data path that: Integrates business logic with non-functional data centric requirements such as enabling data access regardless of its physical location, caching, lineage tracking, etc. Enforce governance on the usage of data; including limiting what data the business logic can access, performing transformations as needed, controlling what the business logic can export and where to Makes data available in locations where it is needed. Thus in a multi cluster scenario it may copy data from one location to another Mesh for Data is an open solution that can be extended to work with a wide range of tools and data stores. For example, the injectable modules and the connectors to external systems (e.g., to a data catalog) can all be third party. Applications Mesh for Data considers applications as first level entities. Before running a workload, an application needs to be registred to a Mesh for Data control plane by applying a M4DApplication resource. This is the declarative definition provided by the data user. The registration provides context about the application such as the purpose for which it's running, the data assets that it needs, and a selector to identify the workload. Additional context such as geo-location is extracted from the platform. The actions taken by Mesh for Data are based on policies and the context of the application. Specifically, Mesh for Data does not consider end-users of an application. It is the responsibility of the application to implement mechanisms such as end user authentication if required, e.g. using Istio authorization with JWT . Security While the Mesh for Data handles enforcement of data governance policies, if one could access the data not through the platform then we lose control over data usage. For this reason, Mesh for Data does not let user applications ever observe data access credentials, both for externally created data assets and for data assets created by the Mesh for Data control plane and applications running in it. Instead, modules run in the data path to handle access to data, including passing the data access credentials to upstream data stores. Security is preserved by authorizing the applications based on their Pod identities. Multicluster Mesh for Data supports data paths that access data stores that are external to the cluster such as cloud managed object stores or databases as well as data stores within the cluster such as databases running in Kubernetes. All applications and modules however will run within a cluster that has Mesh for Data installed. Multi-cloud and hybrid cloud scenarios are supported out of the box by running Mesh for Data in multiple Kubernetes clusters and configuring the manager to use a multi cluster coordination mechanism such as razee. This enables cases such as running transformations on-prem while creating an implicit copy of an on-prem SoR table to a public cloud storage system.","title":"Introduction"},{"location":"concepts/introduction.html#introduction","text":"Mesh for Data is a cloud native platform to unify data access and governance, enabling business agility while securing enterprise data. By providing access and use of data only via the platform, Mesh for Data brings together access and governance for data, greatly reducing risk of data loss. Mesh for Data allows: Data users to use data in a self-service model without manual processes and without dealing with credentials. Use common tools and frameworks for reading from and exporting data to data lakes or data warehouses. Data stewards to control data access and data usage by applications. Use the organization's policy manager and data catalog of choice and let Mesh for Data enforce data usage policies even after data is accessed. Data operators to automate data lifecycle mangement via implicit data copies, eliminating the need for manual versioning and copying of data.","title":"Introduction"},{"location":"concepts/introduction.html#how-does-it-work","text":"The inputs to Mesh for Data are declarative definitions with separation of aspects: Data stewards input definitions related to data governance and security Data users input definitions related to data usage in the business logic of their applications Data operators input definitions related to infrastructure and available resources Upon creation or change of any definition, Mesh for Data compiles together relevant inputs into blueprints of the data path (per application). The blueprint augments the application workload and data sources with additional services and functions packed as pluggable modules. This creates a data path that: Integrates business logic with non-functional data centric requirements such as enabling data access regardless of its physical location, caching, lineage tracking, etc. Enforce governance on the usage of data; including limiting what data the business logic can access, performing transformations as needed, controlling what the business logic can export and where to Makes data available in locations where it is needed. Thus in a multi cluster scenario it may copy data from one location to another Mesh for Data is an open solution that can be extended to work with a wide range of tools and data stores. For example, the injectable modules and the connectors to external systems (e.g., to a data catalog) can all be third party.","title":"How does it work?"},{"location":"concepts/introduction.html#applications","text":"Mesh for Data considers applications as first level entities. Before running a workload, an application needs to be registred to a Mesh for Data control plane by applying a M4DApplication resource. This is the declarative definition provided by the data user. The registration provides context about the application such as the purpose for which it's running, the data assets that it needs, and a selector to identify the workload. Additional context such as geo-location is extracted from the platform. The actions taken by Mesh for Data are based on policies and the context of the application. Specifically, Mesh for Data does not consider end-users of an application. It is the responsibility of the application to implement mechanisms such as end user authentication if required, e.g. using Istio authorization with JWT .","title":"Applications"},{"location":"concepts/introduction.html#security","text":"While the Mesh for Data handles enforcement of data governance policies, if one could access the data not through the platform then we lose control over data usage. For this reason, Mesh for Data does not let user applications ever observe data access credentials, both for externally created data assets and for data assets created by the Mesh for Data control plane and applications running in it. Instead, modules run in the data path to handle access to data, including passing the data access credentials to upstream data stores. Security is preserved by authorizing the applications based on their Pod identities.","title":"Security"},{"location":"concepts/introduction.html#multicluster","text":"Mesh for Data supports data paths that access data stores that are external to the cluster such as cloud managed object stores or databases as well as data stores within the cluster such as databases running in Kubernetes. All applications and modules however will run within a cluster that has Mesh for Data installed. Multi-cloud and hybrid cloud scenarios are supported out of the box by running Mesh for Data in multiple Kubernetes clusters and configuring the manager to use a multi cluster coordination mechanism such as razee. This enables cases such as running transformations on-prem while creating an implicit copy of an on-prem SoR table to a public cloud storage system.","title":"Multicluster"},{"location":"concepts/modules.html","text":"Modules The project currently has two extension mechanisms, namely connectors and modules. This page describes what modules are and how they are leveraged by the control plane to build the data plane flow. What are modules? As described in the architecture page, the control plane generates a description of a data plane based on policies and application requirements. This is known as a blueprint, and includes components that are deployed by the control plane to fulfill different data-centric requirements. For example, a component that can mask data can be used to enforce a data masking policy, or a component that copies data may be used to create a local data copy to meet performance requirements, etc. Modules are the way to describe such data plane components and make them available to the control plane. A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To make a module available to the control plane it must be registered by applying a M4DModule CRD. The functionality described by the module may be deployed (a) per workload, or (b) it may be composed of one or more components that run independent of the workload and its associated control plane. In the case of (a), the control plane handles the deployment of the functional component. In the case of (b) where the functionality of the module runs independently and handles requests from multiple workloads, a client module is what is deployed by the control plane. This client module passes parameters to the external component(s) and monitors the status and results of the requests to the external component(s). The following diagram shows an example with an Arrow Flight module that is fully deployed by the control plane and a second module where the client is deployed by the control plane but the ETL component providing the functionality has been independently deployed and supports multiple workloads. Components that make up a module There are several parts to a module: Optional external component(s): deployed and managed independently of Mesh for Data. Module Workload : the workload that runs once the Helm chart is installed by the control plane. Can be a client to the external component(s) or be independent. Module Helm Chart : the package containing the module workload that the control plane installs as part of a data plane. M4DModule YAML : describes the functional capabilities, supported interfaces, and has links to the Module Helm chart. Registering a module To make the control plane aware of the module so that it can be included in appropriate workload data flows, the administrator must apply the M4DModule YAML in the m4d-system namespace. This makes the control plane aware of the existence of the module. Note that it does not check that the module's helm chart exists. For example, the following registers the arrow-flight-module : kubectl apply -f https://raw.githubusercontent.com/mesh-for-data/arrow-flight-module/master/module.yaml -n m4d-system When is a module used? There are three main data flows in which modules may be used: * Read - preparing data to be read and/or actually reading the data * Write - writing a new data set or appending data to an existing data set * Copy - for performing an implicit data copy on behalf of the application. The decision to do an implicit copy is made by the control plane, typically for performance or governance reasons. A module may be used in one or more of these flows, as is indicated in the module's yaml file. Control plane choice of modules A user workload description M4DApplicaton includes a list of the data sets required, the technologies that will be used to read them, and information about the location and reason for the use of the data. This information together with input from data and enterprise policies, determine which modules are chosen by the control plane. Currently the logic for choosing the modules for the data plane is as follows: 1. If the user is requesting to read data, find all the read flow related modules 1. If the data set protocol/format and the protocol/format requested by the user do not match, then make an implicit copy of the data, storing it such that it is readable via the protocol/format requested by the user. 1. If the governance action(s) required on the data set are not supported by the read module, and it is supported by the implicit copy module ... then make an implicit copy. Otherwise no need for implicit copy, and read will be done from the source directly. Available modules The table below lists the currently available modules: Name Description M4DModule Prerequisite arrow-flight-module reading datasets while performing data transformations https://raw.githubusercontent.com/mesh-for-data/arrow-flight-module/master/module.yaml implicit-copy copies data between any two supported data stores, for example S3 and Kafka, and applies transformations. https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/master/modules/implicit-copy-batch-module.yaml https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/master/modules/implicit-copy-stream-module.yaml - Datashim deployment. - M4DStorageAccount resource deployed in the control plane namespace to hold the details of the storage which is used by the module for coping the data. Contributing Read Module Development for details on the components that make up a module and how to create a module.","title":"Modules"},{"location":"concepts/modules.html#modules","text":"The project currently has two extension mechanisms, namely connectors and modules. This page describes what modules are and how they are leveraged by the control plane to build the data plane flow.","title":"Modules"},{"location":"concepts/modules.html#what-are-modules","text":"As described in the architecture page, the control plane generates a description of a data plane based on policies and application requirements. This is known as a blueprint, and includes components that are deployed by the control plane to fulfill different data-centric requirements. For example, a component that can mask data can be used to enforce a data masking policy, or a component that copies data may be used to create a local data copy to meet performance requirements, etc. Modules are the way to describe such data plane components and make them available to the control plane. A module is packaged as a Helm chart that the control plane can install to a workload's data plane. To make a module available to the control plane it must be registered by applying a M4DModule CRD. The functionality described by the module may be deployed (a) per workload, or (b) it may be composed of one or more components that run independent of the workload and its associated control plane. In the case of (a), the control plane handles the deployment of the functional component. In the case of (b) where the functionality of the module runs independently and handles requests from multiple workloads, a client module is what is deployed by the control plane. This client module passes parameters to the external component(s) and monitors the status and results of the requests to the external component(s). The following diagram shows an example with an Arrow Flight module that is fully deployed by the control plane and a second module where the client is deployed by the control plane but the ETL component providing the functionality has been independently deployed and supports multiple workloads.","title":"What are modules?"},{"location":"concepts/modules.html#components-that-make-up-a-module","text":"There are several parts to a module: Optional external component(s): deployed and managed independently of Mesh for Data. Module Workload : the workload that runs once the Helm chart is installed by the control plane. Can be a client to the external component(s) or be independent. Module Helm Chart : the package containing the module workload that the control plane installs as part of a data plane. M4DModule YAML : describes the functional capabilities, supported interfaces, and has links to the Module Helm chart.","title":"Components that make up a module"},{"location":"concepts/modules.html#registering-a-module","text":"To make the control plane aware of the module so that it can be included in appropriate workload data flows, the administrator must apply the M4DModule YAML in the m4d-system namespace. This makes the control plane aware of the existence of the module. Note that it does not check that the module's helm chart exists. For example, the following registers the arrow-flight-module : kubectl apply -f https://raw.githubusercontent.com/mesh-for-data/arrow-flight-module/master/module.yaml -n m4d-system","title":"Registering a module"},{"location":"concepts/modules.html#when-is-a-module-used","text":"There are three main data flows in which modules may be used: * Read - preparing data to be read and/or actually reading the data * Write - writing a new data set or appending data to an existing data set * Copy - for performing an implicit data copy on behalf of the application. The decision to do an implicit copy is made by the control plane, typically for performance or governance reasons. A module may be used in one or more of these flows, as is indicated in the module's yaml file.","title":"When is a module used?"},{"location":"concepts/modules.html#control-plane-choice-of-modules","text":"A user workload description M4DApplicaton includes a list of the data sets required, the technologies that will be used to read them, and information about the location and reason for the use of the data. This information together with input from data and enterprise policies, determine which modules are chosen by the control plane. Currently the logic for choosing the modules for the data plane is as follows: 1. If the user is requesting to read data, find all the read flow related modules 1. If the data set protocol/format and the protocol/format requested by the user do not match, then make an implicit copy of the data, storing it such that it is readable via the protocol/format requested by the user. 1. If the governance action(s) required on the data set are not supported by the read module, and it is supported by the implicit copy module ... then make an implicit copy. Otherwise no need for implicit copy, and read will be done from the source directly.","title":"Control plane choice of modules"},{"location":"concepts/modules.html#available-modules","text":"The table below lists the currently available modules: Name Description M4DModule Prerequisite arrow-flight-module reading datasets while performing data transformations https://raw.githubusercontent.com/mesh-for-data/arrow-flight-module/master/module.yaml implicit-copy copies data between any two supported data stores, for example S3 and Kafka, and applies transformations. https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/master/modules/implicit-copy-batch-module.yaml https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/master/modules/implicit-copy-stream-module.yaml - Datashim deployment. - M4DStorageAccount resource deployed in the control plane namespace to hold the details of the storage which is used by the module for coping the data.","title":"Available modules"},{"location":"concepts/modules.html#contributing","text":"Read Module Development for details on the components that make up a module and how to create a module.","title":"Contributing"},{"location":"concepts/vault_plugins.html","text":"HashiCorp Vault plugins HashiCorp Vault plugins are standalone applications that Vault server executes to enable third-party secret engines and auth methods. After their enablement during Vault server initialization, the plugins can be used as a regular auth or secrets backends. This project uses secrets plugins to retrieve dataset credentials by the running modules . The plugins retrieve the credentials from where they are stored, for example, data catalog or in kubernetes secret. Vault-plugin-secrets-kubernetes-reader plugin is an example of Vault custom secret plugin which retrieves dataset credentials stored in a kubernetes secret. Additional secret plugins can be developed to retrieve credentials additional location. This tutorial can serve as a good starting point to learn about Vault plugin development. The following steps are for configuring a secret plug-in for Mesh for Data: Enable the plugin during Vault server initialization in a specific path. An example of that can be found in helm chart values.yaml file in the project where Vault-plugin-secrets-kubernetes-reader plugin is enabled in kubernetes-secrets path: vault secrets enable -path = kubernetes-secrets vault-plugin-secrets-kubernetes-reader Add Vault policy to allow the modules to access secrets using the plugin. Following is an example of a policy which gives permission to read secrets in Vault path kubernetes-secrets : vault policy write \"allow-all-dataset-creds\" - <<EOF path \"kubernetes-secrets/*\" { capabilities = [\"read\"] } EOF 3. Have the CatalogDatasetInfo structure from the data catalog response contain the Vault secret path which should be used to retrieve the credentials for a given asset. When Vault plugin is used to retrieve the credentials the parameters to the plugin should follow the plugin usage instructions. This path will later be passed on to the modules . For example, when the credentials are stored in kubernetes secret as is done in the Katalog built-in data catalog; the Vault-plugin-secrets-kubernetes-reader plugin can be used to retrieve the credentials. In this case two parameters should be passed: paysim-csv which is the kubernetes secret name that holds the credentials and m4d-notebook-sample is the secret namespace, both are known to the katalog when constructing the path. The following snippet shows CatalogDatasetInfo structure with Vault secret path in CredentialsInfo field. connectors.CatalogDatasetInfo { DatasetId: m4d-notebook-sample/paysim-csv, Details: & connectors.DatasetDetails { Name: m4d-notebook-sample/paysim-csv, Geo: theshire, DataStore: m4d-notebook-sample/paysim-csv, CredentialsInfo: & connectors.CredentialsInfo { VaultSecretPath: \"/v1/kubernetes-secrets/paysim-csv?namespace=m4d-notebook-sample\" } , } , } 4. Update the modules to use the Vault related values to retrieve dataset credentias during their runtime execution. The values contain secretPath field with the plugin path as described in the previous step. The following snippet, taken from hello-world-module values.yaml file, contains an example of such values. vault: # Address is Vault address address: http://vault.m4d-system:8200 # AuthPath is the path to auth method used to login to Vault authPath: /v1/auth/kubernetes/login # Role is the Vault role used for retrieving the credentials role: module # SecretPath is the path of the secret holding the Credentials in Vault secretPath: /v1/kubernetes-secrets/paysim-csv?namespace = m4d-notebook-sample","title":"HashiCorp Vault plugins"},{"location":"concepts/vault_plugins.html#hashicorp-vault-plugins","text":"HashiCorp Vault plugins are standalone applications that Vault server executes to enable third-party secret engines and auth methods. After their enablement during Vault server initialization, the plugins can be used as a regular auth or secrets backends. This project uses secrets plugins to retrieve dataset credentials by the running modules . The plugins retrieve the credentials from where they are stored, for example, data catalog or in kubernetes secret. Vault-plugin-secrets-kubernetes-reader plugin is an example of Vault custom secret plugin which retrieves dataset credentials stored in a kubernetes secret. Additional secret plugins can be developed to retrieve credentials additional location. This tutorial can serve as a good starting point to learn about Vault plugin development. The following steps are for configuring a secret plug-in for Mesh for Data: Enable the plugin during Vault server initialization in a specific path. An example of that can be found in helm chart values.yaml file in the project where Vault-plugin-secrets-kubernetes-reader plugin is enabled in kubernetes-secrets path: vault secrets enable -path = kubernetes-secrets vault-plugin-secrets-kubernetes-reader Add Vault policy to allow the modules to access secrets using the plugin. Following is an example of a policy which gives permission to read secrets in Vault path kubernetes-secrets : vault policy write \"allow-all-dataset-creds\" - <<EOF path \"kubernetes-secrets/*\" { capabilities = [\"read\"] } EOF 3. Have the CatalogDatasetInfo structure from the data catalog response contain the Vault secret path which should be used to retrieve the credentials for a given asset. When Vault plugin is used to retrieve the credentials the parameters to the plugin should follow the plugin usage instructions. This path will later be passed on to the modules . For example, when the credentials are stored in kubernetes secret as is done in the Katalog built-in data catalog; the Vault-plugin-secrets-kubernetes-reader plugin can be used to retrieve the credentials. In this case two parameters should be passed: paysim-csv which is the kubernetes secret name that holds the credentials and m4d-notebook-sample is the secret namespace, both are known to the katalog when constructing the path. The following snippet shows CatalogDatasetInfo structure with Vault secret path in CredentialsInfo field. connectors.CatalogDatasetInfo { DatasetId: m4d-notebook-sample/paysim-csv, Details: & connectors.DatasetDetails { Name: m4d-notebook-sample/paysim-csv, Geo: theshire, DataStore: m4d-notebook-sample/paysim-csv, CredentialsInfo: & connectors.CredentialsInfo { VaultSecretPath: \"/v1/kubernetes-secrets/paysim-csv?namespace=m4d-notebook-sample\" } , } , } 4. Update the modules to use the Vault related values to retrieve dataset credentias during their runtime execution. The values contain secretPath field with the plugin path as described in the previous step. The following snippet, taken from hello-world-module values.yaml file, contains an example of such values. vault: # Address is Vault address address: http://vault.m4d-system:8200 # AuthPath is the path to auth method used to login to Vault authPath: /v1/auth/kubernetes/login # Role is the Vault role used for retrieving the credentials role: module # SecretPath is the path of the secret holding the Credentials in Vault secretPath: /v1/kubernetes-secrets/paysim-csv?namespace = m4d-notebook-sample","title":"HashiCorp Vault plugins"},{"location":"contribute/index.html","text":"Contribute Mesh for Data is open for contributions and welcomes anyone who wishes to contribute and take part in our journey towards success. This section contains information and guidelines to help you contribute more easily to the project. We would love for you to get involved Join our community in GitHub Discussions","title":"About"},{"location":"contribute/index.html#contribute","text":"Mesh for Data is open for contributions and welcomes anyone who wishes to contribute and take part in our journey towards success. This section contains information and guidelines to help you contribute more easily to the project. We would love for you to get involved Join our community in GitHub Discussions","title":"Contribute"},{"location":"contribute/build-test.html","text":"Build and Test Build the project images make docker-build Run unit tests make test Some tests for controllers are written in a fashion that they can be run on a simulated environment using envtest or on an already existing Kubernetes cluster (or local kind cluster). The default is to use envtest. In order to run the tests in a local cluster the following environment variables can be set: NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Please be aware that the controller is running locally in this case! If a controller is already deployed onto the cluster then the tests can be run with the command below. This will ensure that the tests are only creating CRDs on the cluster and checking their status: USE_EXISTING_CONTROLLER = true NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Environment variables description Environment variable Default Description USE_EXISTING_CLUSTER false This variable controls if an existing K8s cluster should be used or not. If not envtest will spin up an artificial environment that includes a local etcd setup. NO_SIMULATED_PROGRESS false This variable can be used by tests that can manually simulate progress of e.g. jobs or pods. e.g. the simulated test environment from testEnv does not progress pods etc while when testing against an external Kubernetes cluster this will actually run pods. USE_EXISTING_CONTROLLER false This variable controls if a controller should be set up and run by this test suite or if an external one should be used. E.g. in integration tests running against an existing setup a controller is already existing in the Kubernetes cluster and should not be started by the test as two controllers competing may influence the test. Running integration tests Running in one step With the following you will then setup a kind cluster with the local registry, build and push current docker images and finally run the integration tests on it: make run-integration-tests Running step by step It is also possible to call the commands step by step, which sometimes is useful if you want to only repeat a specific step which failed without having to rerun the entire sequence # use the local kind registry export DOCKER_HOSTNAME = kind-registry:5000 export DOCKER_NAMESPACE = m4d-system # build a local kind cluser make kind # deploy the the cluster 3rd party such as cert-manager and vault make cluster-prepare # build all docker images and push them to the local registry make docker # build the mock/test docker images and push them to local registry make -C test/services docker-build docker-push # wait until cluster-prepare setup really completed make cluster-prepare-wait # deploy the m4d CRDs to the kind cluster make -C manager deploy-crd # deploy m4d manager to the kind cluster make -C manager deploy_it # wait until manager is ready make -C manager wait_for_manager # configure Vault make configure-vault # build and push helm charts to the local registry make helm # actually run the integration tests make -C manager run-integration-tests Building in a multi cluster environment As Mesh for Data can run in a multi-cluster environment there is also a test environment that can be used that simulates this scenario. Using kind one can spin up two separate kubernetes clusters with differnt contexts and develop and test in these. Two kind clusters that share the same kind-registry can be set up using: make kind-setup-multi","title":"Build and Test"},{"location":"contribute/build-test.html#build-and-test","text":"","title":"Build and Test"},{"location":"contribute/build-test.html#build-the-project-images","text":"make docker-build","title":"Build the project images"},{"location":"contribute/build-test.html#run-unit-tests","text":"make test Some tests for controllers are written in a fashion that they can be run on a simulated environment using envtest or on an already existing Kubernetes cluster (or local kind cluster). The default is to use envtest. In order to run the tests in a local cluster the following environment variables can be set: NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test Please be aware that the controller is running locally in this case! If a controller is already deployed onto the cluster then the tests can be run with the command below. This will ensure that the tests are only creating CRDs on the cluster and checking their status: USE_EXISTING_CONTROLLER = true NO_SIMULATED_PROGRESS = true USE_EXISTING_CLUSTER = true make -C manager test","title":"Run unit tests"},{"location":"contribute/build-test.html#environment-variables-description","text":"Environment variable Default Description USE_EXISTING_CLUSTER false This variable controls if an existing K8s cluster should be used or not. If not envtest will spin up an artificial environment that includes a local etcd setup. NO_SIMULATED_PROGRESS false This variable can be used by tests that can manually simulate progress of e.g. jobs or pods. e.g. the simulated test environment from testEnv does not progress pods etc while when testing against an external Kubernetes cluster this will actually run pods. USE_EXISTING_CONTROLLER false This variable controls if a controller should be set up and run by this test suite or if an external one should be used. E.g. in integration tests running against an existing setup a controller is already existing in the Kubernetes cluster and should not be started by the test as two controllers competing may influence the test.","title":"Environment variables description"},{"location":"contribute/build-test.html#running-integration-tests","text":"","title":"Running integration tests"},{"location":"contribute/build-test.html#running-in-one-step","text":"With the following you will then setup a kind cluster with the local registry, build and push current docker images and finally run the integration tests on it: make run-integration-tests","title":"Running in one step"},{"location":"contribute/build-test.html#running-step-by-step","text":"It is also possible to call the commands step by step, which sometimes is useful if you want to only repeat a specific step which failed without having to rerun the entire sequence # use the local kind registry export DOCKER_HOSTNAME = kind-registry:5000 export DOCKER_NAMESPACE = m4d-system # build a local kind cluser make kind # deploy the the cluster 3rd party such as cert-manager and vault make cluster-prepare # build all docker images and push them to the local registry make docker # build the mock/test docker images and push them to local registry make -C test/services docker-build docker-push # wait until cluster-prepare setup really completed make cluster-prepare-wait # deploy the m4d CRDs to the kind cluster make -C manager deploy-crd # deploy m4d manager to the kind cluster make -C manager deploy_it # wait until manager is ready make -C manager wait_for_manager # configure Vault make configure-vault # build and push helm charts to the local registry make helm # actually run the integration tests make -C manager run-integration-tests","title":"Running step by step"},{"location":"contribute/build-test.html#building-in-a-multi-cluster-environment","text":"As Mesh for Data can run in a multi-cluster environment there is also a test environment that can be used that simulates this scenario. Using kind one can spin up two separate kubernetes clusters with differnt contexts and develop and test in these. Two kind clusters that share the same kind-registry can be set up using: make kind-setup-multi","title":"Building in a multi cluster environment"},{"location":"contribute/environment.html","text":"Development Environment This page describes what you need to install as a developer and contributor to this project, for setting up a development environment. Operating system Linux and Mac OS operating systems are officially supported. Windows users should consider using Windows Subsystem for Linux 2 (WSL 2), a remote Linux machine, or any other solution such as a virtual machine. Dependencies Install the following on your machine: go 1.13 or above Docker make jq unzip Maven ( mvn ) Java Development Kit version 8 or above Mac only : brew install coreutils (installs the timeout command) Then, run the following command to install additional dependencies: make install-tools This installs additional dependencies to hack/tools/bin . The make targets (e.g., make test ) are configured to use the binaries from hack/tools/bin . However, you may want to add some of these tools to your system PATH for direct usage from your terminal (e.g., for using kubectl ). Editors The project is predominantly written in Go so we recommend Visual Studio Code for its good Go support. Alternatively you can select from Editors Docker hub rate limits As docker hub introduced rate limits on docker image downloads this may effect development using the local kind setup. One option to fix the limit is to use a docker hub login for downloading the images. The environment will run a docker registry as a proxy for all public images. This registry runs in a docker container next to the kind clusters. export DOCKERHUB_USERNAME = 'your docker hub username' export DOCKERHUB_PASSWORD = 'your password'","title":"Development Environment"},{"location":"contribute/environment.html#development-environment","text":"This page describes what you need to install as a developer and contributor to this project, for setting up a development environment.","title":"Development Environment"},{"location":"contribute/environment.html#operating-system","text":"Linux and Mac OS operating systems are officially supported. Windows users should consider using Windows Subsystem for Linux 2 (WSL 2), a remote Linux machine, or any other solution such as a virtual machine.","title":"Operating system"},{"location":"contribute/environment.html#dependencies","text":"Install the following on your machine: go 1.13 or above Docker make jq unzip Maven ( mvn ) Java Development Kit version 8 or above Mac only : brew install coreutils (installs the timeout command) Then, run the following command to install additional dependencies: make install-tools This installs additional dependencies to hack/tools/bin . The make targets (e.g., make test ) are configured to use the binaries from hack/tools/bin . However, you may want to add some of these tools to your system PATH for direct usage from your terminal (e.g., for using kubectl ).","title":"Dependencies"},{"location":"contribute/environment.html#editors","text":"The project is predominantly written in Go so we recommend Visual Studio Code for its good Go support. Alternatively you can select from Editors","title":"Editors"},{"location":"contribute/environment.html#docker-hub-rate-limits","text":"As docker hub introduced rate limits on docker image downloads this may effect development using the local kind setup. One option to fix the limit is to use a docker hub login for downloading the images. The environment will run a docker registry as a proxy for all public images. This registry runs in a docker container next to the kind clusters. export DOCKERHUB_USERNAME = 'your docker hub username' export DOCKERHUB_PASSWORD = 'your password'","title":"Docker hub rate limits"},{"location":"contribute/flow.html","text":"GitHub Workflow This page describes the GitHub workflow that contributors should follow. Issues and pull requests Contributing to Mesh for Data is done following the GitHub workflow of Pull Requests. You should usually open a pull request in the following situations: Start work on a contribution that was that you\u2019ve already discussed in an issue. Submit trivial fixes (for example, a typo, a broken link or an obvious error). A pull request doesn\u2019t have to represent finished work. It\u2019s usually better to open a draft pull request early on, so others can watch or give feedback on your progress. Here\u2019s how to submit a pull request: Fork the main repository Clone the forked repository locally . Connect your local to the original \u201cupstream\u201d repository by adding it as a remote. git clone git@github.com: $( git config user.name ) /mesh-for-data.git git remote add upstream https://github.com/mesh-for-data/mesh-for-data.git git remote set-url --push upstream no_push Pull in changes from \u201cupstream\u201d often so that you stay up to date so that when you submit your pull request, merge conflicts will be less likely. git fetch upstream master git checkout master git merge upstream/master git push origin master Create a branch for your edits from master. Note that your should never add edits to the master branch itself. git checkout -b <branch name> Make commits of logical units , ensuring that commit messages are in the proper format . Push your changes to the created branch in your fork of the repository. Open a pull request to the original repository. Reference any relevant issues or supporting documentation in your PR (for example, \u201cCloses #37.\u201d) As always, you must follow code style , ensure that all tests pass , and add any new tests as appropriate. Thanks for your contribution! Normalize the code To ensure the code is formatted uniformly we use various linters which are invoked using make verify Format of the Commit Message The project follows a rough convention for commit messages that is designed to answer two questions: what changed and why. The subject line should feature the what and the body of the commit should describe the why. Every commit must also include a DCO Sign Off at the end of the commit message. By doing this you state that you certify the Developer Certificate of Origin . This can be automated by adding the -s flag to git commit . You can also mass sign-off a whole PR with git rebase --signoff master . Example commit message: scripts: add the test-cluster command this uses tmux to setup a test cluster that you can easily kill and start for debugging. Fixes #38 Signed-off-by: Legal Name <your.email@example.com> The format can be described more formally as follows: <subsystem>: <what changed> <BLANK LINE> <why this change was made> <BLANK LINE> <footer> <BLANK LINE> <signoff> The first line is the subject and should be no longer than 70 characters, the second line is always blank, and other lines should be wrapped at 80 characters. This allows the message to be easier to read on GitHub as well as in various git tools.","title":"GitHub Workflow"},{"location":"contribute/flow.html#github-workflow","text":"This page describes the GitHub workflow that contributors should follow.","title":"GitHub Workflow"},{"location":"contribute/flow.html#issues-and-pull-requests","text":"Contributing to Mesh for Data is done following the GitHub workflow of Pull Requests. You should usually open a pull request in the following situations: Start work on a contribution that was that you\u2019ve already discussed in an issue. Submit trivial fixes (for example, a typo, a broken link or an obvious error). A pull request doesn\u2019t have to represent finished work. It\u2019s usually better to open a draft pull request early on, so others can watch or give feedback on your progress. Here\u2019s how to submit a pull request: Fork the main repository Clone the forked repository locally . Connect your local to the original \u201cupstream\u201d repository by adding it as a remote. git clone git@github.com: $( git config user.name ) /mesh-for-data.git git remote add upstream https://github.com/mesh-for-data/mesh-for-data.git git remote set-url --push upstream no_push Pull in changes from \u201cupstream\u201d often so that you stay up to date so that when you submit your pull request, merge conflicts will be less likely. git fetch upstream master git checkout master git merge upstream/master git push origin master Create a branch for your edits from master. Note that your should never add edits to the master branch itself. git checkout -b <branch name> Make commits of logical units , ensuring that commit messages are in the proper format . Push your changes to the created branch in your fork of the repository. Open a pull request to the original repository. Reference any relevant issues or supporting documentation in your PR (for example, \u201cCloses #37.\u201d) As always, you must follow code style , ensure that all tests pass , and add any new tests as appropriate. Thanks for your contribution!","title":"Issues and pull requests"},{"location":"contribute/flow.html#normalize-the-code","text":"To ensure the code is formatted uniformly we use various linters which are invoked using make verify","title":"Normalize the code"},{"location":"contribute/flow.html#format-of-the-commit-message","text":"The project follows a rough convention for commit messages that is designed to answer two questions: what changed and why. The subject line should feature the what and the body of the commit should describe the why. Every commit must also include a DCO Sign Off at the end of the commit message. By doing this you state that you certify the Developer Certificate of Origin . This can be automated by adding the -s flag to git commit . You can also mass sign-off a whole PR with git rebase --signoff master . Example commit message: scripts: add the test-cluster command this uses tmux to setup a test cluster that you can easily kill and start for debugging. Fixes #38 Signed-off-by: Legal Name <your.email@example.com> The format can be described more formally as follows: <subsystem>: <what changed> <BLANK LINE> <why this change was made> <BLANK LINE> <footer> <BLANK LINE> <signoff> The first line is the subject and should be no longer than 70 characters, the second line is always blank, and other lines should be wrapped at 80 characters. This allows the message to be easier to read on GitHub as well as in various git tools.","title":"Format of the Commit Message"},{"location":"contribute/modules.html","text":"Module Development This page describes what must be provided when contributing a module . Steps for creating a module Implement the logic of the module you are contributing. The implementation can either be directly in the Module Workload or in an external component. If the logic is in an external component, then the module workload should act as a client - i.e. receiving paramaters from the control plane and passing them to the external component. Create and publish the Module Helm Chart that will be used by the control plane to deploy the module workload, update it, and delete it as necessary. Create the M4DModule YAML which describes the capabilities of the module workload, in which flows it should be considered for inclusion, its supported interfaces, and the link to the module helm chart. Test the new module These steps are described in the following sections in more detail, so that you can create your own modules for use by Mesh for Data. Note that a new module is maintained in its own git repository, separate from the mesh-for-data repository. Module Workload The module workload is associated with a specific user workload and is deployed by the control plane. It may implement the logic required itself, or it may be a client interface to an external component. Credential management Modules that access or write data need credentials in order to access the data store. The credentials are retrieved from HashiCorp Vault . The parameters to login to vault and to read secret are passed as part of the arguments to the module Helm chart. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <module service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath> Module Helm Chart For any module chosen by the control plane to be part of the data path, the control plane needs to be able to install/remove/upgrade an instance of the module. Mesh for Data uses Helm to provide this functionality. Follow the Helm getting started guide if you are unfamiliar with Helm. Note that Helm 3.3 or above is required. The names of the Kubernetes resources deployed by the module helm chart must contain the release name to avoid resource conflicts. A Kubernetes service resource which is used to access the module must have a name equal to the release name (this service name is also used in the optional spec.capabilites.api.endpoint.hostname field). Because the chart is installed by the control plane, the input values to the chart must match the relevant type of arguments . If the module workload needs to return information to the user, that information should be written to the NOTES.txt of the helm chart. For a full example see the Arrow Flight Module chart . Publishing the Helm Chart Once your Helm chart is ready, you need to push it to a OCI-based registry such as ghcr.io . This allows the control plane of Mesh for Data to later pull the chart whenever it needs to be installed. You can use the hack/make-rules/helm.mk Makefile, or manually push the chart: HELM_EXPERIMENTAL_OCI = 1 helm registry login -u <username> <registry> helm chart save <chart folder> <registry>/<path>:<version> helm chart push <registry>/<path>:<version> M4DModule YAML M4DModule is a kubernetes Custom Resource Definition (CRD) which describes to the control plane the functionality provided by the module. The M4DModule CRD has no controller. The specification of the M4DModule Kubernetes CRD is available in the API documentation . The YAML file begins with standard Kubernetes metadata followed by the M4DModule specification: apiVersion : app.m4d.ibm.com/v1alpha1 # always this value kind : M4DModule # always this value metadata : name : \"<module name>\" # the name of your new module namespace : m4d-system # control plane namespace. Always m4d-system spec : ... The child fields of spec are described next. spec.chart This is a link to a the Helm chart stored in the image registry . This is similar to how a Kubernetes Pod references a container image. See Module Helm chart for more details. spec: chart: \"<helm chart link>\" # e.g.: ghcr.io/username/chartname:chartversion spec.statusIndicators Used for tracking the status of the module in terms of success or failure. In many cases this can be omitted and the status will be detected automatically. if the Helm chart includes standard Kubernetes resources such as Deployment and Service, then the status is automatically detected. If however Custom Resource Definitions are used, then the status may not be automatically detected and statusIndicators should be specified. statusIndicators : - kind : \"<module name>\" successCondition : \"<condition>\" # ex: status.status == SUCCEEDED failureCondition : \"<condition>\" # ex: status.status == FAILED errorMessage : \"<field path>\" # ex: status.error spec.dependencies A dependency has a type and a name . Currently dependencies of type module are supported, indicating that another module must also be installed for this module to work. dependencies : - type : module #currently the only option is a dependency on another module deployed by the control plane name : <dependent module name> spec.flows The flows field indicates the types of capabilities supported by the module. Currently supported are three data flows: read for enabling an application to read data or prepare data for being read, write for enabling an application to write data, and copy for performing an implicit data copy on behalf of the application. A module is associated with one or more data flow based on its functionality. flows : # Indicate the data flow(s) in which the control plane should consider using this module - read # optional - write # optional - copy # optional spec.capabilities capabilites.supportedInterfaces lists the supported data services from which the module can read data and to which it can write * flow field can be read , write or copy * protocol field can take a value such as kafka , s3 , jdbc-db2 , m4d-arrow-flight , etc. * format field can take a value such as avro , parquet , json , or csv . Note that a module that targets copy flows will omit the api field and contain just source and sink , a module that only supports reading data assets will omit the sink field and only contain api and source capabilites.api describes the api exposed by the module for reading or writing data from the user's workload: * protocol field can take a value such as kafka , s3 , jdbc-db2 , m4d-arrow-flight , etc * dataformat field can take a value such as parquet , csv , arrow , etc * endpoint field describes the endpoint exposed the module capabilites.api.endpoint describes the endpoint from a networking perspective: * hostname field is the hostname to be used when accessing the module. Equals the release name. Can be omitted. * port field is the port of the service exposed by the module. * scheme field can take a value such as http , https , grpc , grpc+tls , jdbc:oracle:thin:@ , etc An example for a module that copies data from a db2 database table to an s3 bucket in parquet format. capabilities : supportedInterfaces : - flow : copy source : protocol : jdbc-db2 dataformat : table sink : protocol : s3 dataformat : parquet An example for a module that has an API for reading data, and supports reading both parquet and csv formats from s3. capabilities : api : protocol : m4d-arrow-flight dataformat : arrow endpoint : port : 80 scheme : grpc supportedInterfaces : - flow : read source : protocol : s3 dataformat : parquet - flow : read source : protocol : s3 dataformat : csv capabilites.actions are taken from a defined Enforcement Actions Taxonomy a module that does not perform any transformation on the data may omit the capabilities.actions field. The following is an example of how a module would declare that it knows how to redact, remove or encrypt data. For each action there is a level indication, which can be data set level, column level, or row level. In the example shown column level is indicated, and the actions arguments indicate the columns on which the transformation should be performed. capabilities : actions : - id : \"redact-ID\" level : 2 # column args : column_name : column_value - id : \"removed-ID\" level : 2 # column args : column_name : column_value - id : \"encrypt-ID\" level : 2 # column Full Examples The following are examples of YAMLs from fully implemented modules: An example YAML for a module that copies from db2 to s3 and includes transformation actions And an example arrow flight read module YAML, also with transformation support Test Register the module to make the control plane aware of it. Create an M4DApplication YAML for a user workload, ensuring that the data set and other parameters included in it, together with the governance policies defined in the policy manager, will result in your module being chosen based on the control plane logic . Apply the M4DApplication YAML. View the M4DApplication status . Run the user workload and review the results to check if they are what is expected.","title":"Module Development"},{"location":"contribute/modules.html#module-development","text":"This page describes what must be provided when contributing a module .","title":"Module Development"},{"location":"contribute/modules.html#steps-for-creating-a-module","text":"Implement the logic of the module you are contributing. The implementation can either be directly in the Module Workload or in an external component. If the logic is in an external component, then the module workload should act as a client - i.e. receiving paramaters from the control plane and passing them to the external component. Create and publish the Module Helm Chart that will be used by the control plane to deploy the module workload, update it, and delete it as necessary. Create the M4DModule YAML which describes the capabilities of the module workload, in which flows it should be considered for inclusion, its supported interfaces, and the link to the module helm chart. Test the new module These steps are described in the following sections in more detail, so that you can create your own modules for use by Mesh for Data. Note that a new module is maintained in its own git repository, separate from the mesh-for-data repository.","title":"Steps for creating a module"},{"location":"contribute/modules.html#module-workload","text":"The module workload is associated with a specific user workload and is deployed by the control plane. It may implement the logic required itself, or it may be a client interface to an external component.","title":"Module Workload"},{"location":"contribute/modules.html#credential-management","text":"Modules that access or write data need credentials in order to access the data store. The credentials are retrieved from HashiCorp Vault . The parameters to login to vault and to read secret are passed as part of the arguments to the module Helm chart. An example for Vault Login API call which uses the Vault parameters is as follows: $ curl -v -X POST <address>/<authPath> -H \"Content-Type: application/json\" --data '{\"jwt\": <module service account token>, \"role\": <role>}' An example for Vault Read Secret API call which uses the Vault parameters is as follows: $ curl --header \"X-Vault-Token: ...\" -X GET https://<address>/<secretPath>","title":"Credential management"},{"location":"contribute/modules.html#module-helm-chart","text":"For any module chosen by the control plane to be part of the data path, the control plane needs to be able to install/remove/upgrade an instance of the module. Mesh for Data uses Helm to provide this functionality. Follow the Helm getting started guide if you are unfamiliar with Helm. Note that Helm 3.3 or above is required. The names of the Kubernetes resources deployed by the module helm chart must contain the release name to avoid resource conflicts. A Kubernetes service resource which is used to access the module must have a name equal to the release name (this service name is also used in the optional spec.capabilites.api.endpoint.hostname field). Because the chart is installed by the control plane, the input values to the chart must match the relevant type of arguments . If the module workload needs to return information to the user, that information should be written to the NOTES.txt of the helm chart. For a full example see the Arrow Flight Module chart .","title":"Module Helm Chart"},{"location":"contribute/modules.html#publishing-the-helm-chart","text":"Once your Helm chart is ready, you need to push it to a OCI-based registry such as ghcr.io . This allows the control plane of Mesh for Data to later pull the chart whenever it needs to be installed. You can use the hack/make-rules/helm.mk Makefile, or manually push the chart: HELM_EXPERIMENTAL_OCI = 1 helm registry login -u <username> <registry> helm chart save <chart folder> <registry>/<path>:<version> helm chart push <registry>/<path>:<version>","title":"Publishing the Helm Chart"},{"location":"contribute/modules.html#m4dmodule-yaml","text":"M4DModule is a kubernetes Custom Resource Definition (CRD) which describes to the control plane the functionality provided by the module. The M4DModule CRD has no controller. The specification of the M4DModule Kubernetes CRD is available in the API documentation . The YAML file begins with standard Kubernetes metadata followed by the M4DModule specification: apiVersion : app.m4d.ibm.com/v1alpha1 # always this value kind : M4DModule # always this value metadata : name : \"<module name>\" # the name of your new module namespace : m4d-system # control plane namespace. Always m4d-system spec : ... The child fields of spec are described next.","title":"M4DModule YAML"},{"location":"contribute/modules.html#specchart","text":"This is a link to a the Helm chart stored in the image registry . This is similar to how a Kubernetes Pod references a container image. See Module Helm chart for more details. spec: chart: \"<helm chart link>\" # e.g.: ghcr.io/username/chartname:chartversion","title":"spec.chart"},{"location":"contribute/modules.html#specstatusindicators","text":"Used for tracking the status of the module in terms of success or failure. In many cases this can be omitted and the status will be detected automatically. if the Helm chart includes standard Kubernetes resources such as Deployment and Service, then the status is automatically detected. If however Custom Resource Definitions are used, then the status may not be automatically detected and statusIndicators should be specified. statusIndicators : - kind : \"<module name>\" successCondition : \"<condition>\" # ex: status.status == SUCCEEDED failureCondition : \"<condition>\" # ex: status.status == FAILED errorMessage : \"<field path>\" # ex: status.error","title":"spec.statusIndicators"},{"location":"contribute/modules.html#specdependencies","text":"A dependency has a type and a name . Currently dependencies of type module are supported, indicating that another module must also be installed for this module to work. dependencies : - type : module #currently the only option is a dependency on another module deployed by the control plane name : <dependent module name>","title":"spec.dependencies"},{"location":"contribute/modules.html#specflows","text":"The flows field indicates the types of capabilities supported by the module. Currently supported are three data flows: read for enabling an application to read data or prepare data for being read, write for enabling an application to write data, and copy for performing an implicit data copy on behalf of the application. A module is associated with one or more data flow based on its functionality. flows : # Indicate the data flow(s) in which the control plane should consider using this module - read # optional - write # optional - copy # optional","title":"spec.flows"},{"location":"contribute/modules.html#speccapabilities","text":"capabilites.supportedInterfaces lists the supported data services from which the module can read data and to which it can write * flow field can be read , write or copy * protocol field can take a value such as kafka , s3 , jdbc-db2 , m4d-arrow-flight , etc. * format field can take a value such as avro , parquet , json , or csv . Note that a module that targets copy flows will omit the api field and contain just source and sink , a module that only supports reading data assets will omit the sink field and only contain api and source capabilites.api describes the api exposed by the module for reading or writing data from the user's workload: * protocol field can take a value such as kafka , s3 , jdbc-db2 , m4d-arrow-flight , etc * dataformat field can take a value such as parquet , csv , arrow , etc * endpoint field describes the endpoint exposed the module capabilites.api.endpoint describes the endpoint from a networking perspective: * hostname field is the hostname to be used when accessing the module. Equals the release name. Can be omitted. * port field is the port of the service exposed by the module. * scheme field can take a value such as http , https , grpc , grpc+tls , jdbc:oracle:thin:@ , etc An example for a module that copies data from a db2 database table to an s3 bucket in parquet format. capabilities : supportedInterfaces : - flow : copy source : protocol : jdbc-db2 dataformat : table sink : protocol : s3 dataformat : parquet An example for a module that has an API for reading data, and supports reading both parquet and csv formats from s3. capabilities : api : protocol : m4d-arrow-flight dataformat : arrow endpoint : port : 80 scheme : grpc supportedInterfaces : - flow : read source : protocol : s3 dataformat : parquet - flow : read source : protocol : s3 dataformat : csv capabilites.actions are taken from a defined Enforcement Actions Taxonomy a module that does not perform any transformation on the data may omit the capabilities.actions field. The following is an example of how a module would declare that it knows how to redact, remove or encrypt data. For each action there is a level indication, which can be data set level, column level, or row level. In the example shown column level is indicated, and the actions arguments indicate the columns on which the transformation should be performed. capabilities : actions : - id : \"redact-ID\" level : 2 # column args : column_name : column_value - id : \"removed-ID\" level : 2 # column args : column_name : column_value - id : \"encrypt-ID\" level : 2 # column","title":"spec.capabilities"},{"location":"contribute/modules.html#full-examples","text":"The following are examples of YAMLs from fully implemented modules: An example YAML for a module that copies from db2 to s3 and includes transformation actions And an example arrow flight read module YAML, also with transformation support","title":"Full Examples"},{"location":"contribute/modules.html#test","text":"Register the module to make the control plane aware of it. Create an M4DApplication YAML for a user workload, ensuring that the data set and other parameters included in it, together with the governance policies defined in the policy manager, will result in your module being chosen based on the control plane logic . Apply the M4DApplication YAML. View the M4DApplication status . Run the user workload and review the results to check if they are what is expected.","title":"Test"},{"location":"contribute/documentation/index.html","text":"Contribute Documentation The content of this website is the documentation of the project. The documentation is managed in /site/docs as markdown files. MkDocs and the Material theme are used to generate the website from these markdown files. Reference pages are auto generated from the source code. Therefore, if you change Kubernetes Custom Resource Definitions or the connectors API then you must add reasonable documentation comments. The rest of the documentation pages are written manually. Contributing to the documentation is therefore similar to code contribution and follows the same process of using pull requests. However, when writing documentation you must also follow the formatting and style guidelines.","title":"Contribute Documentation"},{"location":"contribute/documentation/index.html#contribute-documentation","text":"The content of this website is the documentation of the project. The documentation is managed in /site/docs as markdown files. MkDocs and the Material theme are used to generate the website from these markdown files. Reference pages are auto generated from the source code. Therefore, if you change Kubernetes Custom Resource Definitions or the connectors API then you must add reasonable documentation comments. The rest of the documentation pages are written manually. Contributing to the documentation is therefore similar to code contribution and follows the same process of using pull requests. However, when writing documentation you must also follow the formatting and style guidelines.","title":"Contribute Documentation"},{"location":"contribute/documentation/formatting.html","text":"Formatting Standards This page shows the formatting standards for the Mesh for Data documentation. Link to other pages using relative links When linking between pages in the documentation you can simply use the regular Markdown linking syntax, including the relative path to the Markdown document you wish to link to. For example: Please see the [ project license ]( license.md ) for further details. If the target documentation file is in another directory you'll need to make sure to include any relative directory path in the link: Please see the [ project license ]( ../about/license.md ) for further details. Prefer SVG format for diagrams Place image files in the docs/static directory. Use regular Markdown syntax for images. For example: ![](../static/myimage.svg) To make localization easier and enhance accessibility, the preferred image format is SVG. We recommend to use draw.io for creating images and diagrams. Use Export as to save your image in SVG format. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io and be sure to check Embed images if you diagram includes any. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. This approach helps make diagrams easier to understand and the content more accessible. Do not wrap lines Never wrap lines after a fixed number of characters or in a middle of a senstence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line. Use angle brackets for placeholders Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods. Use bold to emphasize user interface elements Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'. Use bold to emphasize important text Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters . Don't use capitalization for emphasis Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks `` around the referenced value to make the connection explicit. For example, use IsolationPolicy , not Isolation Policy or isolation policy . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The isolation policy configuration takes place in a YAML file.\" Use italics to emphasize new terms Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane . Use back-ticks around file names, directories, and paths Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file. Use back-ticks around inline code and commands Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use \"foo apply\". Use code-blocks for commands you intend readers to execute. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands. Use back-ticks around object field names Do Don't Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the \"rule\" field is a Rule object.","title":"Formatting Standards"},{"location":"contribute/documentation/formatting.html#formatting-standards","text":"This page shows the formatting standards for the Mesh for Data documentation.","title":"Formatting Standards"},{"location":"contribute/documentation/formatting.html#link-to-other-pages-using-relative-links","text":"When linking between pages in the documentation you can simply use the regular Markdown linking syntax, including the relative path to the Markdown document you wish to link to. For example: Please see the [ project license ]( license.md ) for further details. If the target documentation file is in another directory you'll need to make sure to include any relative directory path in the link: Please see the [ project license ]( ../about/license.md ) for further details.","title":"Link to other pages using relative links"},{"location":"contribute/documentation/formatting.html#prefer-svg-format-for-diagrams","text":"Place image files in the docs/static directory. Use regular Markdown syntax for images. For example: ![](../static/myimage.svg) To make localization easier and enhance accessibility, the preferred image format is SVG. We recommend to use draw.io for creating images and diagrams. Use Export as to save your image in SVG format. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io and be sure to check Embed images if you diagram includes any. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. This approach helps make diagrams easier to understand and the content more accessible.","title":"Prefer SVG format for diagrams"},{"location":"contribute/documentation/formatting.html#do-not-wrap-lines","text":"Never wrap lines after a fixed number of characters or in a middle of a senstence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line.","title":"Do not wrap lines"},{"location":"contribute/documentation/formatting.html#use-angle-brackets-for-placeholders","text":"Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods.","title":"Use angle brackets for placeholders"},{"location":"contribute/documentation/formatting.html#use-bold-to-emphasize-user-interface-elements","text":"Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'.","title":"Use bold to emphasize user interface elements"},{"location":"contribute/documentation/formatting.html#use-bold-to-emphasize-important-text","text":"Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters .","title":"Use bold to emphasize important text"},{"location":"contribute/documentation/formatting.html#dont-use-capitalization-for-emphasis","text":"Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks `` around the referenced value to make the connection explicit. For example, use IsolationPolicy , not Isolation Policy or isolation policy . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The isolation policy configuration takes place in a YAML file.\"","title":"Don't use capitalization for emphasis"},{"location":"contribute/documentation/formatting.html#use-italics-to-emphasize-new-terms","text":"Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane .","title":"Use italics to emphasize new terms"},{"location":"contribute/documentation/formatting.html#use-back-ticks-around-file-names-directories-and-paths","text":"Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file.","title":"Use back-ticks around file names, directories, and paths"},{"location":"contribute/documentation/formatting.html#use-back-ticks-around-inline-code-and-commands","text":"Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use \"foo apply\". Use code-blocks for commands you intend readers to execute. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands.","title":"Use back-ticks around inline code and commands"},{"location":"contribute/documentation/formatting.html#use-back-ticks-around-object-field-names","text":"Do Don't Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the \"rule\" field is a Rule object.","title":"Use back-ticks around object field names"},{"location":"contribute/documentation/style.html","text":"Style Guide This page provides basic style guidance for keeping the documentation of Mesh for Data clear and understandable . Choose the right title Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct Use sentence case for headings Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https Use present tense Do Don't This command starts a proxy. This command will start a proxy. Exception: Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided. Use active voice Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file. Use simple and direct language Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods. Prefer shorter words over longer alternatives Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ... Address the reader as \"you\" Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ... Avoid using \"we\" Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Mesh for Data provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods. Avoid jargon and idioms Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ... Avoid statements that will soon be out of date Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ... Avoid statements about the future Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a clear indication under the front matter that identifies the information accordingly: Warning This page describes a feature that is not yet released The only exceptions to this rule are design or architecture documents that can describe a vision. However, you must clearly distiquish between implemented features and a vision. Create useful links There are good hyperlinks, and bad hyperlinks. The common practice of calling links here or click here are examples of bad hyperlinks. Check out this excellent article explaining what makes a good hyperlink and try to keep these guidelines in mind when creating or reviewing site content.","title":"Style Guide"},{"location":"contribute/documentation/style.html#style-guide","text":"This page provides basic style guidance for keeping the documentation of Mesh for Data clear and understandable .","title":"Style Guide"},{"location":"contribute/documentation/style.html#choose-the-right-title","text":"Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct","title":"Choose the right title"},{"location":"contribute/documentation/style.html#use-sentence-case-for-headings","text":"Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https","title":"Use sentence case for headings"},{"location":"contribute/documentation/style.html#use-present-tense","text":"Do Don't This command starts a proxy. This command will start a proxy. Exception: Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided.","title":"Use present tense"},{"location":"contribute/documentation/style.html#use-active-voice","text":"Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file.","title":"Use active voice"},{"location":"contribute/documentation/style.html#use-simple-and-direct-language","text":"Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods.","title":"Use simple and direct language"},{"location":"contribute/documentation/style.html#prefer-shorter-words-over-longer-alternatives","text":"Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ...","title":"Prefer shorter words over longer alternatives"},{"location":"contribute/documentation/style.html#address-the-reader-as-you","text":"Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ...","title":"Address the reader as \"you\""},{"location":"contribute/documentation/style.html#avoid-using-we","text":"Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Mesh for Data provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods.","title":"Avoid using \"we\""},{"location":"contribute/documentation/style.html#avoid-jargon-and-idioms","text":"Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ...","title":"Avoid jargon and idioms"},{"location":"contribute/documentation/style.html#avoid-statements-that-will-soon-be-out-of-date","text":"Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ...","title":"Avoid statements that will soon be out of date"},{"location":"contribute/documentation/style.html#avoid-statements-about-the-future","text":"Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a clear indication under the front matter that identifies the information accordingly: Warning This page describes a feature that is not yet released The only exceptions to this rule are design or architecture documents that can describe a vision. However, you must clearly distiquish between implemented features and a vision.","title":"Avoid statements about the future"},{"location":"contribute/documentation/style.html#create-useful-links","text":"There are good hyperlinks, and bad hyperlinks. The common practice of calling links here or click here are examples of bad hyperlinks. Check out this excellent article explaining what makes a good hyperlink and try to keep these guidelines in mind when creating or reviewing site content.","title":"Create useful links"},{"location":"get-started/quickstart.html","text":"Quick Start Guide Follow this guide to install Mesh for Data using default parameters that are suitable for experimentation on a single cluster. Before you begin Ensure that you have the following: Helm 3.3 or newer must be installed and configured on your machine. Kubectl 1.18 or newer must be installed on your machine. Access to a Kubernetes cluster such as Kind as a cluster administrator. Install cert-manager Mesh for Data requires cert-manager to be installed to your cluster. Many clusters already include cert-manager. Check if cert-manager namespace exists in your cluster and only run the following if it doesn't exist: helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.2.0 \\ --create-namespace \\ --set installCRDs = true \\ --wait --timeout 120s Install Hashicorp Vault and plugins Hashicorp Vault and a secrets-kubernetes-reader plugin are used by Mesh for Data for credential management. Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: Kubernetes git clone https://github.com/mesh-for-data/mesh-for-data.git cd mesh-for-data helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update charts/vault helm install vault charts/vault --create-namespace -n m4d-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s OpenShift git clone https://github.com/mesh-for-data/mesh-for-data.git cd mesh-for-data helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update charts/vault helm install vault charts/vault --create-namespace -n m4d-system \\ --set \"global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s Run the following to install vault and the plugin in development mode: Kubernetes helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update m4d-charts/vault helm install vault m4d-charts/vault --create-namespace -n m4d-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/v0.3.0/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s OpenShift helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update m4d-charts/vault helm install vault m4d-charts/vault --create-namespace -n m4d-system \\ --set \"global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/v0.3.0/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s Install control plane Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: git clone https://github.com/mesh-for-data/mesh-for-data.git cd mesh-for-data helm install m4d-crd charts/m4d-crd -n m4d-system --wait helm install m4d charts/m4d --set global.tag = latest -n m4d-system --wait The control plane includes a manager service that connects to a data catalog and to a policy manager. Install the latest release of Mesh for Data with a built-in data catalog and with Open Policy Agent as the policy manager: helm repo add m4d-charts https://mesh-for-data.github.io/charts helm repo update helm install m4d-crd m4d-charts/m4d-crd -n m4d-system --wait helm install m4d m4d-charts/m4d -n m4d-system --wait Install modules Modules are plugins that the control plane deploys whenever required. The arrow flight module enables reading data through Apache Arrow Flight API. Install the latest 1 release of arrow-flight-module: kubectl apply -f https://github.com/mesh-for-data/arrow-flight-module/releases/latest/download/module.yaml -n m4d-system Refer to the documentation of arrow-flight-module for other versions \u21a9","title":"Quick Start Guide"},{"location":"get-started/quickstart.html#quick-start-guide","text":"Follow this guide to install Mesh for Data using default parameters that are suitable for experimentation on a single cluster.","title":"Quick Start Guide"},{"location":"get-started/quickstart.html#before-you-begin","text":"Ensure that you have the following: Helm 3.3 or newer must be installed and configured on your machine. Kubectl 1.18 or newer must be installed on your machine. Access to a Kubernetes cluster such as Kind as a cluster administrator.","title":"Before you begin"},{"location":"get-started/quickstart.html#install-cert-manager","text":"Mesh for Data requires cert-manager to be installed to your cluster. Many clusters already include cert-manager. Check if cert-manager namespace exists in your cluster and only run the following if it doesn't exist: helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.2.0 \\ --create-namespace \\ --set installCRDs = true \\ --wait --timeout 120s","title":"Install cert-manager"},{"location":"get-started/quickstart.html#install-hashicorp-vault-and-plugins","text":"Hashicorp Vault and a secrets-kubernetes-reader plugin are used by Mesh for Data for credential management. Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: Kubernetes git clone https://github.com/mesh-for-data/mesh-for-data.git cd mesh-for-data helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update charts/vault helm install vault charts/vault --create-namespace -n m4d-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s OpenShift git clone https://github.com/mesh-for-data/mesh-for-data.git cd mesh-for-data helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update charts/vault helm install vault charts/vault --create-namespace -n m4d-system \\ --set \"global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s Run the following to install vault and the plugin in development mode: Kubernetes helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update m4d-charts/vault helm install vault m4d-charts/vault --create-namespace -n m4d-system \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/v0.3.0/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s OpenShift helm repo add hashicorp https://helm.releases.hashicorp.com helm dependency update m4d-charts/vault helm install vault m4d-charts/vault --create-namespace -n m4d-system \\ --set \"global.openshift=true\" \\ --set \"vault.injector.enabled=false\" \\ --set \"vault.server.dev.enabled=true\" \\ --values https://raw.githubusercontent.com/mesh-for-data/mesh-for-data/v0.3.0/charts/vault/env/dev/vault-single-cluster-values.yaml kubectl wait --for = condition = ready --all pod -n m4d-system --timeout = 120s","title":"Install Hashicorp Vault and plugins"},{"location":"get-started/quickstart.html#install-control-plane","text":"Install latest development version from GitHub The published Helm charts are only available for released versions. To install the dev version install the charts from the source code. For example: git clone https://github.com/mesh-for-data/mesh-for-data.git cd mesh-for-data helm install m4d-crd charts/m4d-crd -n m4d-system --wait helm install m4d charts/m4d --set global.tag = latest -n m4d-system --wait The control plane includes a manager service that connects to a data catalog and to a policy manager. Install the latest release of Mesh for Data with a built-in data catalog and with Open Policy Agent as the policy manager: helm repo add m4d-charts https://mesh-for-data.github.io/charts helm repo update helm install m4d-crd m4d-charts/m4d-crd -n m4d-system --wait helm install m4d m4d-charts/m4d -n m4d-system --wait","title":"Install control plane"},{"location":"get-started/quickstart.html#install-modules","text":"Modules are plugins that the control plane deploys whenever required. The arrow flight module enables reading data through Apache Arrow Flight API. Install the latest 1 release of arrow-flight-module: kubectl apply -f https://github.com/mesh-for-data/arrow-flight-module/releases/latest/download/module.yaml -n m4d-system Refer to the documentation of arrow-flight-module for other versions \u21a9","title":"Install modules"},{"location":"reference/connectors.html","text":"Protocol Documentation Top credentials.proto Credentials Field Type Label Description access_key string access credential for the bucket where the asset is stored secret_key string username string password string api_key string api key assigned to the bucket in which the asset is stored resource_instance_id string resource instance id for the bucket Top data_catalog_response.proto CatalogDatasetInfo Field Type Label Description dataset_id string details DatasetDetails Top policy_manager_response.proto ComponentVersion Field Type Label Description name string id string version string DatasetDecision Field Type Label Description dataset DatasetIdentifier decisions OperationDecision repeated EnforcementAction Field Type Label Description name string id string level EnforcementAction.EnforcementActionLevel args EnforcementAction.ArgsEntry repeated EnforcementAction.ArgsEntry Field Type Label Description key string value string OperationDecision Field Type Label Description operation AccessOperation enforcement_actions EnforcementAction repeated used_policies Policy repeated PoliciesDecisions Field Type Label Description component_versions ComponentVersion repeated dataset_decisions DatasetDecision repeated one per dataset general_decisions OperationDecision repeated Policy Field Type Label Description id string name string description string type string hierarchy string repeated EnforcementAction.EnforcementActionLevel Name Number Description UNKNOWN 0 DATASET 1 COLUMN 2 ROW 3 CELL 4 Top data_catalog_request.proto CatalogDatasetRequest Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON Top policy_manager_request.proto AccessOperation Field Type Label Description type AccessOperation.AccessType destination string Destination for transfer or write. ApplicationContext Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials app_info ApplicationDetails datasets DatasetContext repeated general_operations AccessOperation repeated ApplicationDetails Field Type Label Description processing_geography string properties ApplicationDetails.PropertiesEntry repeated ApplicationDetails.PropertiesEntry Field Type Label Description key string value string DatasetContext Field Type Label Description dataset DatasetIdentifier operation AccessOperation DatasetIdentifier Field Type Label Description dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON AccessOperation.AccessType Name Number Description UNKNOWN 0 READ 1 COPY 2 WRITE 3 Top data_catalog_service.proto DataCatalogService Method Name Request Type Response Type Description GetDatasetInfo CatalogDatasetRequest CatalogDatasetInfo RegisterDatasetInfo RegisterAssetRequest RegisterAssetResponse Top policy_manager_service.proto PolicyManagerService Method Name Request Type Response Type Description GetPoliciesDecisions ApplicationContext PoliciesDecisions Top register_asset_response.proto RegisterAssetResponse Field Type Label Description asset_id string Returns the id of the new asset registered in a catalog Top dataset_details.proto CredentialsInfo Field Type Label Description vault_secret_path string the path to Vault secret which is used to retrive the dataset credentials from the catalog. DataComponentMetadata Field Type Label Description component_type string e.g., column named_metadata DataComponentMetadata.NamedMetadataEntry repeated Named terms, that exist in Catalog toxonomy and the values for these terms for columns we will have \"SchemaDetails\" key, that will include technical schema details for this column TODO: Consider create special field for schema outside of metadata tags string repeated Tags - can be any free text added to a component (no taxonomy) DataComponentMetadata.NamedMetadataEntry Field Type Label Description key string value string DataStore Field Type Label Description type DataStore.DataStoreType name string for auditing and readability. Can be same as location type or can have more info if availble from catalog db2 Db2DataStore oneof location { // should have been oneof but for technical rasons, a problem to translate it to JSON, we remove the oneof for now should have been local, db2, s3 without \"location\" but had a problem to compile it in proto - collision with proto name DataLocationDb2 s3 S3DataStore kafka KafkaDataStore DatasetDetails Field Type Label Description name string name in Catalog data_owner string information on the owner of data asset - can have different formats for different catalogs data_store DataStore All info about the data store data_format string geo string geography location where data resides (if this information available) metadata DatasetMetadata LocationType locationType = 10; //publicCloud/privateCloud etc. Should be filled later when we understand better if we have a closed set of values and how they are used. credentials_info CredentialsInfo information about how to retrive dataset credentials from the catalog. DatasetMetadata Field Type Label Description dataset_named_metadata DatasetMetadata.DatasetNamedMetadataEntry repeated dataset_tags string repeated Tags - can be any free text added to a component (no taxonomy) components_metadata DatasetMetadata.ComponentsMetadataEntry repeated metadata for each component in asset. In tabular data each column is a component, then we will have: column name -> column metadata DatasetMetadata.ComponentsMetadataEntry Field Type Label Description key string value DataComponentMetadata DatasetMetadata.DatasetNamedMetadataEntry Field Type Label Description key string value string Db2DataStore Field Type Label Description url string database string table string reformat to SCHEMA.TABLE struct port string ssl string Note that bool value if set to \"false\" does not appear in the struct at all KafkaDataStore Field Type Label Description topic_name string bootstrap_servers string schema_registry string key_deserializer string value_deserializer string security_protocol string sasl_mechanism string ssl_truststore string ssl_truststore_password string S3DataStore Field Type Label Description endpoint string bucket string object_key string can be object name or the prefix for dataset region string WKC does not return it, it will stay empty in our case!!! DataStore.DataStoreType Name Number Description UNKNOWN 0 LOCAL 1 S3 2 DB2 3 KAFKA 4 Top register_asset_request.proto RegisterAssetRequest Field Type Label Description creds Credentials dataset_details DatasetDetails destination_catalog_id string credential_path string link to vault plugin for reading k8s secret with user credentials Scalar Value Types .proto Type Notes C++ Java Python Go C# PHP Ruby double double double float float64 double float Float float float float float float32 float float Float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required) int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required) uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required) sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required) sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required) fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required) sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum bool bool boolean boolean bool bool boolean TrueClass/FalseClass string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8) bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)","title":"Connectors API"},{"location":"reference/connectors.html#protocol-documentation","text":"Top","title":"Protocol Documentation"},{"location":"reference/connectors.html#credentialsproto","text":"","title":"credentials.proto"},{"location":"reference/connectors.html#credentials","text":"Field Type Label Description access_key string access credential for the bucket where the asset is stored secret_key string username string password string api_key string api key assigned to the bucket in which the asset is stored resource_instance_id string resource instance id for the bucket Top","title":"Credentials"},{"location":"reference/connectors.html#data_catalog_responseproto","text":"","title":"data_catalog_response.proto"},{"location":"reference/connectors.html#catalogdatasetinfo","text":"Field Type Label Description dataset_id string details DatasetDetails Top","title":"CatalogDatasetInfo"},{"location":"reference/connectors.html#policy_manager_responseproto","text":"","title":"policy_manager_response.proto"},{"location":"reference/connectors.html#componentversion","text":"Field Type Label Description name string id string version string","title":"ComponentVersion"},{"location":"reference/connectors.html#datasetdecision","text":"Field Type Label Description dataset DatasetIdentifier decisions OperationDecision repeated","title":"DatasetDecision"},{"location":"reference/connectors.html#enforcementaction","text":"Field Type Label Description name string id string level EnforcementAction.EnforcementActionLevel args EnforcementAction.ArgsEntry repeated","title":"EnforcementAction"},{"location":"reference/connectors.html#enforcementactionargsentry","text":"Field Type Label Description key string value string","title":"EnforcementAction.ArgsEntry"},{"location":"reference/connectors.html#operationdecision","text":"Field Type Label Description operation AccessOperation enforcement_actions EnforcementAction repeated used_policies Policy repeated","title":"OperationDecision"},{"location":"reference/connectors.html#policiesdecisions","text":"Field Type Label Description component_versions ComponentVersion repeated dataset_decisions DatasetDecision repeated one per dataset general_decisions OperationDecision repeated","title":"PoliciesDecisions"},{"location":"reference/connectors.html#policy","text":"Field Type Label Description id string name string description string type string hierarchy string repeated","title":"Policy"},{"location":"reference/connectors.html#enforcementactionenforcementactionlevel","text":"Name Number Description UNKNOWN 0 DATASET 1 COLUMN 2 ROW 3 CELL 4 Top","title":"EnforcementAction.EnforcementActionLevel"},{"location":"reference/connectors.html#data_catalog_requestproto","text":"","title":"data_catalog_request.proto"},{"location":"reference/connectors.html#catalogdatasetrequest","text":"Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON Top","title":"CatalogDatasetRequest"},{"location":"reference/connectors.html#policy_manager_requestproto","text":"","title":"policy_manager_request.proto"},{"location":"reference/connectors.html#accessoperation","text":"Field Type Label Description type AccessOperation.AccessType destination string Destination for transfer or write.","title":"AccessOperation"},{"location":"reference/connectors.html#applicationcontext","text":"Field Type Label Description credential_path string link to vault plugin for reading k8s secret with user credentials app_info ApplicationDetails datasets DatasetContext repeated general_operations AccessOperation repeated","title":"ApplicationContext"},{"location":"reference/connectors.html#applicationdetails","text":"Field Type Label Description processing_geography string properties ApplicationDetails.PropertiesEntry repeated","title":"ApplicationDetails"},{"location":"reference/connectors.html#applicationdetailspropertiesentry","text":"Field Type Label Description key string value string","title":"ApplicationDetails.PropertiesEntry"},{"location":"reference/connectors.html#datasetcontext","text":"Field Type Label Description dataset DatasetIdentifier operation AccessOperation","title":"DatasetContext"},{"location":"reference/connectors.html#datasetidentifier","text":"Field Type Label Description dataset_id string identifier of asset - always needed. JSON expected. Interpreted by the Connector, can contain any additional information as part of JSON","title":"DatasetIdentifier"},{"location":"reference/connectors.html#accessoperationaccesstype","text":"Name Number Description UNKNOWN 0 READ 1 COPY 2 WRITE 3 Top","title":"AccessOperation.AccessType"},{"location":"reference/connectors.html#data_catalog_serviceproto","text":"","title":"data_catalog_service.proto"},{"location":"reference/connectors.html#datacatalogservice","text":"Method Name Request Type Response Type Description GetDatasetInfo CatalogDatasetRequest CatalogDatasetInfo RegisterDatasetInfo RegisterAssetRequest RegisterAssetResponse Top","title":"DataCatalogService"},{"location":"reference/connectors.html#policy_manager_serviceproto","text":"","title":"policy_manager_service.proto"},{"location":"reference/connectors.html#policymanagerservice","text":"Method Name Request Type Response Type Description GetPoliciesDecisions ApplicationContext PoliciesDecisions Top","title":"PolicyManagerService"},{"location":"reference/connectors.html#register_asset_responseproto","text":"","title":"register_asset_response.proto"},{"location":"reference/connectors.html#registerassetresponse","text":"Field Type Label Description asset_id string Returns the id of the new asset registered in a catalog Top","title":"RegisterAssetResponse"},{"location":"reference/connectors.html#dataset_detailsproto","text":"","title":"dataset_details.proto"},{"location":"reference/connectors.html#credentialsinfo","text":"Field Type Label Description vault_secret_path string the path to Vault secret which is used to retrive the dataset credentials from the catalog.","title":"CredentialsInfo"},{"location":"reference/connectors.html#datacomponentmetadata","text":"Field Type Label Description component_type string e.g., column named_metadata DataComponentMetadata.NamedMetadataEntry repeated Named terms, that exist in Catalog toxonomy and the values for these terms for columns we will have \"SchemaDetails\" key, that will include technical schema details for this column TODO: Consider create special field for schema outside of metadata tags string repeated Tags - can be any free text added to a component (no taxonomy)","title":"DataComponentMetadata"},{"location":"reference/connectors.html#datacomponentmetadatanamedmetadataentry","text":"Field Type Label Description key string value string","title":"DataComponentMetadata.NamedMetadataEntry"},{"location":"reference/connectors.html#datastore","text":"Field Type Label Description type DataStore.DataStoreType name string for auditing and readability. Can be same as location type or can have more info if availble from catalog db2 Db2DataStore oneof location { // should have been oneof but for technical rasons, a problem to translate it to JSON, we remove the oneof for now should have been local, db2, s3 without \"location\" but had a problem to compile it in proto - collision with proto name DataLocationDb2 s3 S3DataStore kafka KafkaDataStore","title":"DataStore"},{"location":"reference/connectors.html#datasetdetails","text":"Field Type Label Description name string name in Catalog data_owner string information on the owner of data asset - can have different formats for different catalogs data_store DataStore All info about the data store data_format string geo string geography location where data resides (if this information available) metadata DatasetMetadata LocationType locationType = 10; //publicCloud/privateCloud etc. Should be filled later when we understand better if we have a closed set of values and how they are used. credentials_info CredentialsInfo information about how to retrive dataset credentials from the catalog.","title":"DatasetDetails"},{"location":"reference/connectors.html#datasetmetadata","text":"Field Type Label Description dataset_named_metadata DatasetMetadata.DatasetNamedMetadataEntry repeated dataset_tags string repeated Tags - can be any free text added to a component (no taxonomy) components_metadata DatasetMetadata.ComponentsMetadataEntry repeated metadata for each component in asset. In tabular data each column is a component, then we will have: column name -> column metadata","title":"DatasetMetadata"},{"location":"reference/connectors.html#datasetmetadatacomponentsmetadataentry","text":"Field Type Label Description key string value DataComponentMetadata","title":"DatasetMetadata.ComponentsMetadataEntry"},{"location":"reference/connectors.html#datasetmetadatadatasetnamedmetadataentry","text":"Field Type Label Description key string value string","title":"DatasetMetadata.DatasetNamedMetadataEntry"},{"location":"reference/connectors.html#db2datastore","text":"Field Type Label Description url string database string table string reformat to SCHEMA.TABLE struct port string ssl string Note that bool value if set to \"false\" does not appear in the struct at all","title":"Db2DataStore"},{"location":"reference/connectors.html#kafkadatastore","text":"Field Type Label Description topic_name string bootstrap_servers string schema_registry string key_deserializer string value_deserializer string security_protocol string sasl_mechanism string ssl_truststore string ssl_truststore_password string","title":"KafkaDataStore"},{"location":"reference/connectors.html#s3datastore","text":"Field Type Label Description endpoint string bucket string object_key string can be object name or the prefix for dataset region string WKC does not return it, it will stay empty in our case!!!","title":"S3DataStore"},{"location":"reference/connectors.html#datastoredatastoretype","text":"Name Number Description UNKNOWN 0 LOCAL 1 S3 2 DB2 3 KAFKA 4 Top","title":"DataStore.DataStoreType"},{"location":"reference/connectors.html#register_asset_requestproto","text":"","title":"register_asset_request.proto"},{"location":"reference/connectors.html#registerassetrequest","text":"Field Type Label Description creds Credentials dataset_details DatasetDetails destination_catalog_id string credential_path string link to vault plugin for reading k8s secret with user credentials","title":"RegisterAssetRequest"},{"location":"reference/connectors.html#scalar-value-types","text":".proto Type Notes C++ Java Python Go C# PHP Ruby double double double float float64 double float Float float float float float float32 float float Float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int32 int integer Bignum or Fixnum (as required) int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long int64 long integer/string Bignum uint32 Uses variable-length encoding. uint32 int int/long uint32 uint integer Bignum or Fixnum (as required) uint64 Uses variable-length encoding. uint64 long int/long uint64 ulong integer/string Bignum or Fixnum (as required) sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int int32 int integer Bignum or Fixnum (as required) sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long int64 long integer/string Bignum fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int uint32 uint integer Bignum or Fixnum (as required) fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long uint64 ulong integer/string Bignum sfixed32 Always four bytes. int32 int int int32 int integer Bignum or Fixnum (as required) sfixed64 Always eight bytes. int64 long int/long int64 long integer/string Bignum bool bool boolean boolean bool bool boolean TrueClass/FalseClass string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode string string string String (UTF-8) bytes May contain any arbitrary sequence of bytes. string ByteString str []byte ByteString string String (ASCII-8BIT)","title":"Scalar Value Types"},{"location":"reference/crds.html","text":"API Reference Packages: app.m4d.ibm.com/v1alpha1 Blueprint M4DApplication M4DModule M4DStorageAccount Plotter katalog.m4d.ibm.com/v1alpha1 Asset motion.m4d.ibm.com/v1alpha1 BatchTransfer StreamTransfer app.m4d.ibm.com/v1alpha1 Resource Types: Blueprint M4DApplication M4DModule M4DStorageAccount Plotter Blueprint \u21a9 Parent Blueprint is the Schema for the blueprints API Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string Blueprint true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BlueprintSpec defines the desired state of Blueprint, which is the runtime environment which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. The blueprint uses an \"argo like\" syntax which indicates the components and the flow of data between them as steps TODO: Add an indication of the communication relationships between the components false status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring false Blueprint.spec \u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which is the runtime environment which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. The blueprint uses an \"argo like\" syntax which indicates the components and the flow of data between them as steps TODO: Add an indication of the communication relationships between the components Name Type Description Required entrypoint string true flow object DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow true templates []object true Blueprint.spec.flow \u21a9 Parent DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow Name Type Description Required name string true steps []object true Blueprint.spec.flow.steps[index] \u21a9 Parent FlowStep is one step indicates an instance of a module in the blueprint, It includes the name of the module template (spec) and the parameters received by the component instance that is initiated by the orchestrator. Name Type Description Required arguments object Arguments are the input parameters for a specific instance of a module. false name string Name is the name of the instance of the module. For example, if the application is named \"notebook\" and an implicitcopy module is deemed necessary. The FlowStep name would be notebook-implicitcopy. true template string Template is the name of the specification in the Blueprint describing how to instantiate a component indicated by the module. It is the name of a M4DModule CRD. For example: implicit-copy-db2wh-to-s3-latest true Blueprint.spec.flow.steps[index].arguments \u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required copy object CopyArgs are parameters specific to modules that copy data from one data store to another. false read []object ReadArgs are parameters that are specific to modules that enable an application to read data false write []object WriteArgs are parameters that are specific to modules that enable an application to write data false Blueprint.spec.flow.steps[index].arguments.copy \u21a9 Parent CopyArgs are parameters specific to modules that copy data from one data store to another. Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is copied. false destination object Destination is the data store to which the data will be copied true source object Source is the where the data currently resides true Blueprint.spec.flow.steps[index].arguments.copy.destination \u21a9 Parent Destination is the data store to which the data will be copied Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Blueprint.spec.flow.steps[index].arguments.copy.destination.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Blueprint.spec.flow.steps[index].arguments.copy.source \u21a9 Parent Source is the where the data currently resides Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Blueprint.spec.flow.steps[index].arguments.copy.source.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Blueprint.spec.flow.steps[index].arguments.read[index] \u21a9 Parent ReadModuleArgs define the input parameters for modules that read data from location A Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data false assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the M4DApplication resource true source object Source of the read path module true Blueprint.spec.flow.steps[index].arguments.read[index].source \u21a9 Parent Source of the read path module Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Blueprint.spec.flow.steps[index].arguments.read[index].source.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Blueprint.spec.flow.steps[index].arguments.write[index] \u21a9 Parent WriteModuleArgs define the input parameters for modules that write data to location B Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is written. false destination object Destination is the data store to which the data will be written true Blueprint.spec.flow.steps[index].arguments.write[index].destination \u21a9 Parent Destination is the data store to which the data will be written Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Blueprint.spec.flow.steps[index].arguments.write[index].destination.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Blueprint.spec.templates[index] \u21a9 Parent ComponentTemplate is a copy of a M4DModule Custom Resource. It contains the information necessary to instantiate a component in a FlowStep, which provides the functionality described by the module. There are 3 different module types. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true kind string Kind of k8s resource true name string Name of the template true Blueprint.spec.templates[index].chart \u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required values map[string]string Values to pass to helm chart installation false name string Name of helm chart true Blueprint.status \u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. false observedState object ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false Blueprint.status.observedState \u21a9 Parent ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false M4DApplication \u21a9 Parent M4DApplication provides information about the application being used by a Data Scientist, the nature of the processing, and the data sets that the Data Scientist has chosen for processing by the application. The M4DApplication controller (aka pilot) obtains instructions regarding any governance related changes that must be performed on the data, identifies the modules capable of performing such changes, and finally generates the Blueprint which defines the secure runtime environment and all the components in it. This runtime environment provides the Data Scientist's application with access to the data requested in a secure manner and without having to provide any credentials for the data sets. The credentials are obtained automatically by the manager from an external credential management system, which may or may not be part of a data catalog. Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string M4DApplication true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object M4DApplicationSpec defines the desired state of M4DApplication. false status object M4DApplicationStatus defines the observed state of M4DApplication. false M4DApplication.spec \u21a9 Parent M4DApplicationSpec defines the desired state of M4DApplication. Name Type Description Required secretRef string SecretRef points to the secret that holds credentials for each system the user has been authenticated with. The secret is deployed in M4dApplication namespace. false selector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. false appInfo map[string]string AppInfo contains information describing the reasons for the processing that will be done by the Data Scientist's application. true data []object Data contains the identifiers of the data to be used by the Data Scientist's application, and the protocol used to access it and the format expected. true M4DApplication.spec.selector \u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. Name Type Description Required clusterName string Cluster name false workloadSelector object WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. true M4DApplication.spec.selector.workloadSelector \u21a9 Parent WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false M4DApplication.spec.selector.workloadSelector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true M4DApplication.spec.data[index] \u21a9 Parent DataContext indicates data set chosen by the Data Scientist to be used by his application, and includes information about the data format and technologies used by the application to access the data. Name Type Description Required catalogService string CatalogService represents the catalog service for accessing the requested dataset. If not specified, the enterprise catalog service will be used. false dataSetID string DataSetID is a unique identifier of the dataset chosen from the data catalog for processing by the data user application. true requirements object Requirements from the system true M4DApplication.spec.data[index].requirements \u21a9 Parent Requirements from the system Name Type Description Required copy object CopyRequrements include the requirements for copying the data false interface object Interface indicates the protocol and format expected by the data user true M4DApplication.spec.data[index].requirements.copy \u21a9 Parent CopyRequrements include the requirements for copying the data Name Type Description Required catalog object Catalog indicates that the data asset must be cataloged. false required boolean Required indicates that the data must be copied. false M4DApplication.spec.data[index].requirements.copy.catalog \u21a9 Parent Catalog indicates that the data asset must be cataloged. Name Type Description Required catalogID string CatalogID specifies the catalog where the data will be cataloged. false service string CatalogService specifies the datacatalog service that will be used for catalogging the data into. false M4DApplication.spec.data[index].requirements.interface \u21a9 Parent Interface indicates the protocol and format expected by the data user Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true M4DApplication.status \u21a9 Parent M4DApplicationStatus defines the observed state of M4DApplication. Name Type Description Required catalogedAssets map[string]string CatalogedAssets provide the new asset identifiers after being registered in the enterprise catalog It maps the original asset id to the cataloged asset id. false conditions []object Conditions represent the possible error and failure conditions false dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false generated object Generated resource identifier false observedGeneration integer ObservedGeneration is taken from the M4DApplication metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether the Blueprint status changed. false provisionedStorage map[string]object ProvisionedStorage maps a dataset (identified by AssetID) to the new provisioned bucket. It allows M4DApplication controller to manage buckets in case the spec has been modified, an error has occurred, or a delete event has been received. ProvisionedStorage has the information required to register the dataset once the owned plotter resource is ready false readEndpointsMap map[string]object ReadEndpointsMap maps an datasetID (after parsing from json to a string with dashes) to the endpoint spec from which the asset will be served to the application false ready boolean Ready is true if a blueprint has been successfully orchestrated false M4DApplication.status.conditions[index] \u21a9 Parent Condition describes the state of a M4DApplication at a certain point. Name Type Description Required message string Message contains the details of the current condition false status string Status of the condition: true or false true type string Type of the condition true M4DApplication.status.generated \u21a9 Parent Generated resource identifier Name Type Description Required appVersion integer Version of M4DApplication that has generated this resource true kind string Kind of the resource (Blueprint, Plotter) true name string Name of the resource true namespace string Namespace of the resource true M4DApplication.status.provisionedStorage[key] \u21a9 Parent DatasetDetails contain dataset connection and metadata required to register this dataset in the enterprise catalog Name Type Description Required datasetRef string Reference to a Dataset resource containing the request to provision storage false details object Dataset information false secretRef string Reference to a secret where the credentials are stored false M4DApplication.status.readEndpointsMap[key] \u21a9 Parent EndpointSpec is used both by the module creator and by the status of the m4dapplication Name Type Description Required hostname string Always equals the release name. Can be omitted. false port integer true scheme string For example: http, https, grpc, grpc+tls, jdbc:oracle:thin:@ etc true M4DModule \u21a9 Parent M4DModule is a description of an injectable component. the parameters it requires, as well as the specification of how to instantiate such a component. It is used as metadata only. There is no status nor reconciliation. Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string M4DModule true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object M4DModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. true M4DModule.spec \u21a9 Parent M4DModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. Name Type Description Required dependencies []object Other components that must be installed in order for this module to work false statusIndicators []object StatusIndicators allow to check status of a non-standard resource that can not be computed by helm/kstatus false capabilities object Capabilities declares what this module knows how to do and the types of data it knows how to handle true chart object Reference to a Helm chart that allows deployment of the resources required for this module true flows []enum Flows is a list of the types of capabilities supported by the module - copy, read, write true M4DModule.spec.dependencies[index] \u21a9 Parent Dependency details another component on which this module relies - i.e. a pre-requisit Name Type Description Required name string Name is the name of the dependent component true type enum Type provides information used in determining how to instantiate the component [module connector feature] true M4DModule.spec.statusIndicators[index] \u21a9 Parent ResourceStatusIndicator is used to determine the status of an orchestrated resource Name Type Description Required errorMessage string ErrorMessage specifies the resource field to check for an error, e.g. status.errorMsg false failureCondition string FailureCondition specifies a condition that indicates the resource failure It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) false kind string Kind provides information about the resource kind true successCondition string SuccessCondition specifies a condition that indicates that the resource is ready It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) true M4DModule.spec.capabilities \u21a9 Parent Capabilities declares what this module knows how to do and the types of data it knows how to handle Name Type Description Required actions []object Actions are the data transformations that the module supports false api object API indicates to the application how to access/write the data false supportedInterfaces []object Copy should have one or more instances in the list, and its content should have source and sink Read should have one or more instances in the list, each with source populated Write should have one or more instances in the list, each with sink populated TODO - In the future if we have a module type that doesn't interface directly with data then this list could be empty true M4DModule.spec.capabilities.actions[index] \u21a9 Parent SupportedAction declares an action that the module supports (action identifier and its scope) Name Type Description Required id string false level integer false M4DModule.spec.capabilities.api \u21a9 Parent API indicates to the application how to access/write the data Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false endpoint object EndpointSpec is used both by the module creator and by the status of the m4dapplication true protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true M4DModule.spec.capabilities.api.endpoint \u21a9 Parent EndpointSpec is used both by the module creator and by the status of the m4dapplication Name Type Description Required hostname string Always equals the release name. Can be omitted. false port integer true scheme string For example: http, https, grpc, grpc+tls, jdbc:oracle:thin:@ etc true M4DModule.spec.capabilities.supportedInterfaces[index] \u21a9 Parent ModuleInOut specifies the protocol and format of the data input and output by the module - if any Name Type Description Required sink object Sink specifies the output data protocol and format false source object Source specifies the input data protocol and format false flow enum Flow for which this interface is supported [copy read write] true M4DModule.spec.capabilities.supportedInterfaces[index].sink \u21a9 Parent Sink specifies the output data protocol and format Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true M4DModule.spec.capabilities.supportedInterfaces[index].source \u21a9 Parent Source specifies the input data protocol and format Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true M4DModule.spec.chart \u21a9 Parent Reference to a Helm chart that allows deployment of the resources required for this module Name Type Description Required values map[string]string Values to pass to helm chart installation false name string Name of helm chart true M4DStorageAccount \u21a9 Parent M4DStorageAccount defines a storage account used for copying data. Only S3 based storage is supported. It contains endpoint, region and a reference to the credentials a Owner of the asset is responsible to store the credentials Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string M4DStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object M4DStorageAccountSpec defines the desired state of M4DStorageAccount false status object M4DStorageAccountStatus defines the observed state of M4DStorageAccount false M4DStorageAccount.spec \u21a9 Parent M4DStorageAccountSpec defines the desired state of M4DStorageAccount Name Type Description Required endpoint string Endpoint true regions []string Regions true secretRef string A name of k8s secret deployed in the control plane. This secret includes secretKey and accessKey credentials for S3 bucket true Plotter \u21a9 Parent Plotter is the Schema for the plotters API Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string Plotter true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter installs the runtime environment (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. false status object PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring false Plotter.spec \u21a9 Parent PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter installs the runtime environment (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. Name Type Description Required blueprints map[string]object Blueprints structure represents remote blueprints mapped by the identifier of a cluster in which they will be running true selector object Selector enables to connect the resource to the application Should match the selector of the owner - M4DApplication CRD. true Plotter.spec.blueprints[key] \u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which is the runtime environment which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. The blueprint uses an \"argo like\" syntax which indicates the components and the flow of data between them as steps TODO: Add an indication of the communication relationships between the components Name Type Description Required entrypoint string true flow object DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow true templates []object true Plotter.spec.blueprints[key].flow \u21a9 Parent DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow Name Type Description Required name string true steps []object true Plotter.spec.blueprints[key].flow.steps[index] \u21a9 Parent FlowStep is one step indicates an instance of a module in the blueprint, It includes the name of the module template (spec) and the parameters received by the component instance that is initiated by the orchestrator. Name Type Description Required arguments object Arguments are the input parameters for a specific instance of a module. false name string Name is the name of the instance of the module. For example, if the application is named \"notebook\" and an implicitcopy module is deemed necessary. The FlowStep name would be notebook-implicitcopy. true template string Template is the name of the specification in the Blueprint describing how to instantiate a component indicated by the module. It is the name of a M4DModule CRD. For example: implicit-copy-db2wh-to-s3-latest true Plotter.spec.blueprints[key].flow.steps[index].arguments \u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required copy object CopyArgs are parameters specific to modules that copy data from one data store to another. false read []object ReadArgs are parameters that are specific to modules that enable an application to read data false write []object WriteArgs are parameters that are specific to modules that enable an application to write data false Plotter.spec.blueprints[key].flow.steps[index].arguments.copy \u21a9 Parent CopyArgs are parameters specific to modules that copy data from one data store to another. Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is copied. false destination object Destination is the data store to which the data will be copied true source object Source is the where the data currently resides true Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.destination \u21a9 Parent Destination is the data store to which the data will be copied Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.destination.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.source \u21a9 Parent Source is the where the data currently resides Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.source.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Plotter.spec.blueprints[key].flow.steps[index].arguments.read[index] \u21a9 Parent ReadModuleArgs define the input parameters for modules that read data from location A Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data false assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the M4DApplication resource true source object Source of the read path module true Plotter.spec.blueprints[key].flow.steps[index].arguments.read[index].source \u21a9 Parent Source of the read path module Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Plotter.spec.blueprints[key].flow.steps[index].arguments.read[index].source.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Plotter.spec.blueprints[key].flow.steps[index].arguments.write[index] \u21a9 Parent WriteModuleArgs define the input parameters for modules that write data to location B Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is written. false destination object Destination is the data store to which the data will be written true Plotter.spec.blueprints[key].flow.steps[index].arguments.write[index].destination \u21a9 Parent Destination is the data store to which the data will be written Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true Plotter.spec.blueprints[key].flow.steps[index].arguments.write[index].destination.vault \u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true Plotter.spec.blueprints[key].templates[index] \u21a9 Parent ComponentTemplate is a copy of a M4DModule Custom Resource. It contains the information necessary to instantiate a component in a FlowStep, which provides the functionality described by the module. There are 3 different module types. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true kind string Kind of k8s resource true name string Name of the template true Plotter.spec.blueprints[key].templates[index].chart \u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required values map[string]string Values to pass to helm chart installation false name string Name of helm chart true Plotter.spec.selector \u21a9 Parent Selector enables to connect the resource to the application Should match the selector of the owner - M4DApplication CRD. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false Plotter.spec.selector.matchExpressions[index] \u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true Plotter.status \u21a9 Parent PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring Name Type Description Required blueprints map[string]object false observedGeneration integer ObservedGeneration is taken from the Plotter metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated blueprints should be checked. false observedState object ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions false readyTimestamp string false Plotter.status.blueprints[key] \u21a9 Parent MetaBlueprint defines blueprint metadata (name, namespace) and status Name Type Description Required name string true namespace string true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring true Plotter.status.blueprints[key].status \u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. false observedState object ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false Plotter.status.blueprints[key].status.observedState \u21a9 Parent ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false Plotter.status.observedState \u21a9 Parent ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false katalog.m4d.ibm.com/v1alpha1 Resource Types: Asset Asset \u21a9 Parent Name Type Description Required apiVersion string katalog.m4d.ibm.com/v1alpha1 true kind string Asset true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object true Asset.spec \u21a9 Parent Name Type Description Required assetDetails object Asset details true assetMetadata object true secretRef object Reference to a Secret resource holding credentials for this asset true Asset.spec.assetDetails \u21a9 Parent Asset details Name Type Description Required dataFormat string false connection object Connection information true Asset.spec.assetDetails.connection \u21a9 Parent Connection information Name Type Description Required db2 object false kafka object false s3 object Connection information for S3 compatible object store false type enum [s3 db2 kafka] true Asset.spec.assetDetails.connection.db2 \u21a9 Parent Name Type Description Required database string false port string false ssl string false table string false url string false Asset.spec.assetDetails.connection.kafka \u21a9 Parent Name Type Description Required bootstrap_servers string false key_deserializer string false sasl_mechanism string false schema_registry string false security_protocol string false ssl_truststore string false ssl_truststore_password string false topic_name string false value_deserializer string false Asset.spec.assetDetails.connection.s3 \u21a9 Parent Connection information for S3 compatible object store Name Type Description Required region string false bucket string true endpoint string true objectKey string true Asset.spec.assetMetadata \u21a9 Parent Name Type Description Required componentsMetadata map[string]object metadata for each component in asset (e.g., column) false geography string false namedMetadata map[string]string false owner string false tags []string Tags associated with the asset false Asset.spec.assetMetadata.componentsMetadata[key] \u21a9 Parent Name Type Description Required componentType string false namedMetadata map[string]string Named terms, that exist in Catalog toxonomy and the values for these terms for columns we will have \"SchemaDetails\" key, that will include technical schema details for this column TODO: Consider create special field for schema outside of metadata false tags []string Tags - can be any free text added to a component (no taxonomy) false Asset.spec.secretRef \u21a9 Parent Reference to a Secret resource holding credentials for this asset Name Type Description Required name string Name of the Secret resource (must exist in the same namespace) true motion.m4d.ibm.com/v1alpha1 Resource Types: BatchTransfer StreamTransfer BatchTransfer \u21a9 Parent BatchTransfer is the Schema for the batchtransfers API Name Type Description Required apiVersion string motion.m4d.ibm.com/v1alpha1 true kind string BatchTransfer true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BatchTransferSpec defines the state of a BatchTransfer. The state includes source/destination specification, a schedule and the means by which data movement is to be conducted. The means is given as a kubernetes job description. In addition, the state also contains a sketch of a transformation instruction. In future releases, the transformation description should be specified in a separate CRD. false status object BatchTransferStatus defines the observed state of BatchTransfer This includes a reference to the job that implements the movement as well as the last schedule time. What is missing: Extended status information such as: - number of records moved - technical meta-data false BatchTransfer.spec \u21a9 Parent BatchTransferSpec defines the state of a BatchTransfer. The state includes source/destination specification, a schedule and the means by which data movement is to be conducted. The means is given as a kubernetes job description. In addition, the state also contains a sketch of a transformation instruction. In future releases, the transformation description should be specified in a separate CRD. Name Type Description Required failedJobHistoryLimit integer Maximal number of failed Kubernetes job objects that should be kept. This property will be defaulted by the webhook if not set. false flowType enum Data flow type that specifies if this is a stream or a batch workflow [Batch Stream] false image string Image that should be used for the actual batch job. This is usually a datamover image. This property will be defaulted by the webhook if not set. false imagePullPolicy string Image pull policy that should be used for the actual job. This property will be defaulted by the webhook if not set. false maxFailedRetries integer Maximal number of failed retries until the batch job should stop trying. This property will be defaulted by the webhook if not set. false noFinalizer boolean If this batch job instance should have a finalizer or not. This property will be defaulted by the webhook if not set. false readDataType enum Data type of the data that is read from source (log data or change data) [LogData ChangeData] false schedule string Cron schedule if this BatchTransfer job should run on a regular schedule. Values are specified like cron job schedules. A good translation to human language can be found here https://crontab.guru/ false secretProviderRole string Secret provider role that should be used for the actual job. This property will be defaulted by the webhook if not set. false secretProviderURL string Secret provider url that should be used for the actual job. This property will be defaulted by the webhook if not set. false spark object Optional Spark configuration for tuning false successfulJobHistoryLimit integer Maximal number of successful Kubernetes job objects that should be kept. This property will be defaulted by the webhook if not set. false suspend boolean If this batch job instance is run on a schedule the regular schedule can be suspended with this property. This property will be defaulted by the webhook if not set. false transformation []object Transformations to be applied to the source data before writing to destination false writeDataType enum Data type of how the data should be written to the target (log data or change data) [LogData ChangeData] false writeOperation enum Write operation that should be performed when writing (overwrite,append,update) Caution: Some write operations are only available for batch and some only for stream. [Overwrite Append Update] false destination object Destination data store for this batch job true source object Source data store for this batch job true BatchTransfer.spec.spark \u21a9 Parent Optional Spark configuration for tuning Name Type Description Required appName string Name of the transaction. Mainly used for debugging and lineage tracking. false driverCores integer Number of cores that the driver should use false driverMemory integer Memory that the driver should have false executorCores integer Number of cores that each executor should have false executorMemory string Memory that each executor should have false image string Image to be used for executors false imagePullPolicy string Image pull policy to be used for executor false numExecutors integer Number of executors to be started false options map[string]string Additional options for Spark configuration. false shufflePartitions integer Number of shuffle partitions for Spark false BatchTransfer.spec.transformation[index] \u21a9 Parent to be refined... Name Type Description Required action enum Transformation action that should be performed. [RemoveColumns EncryptColumns DigestColumns RedactColumns SampleRows FilterRows] false columns []string Columns that are involved in this action. This property is optional as for some actions no columns have to be specified. E.g. filter is a row based transformation. false name string Name of the transaction. Mainly used for debugging and lineage tracking. false options map[string]string Additional options for this transformation. false BatchTransfer.spec.destination \u21a9 Parent Destination data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false BatchTransfer.spec.destination.cloudant \u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true BatchTransfer.spec.destination.cloudant.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.destination.database \u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true BatchTransfer.spec.destination.database.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.destination.kafka \u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true BatchTransfer.spec.destination.kafka.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.destination.s3 \u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true BatchTransfer.spec.destination.s3.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.source \u21a9 Parent Source data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false BatchTransfer.spec.source.cloudant \u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true BatchTransfer.spec.source.cloudant.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.source.database \u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true BatchTransfer.spec.source.database.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.source.kafka \u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true BatchTransfer.spec.source.kafka.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.spec.source.s3 \u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true BatchTransfer.spec.source.s3.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true BatchTransfer.status \u21a9 Parent BatchTransferStatus defines the observed state of BatchTransfer This includes a reference to the job that implements the movement as well as the last schedule time. What is missing: Extended status information such as: - number of records moved - technical meta-data Name Type Description Required active object A pointer to the currently running job (or nil) false error string false lastCompleted object ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . false lastFailed object ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . false lastRecordTime string false lastScheduleTime string Information when was the last time the job was successfully scheduled. false lastSuccessTime string false numRecords integer false status enum [STARTING RUNNING SUCCEEDED FAILED] false BatchTransfer.status.active \u21a9 Parent A pointer to the currently running job (or nil) Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false BatchTransfer.status.lastCompleted \u21a9 Parent ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false BatchTransfer.status.lastFailed \u21a9 Parent ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false StreamTransfer \u21a9 Parent StreamTransfer is the Schema for the streamtransfers API Name Type Description Required apiVersion string motion.m4d.ibm.com/v1alpha1 true kind string StreamTransfer true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object StreamTransferSpec defines the desired state of StreamTransfer false status object StreamTransferStatus defines the observed state of StreamTransfer false StreamTransfer.spec \u21a9 Parent StreamTransferSpec defines the desired state of StreamTransfer Name Type Description Required flowType enum Data flow type that specifies if this is a stream or a batch workflow [Batch Stream] false image string Image that should be used for the actual batch job. This is usually a datamover image. This property will be defaulted by the webhook if not set. false imagePullPolicy string Image pull policy that should be used for the actual job. This property will be defaulted by the webhook if not set. false noFinalizer boolean If this batch job instance should have a finalizer or not. This property will be defaulted by the webhook if not set. false readDataType enum Data type of the data that is read from source (log data or change data) [LogData ChangeData] false secretProviderRole string Secret provider role that should be used for the actual job. This property will be defaulted by the webhook if not set. false secretProviderURL string Secret provider url that should be used for the actual job. This property will be defaulted by the webhook if not set. false suspend boolean If this batch job instance is run on a schedule the regular schedule can be suspended with this property. This property will be defaulted by the webhook if not set. false transformation []object Transformations to be applied to the source data before writing to destination false triggerInterval string Interval in which the Micro batches of this stream should be triggered The default is '5 seconds'. false writeDataType enum Data type of how the data should be written to the target (log data or change data) [LogData ChangeData] false writeOperation enum Write operation that should be performed when writing (overwrite,append,update) Caution: Some write operations are only available for batch and some only for stream. [Overwrite Append Update] false destination object Destination data store for this batch job true source object Source data store for this batch job true StreamTransfer.spec.transformation[index] \u21a9 Parent to be refined... Name Type Description Required action enum Transformation action that should be performed. [RemoveColumns EncryptColumns DigestColumns RedactColumns SampleRows FilterRows] false columns []string Columns that are involved in this action. This property is optional as for some actions no columns have to be specified. E.g. filter is a row based transformation. false name string Name of the transaction. Mainly used for debugging and lineage tracking. false options map[string]string Additional options for this transformation. false StreamTransfer.spec.destination \u21a9 Parent Destination data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false StreamTransfer.spec.destination.cloudant \u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true StreamTransfer.spec.destination.cloudant.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.destination.database \u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true StreamTransfer.spec.destination.database.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.destination.kafka \u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true StreamTransfer.spec.destination.kafka.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.destination.s3 \u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true StreamTransfer.spec.destination.s3.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.source \u21a9 Parent Source data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false StreamTransfer.spec.source.cloudant \u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true StreamTransfer.spec.source.cloudant.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.source.database \u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true StreamTransfer.spec.source.database.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.source.kafka \u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true StreamTransfer.spec.source.kafka.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.spec.source.s3 \u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true StreamTransfer.spec.source.s3.vault \u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true StreamTransfer.status \u21a9 Parent StreamTransferStatus defines the observed state of StreamTransfer Name Type Description Required active object A pointer to the currently running job (or nil) false error string false status enum [STARTING RUNNING STOPPED FAILING] false StreamTransfer.status.active \u21a9 Parent A pointer to the currently running job (or nil) Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false","title":"API Reference"},{"location":"reference/crds.html#api-reference","text":"Packages: app.m4d.ibm.com/v1alpha1 Blueprint M4DApplication M4DModule M4DStorageAccount Plotter katalog.m4d.ibm.com/v1alpha1 Asset motion.m4d.ibm.com/v1alpha1 BatchTransfer StreamTransfer","title":"API Reference"},{"location":"reference/crds.html#appm4dibmcomv1alpha1","text":"Resource Types: Blueprint M4DApplication M4DModule M4DStorageAccount Plotter","title":"app.m4d.ibm.com/v1alpha1"},{"location":"reference/crds.html#blueprint","text":"\u21a9 Parent Blueprint is the Schema for the blueprints API Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string Blueprint true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BlueprintSpec defines the desired state of Blueprint, which is the runtime environment which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. The blueprint uses an \"argo like\" syntax which indicates the components and the flow of data between them as steps TODO: Add an indication of the communication relationships between the components false status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring false","title":"Blueprint"},{"location":"reference/crds.html#blueprintspec","text":"\u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which is the runtime environment which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. The blueprint uses an \"argo like\" syntax which indicates the components and the flow of data between them as steps TODO: Add an indication of the communication relationships between the components Name Type Description Required entrypoint string true flow object DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow true templates []object true","title":"Blueprint.spec"},{"location":"reference/crds.html#blueprintspecflow","text":"\u21a9 Parent DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow Name Type Description Required name string true steps []object true","title":"Blueprint.spec.flow"},{"location":"reference/crds.html#blueprintspecflowstepsindex","text":"\u21a9 Parent FlowStep is one step indicates an instance of a module in the blueprint, It includes the name of the module template (spec) and the parameters received by the component instance that is initiated by the orchestrator. Name Type Description Required arguments object Arguments are the input parameters for a specific instance of a module. false name string Name is the name of the instance of the module. For example, if the application is named \"notebook\" and an implicitcopy module is deemed necessary. The FlowStep name would be notebook-implicitcopy. true template string Template is the name of the specification in the Blueprint describing how to instantiate a component indicated by the module. It is the name of a M4DModule CRD. For example: implicit-copy-db2wh-to-s3-latest true","title":"Blueprint.spec.flow.steps[index]"},{"location":"reference/crds.html#blueprintspecflowstepsindexarguments","text":"\u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required copy object CopyArgs are parameters specific to modules that copy data from one data store to another. false read []object ReadArgs are parameters that are specific to modules that enable an application to read data false write []object WriteArgs are parameters that are specific to modules that enable an application to write data false","title":"Blueprint.spec.flow.steps[index].arguments"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentscopy","text":"\u21a9 Parent CopyArgs are parameters specific to modules that copy data from one data store to another. Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is copied. false destination object Destination is the data store to which the data will be copied true source object Source is the where the data currently resides true","title":"Blueprint.spec.flow.steps[index].arguments.copy"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentscopydestination","text":"\u21a9 Parent Destination is the data store to which the data will be copied Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Blueprint.spec.flow.steps[index].arguments.copy.destination"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentscopydestinationvault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Blueprint.spec.flow.steps[index].arguments.copy.destination.vault"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentscopysource","text":"\u21a9 Parent Source is the where the data currently resides Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Blueprint.spec.flow.steps[index].arguments.copy.source"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentscopysourcevault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Blueprint.spec.flow.steps[index].arguments.copy.source.vault"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentsreadindex","text":"\u21a9 Parent ReadModuleArgs define the input parameters for modules that read data from location A Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data false assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the M4DApplication resource true source object Source of the read path module true","title":"Blueprint.spec.flow.steps[index].arguments.read[index]"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentsreadindexsource","text":"\u21a9 Parent Source of the read path module Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Blueprint.spec.flow.steps[index].arguments.read[index].source"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentsreadindexsourcevault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Blueprint.spec.flow.steps[index].arguments.read[index].source.vault"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentswriteindex","text":"\u21a9 Parent WriteModuleArgs define the input parameters for modules that write data to location B Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is written. false destination object Destination is the data store to which the data will be written true","title":"Blueprint.spec.flow.steps[index].arguments.write[index]"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentswriteindexdestination","text":"\u21a9 Parent Destination is the data store to which the data will be written Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Blueprint.spec.flow.steps[index].arguments.write[index].destination"},{"location":"reference/crds.html#blueprintspecflowstepsindexargumentswriteindexdestinationvault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Blueprint.spec.flow.steps[index].arguments.write[index].destination.vault"},{"location":"reference/crds.html#blueprintspectemplatesindex","text":"\u21a9 Parent ComponentTemplate is a copy of a M4DModule Custom Resource. It contains the information necessary to instantiate a component in a FlowStep, which provides the functionality described by the module. There are 3 different module types. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true kind string Kind of k8s resource true name string Name of the template true","title":"Blueprint.spec.templates[index]"},{"location":"reference/crds.html#blueprintspectemplatesindexchart","text":"\u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required values map[string]string Values to pass to helm chart installation false name string Name of helm chart true","title":"Blueprint.spec.templates[index].chart"},{"location":"reference/crds.html#blueprintstatus","text":"\u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. false observedState object ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false","title":"Blueprint.status"},{"location":"reference/crds.html#blueprintstatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Blueprint.status.observedState"},{"location":"reference/crds.html#m4dapplication","text":"\u21a9 Parent M4DApplication provides information about the application being used by a Data Scientist, the nature of the processing, and the data sets that the Data Scientist has chosen for processing by the application. The M4DApplication controller (aka pilot) obtains instructions regarding any governance related changes that must be performed on the data, identifies the modules capable of performing such changes, and finally generates the Blueprint which defines the secure runtime environment and all the components in it. This runtime environment provides the Data Scientist's application with access to the data requested in a secure manner and without having to provide any credentials for the data sets. The credentials are obtained automatically by the manager from an external credential management system, which may or may not be part of a data catalog. Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string M4DApplication true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object M4DApplicationSpec defines the desired state of M4DApplication. false status object M4DApplicationStatus defines the observed state of M4DApplication. false","title":"M4DApplication"},{"location":"reference/crds.html#m4dapplicationspec","text":"\u21a9 Parent M4DApplicationSpec defines the desired state of M4DApplication. Name Type Description Required secretRef string SecretRef points to the secret that holds credentials for each system the user has been authenticated with. The secret is deployed in M4dApplication namespace. false selector object Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. false appInfo map[string]string AppInfo contains information describing the reasons for the processing that will be done by the Data Scientist's application. true data []object Data contains the identifiers of the data to be used by the Data Scientist's application, and the protocol used to access it and the format expected. true","title":"M4DApplication.spec"},{"location":"reference/crds.html#m4dapplicationspecselector","text":"\u21a9 Parent Selector enables to connect the resource to the application Application labels should match the labels in the selector. For some flows the selector may not be used. Name Type Description Required clusterName string Cluster name false workloadSelector object WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. true","title":"M4DApplication.spec.selector"},{"location":"reference/crds.html#m4dapplicationspecselectorworkloadselector","text":"\u21a9 Parent WorkloadSelector enables to connect the resource to the application Application labels should match the labels in the selector. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"M4DApplication.spec.selector.workloadSelector"},{"location":"reference/crds.html#m4dapplicationspecselectorworkloadselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true","title":"M4DApplication.spec.selector.workloadSelector.matchExpressions[index]"},{"location":"reference/crds.html#m4dapplicationspecdataindex","text":"\u21a9 Parent DataContext indicates data set chosen by the Data Scientist to be used by his application, and includes information about the data format and technologies used by the application to access the data. Name Type Description Required catalogService string CatalogService represents the catalog service for accessing the requested dataset. If not specified, the enterprise catalog service will be used. false dataSetID string DataSetID is a unique identifier of the dataset chosen from the data catalog for processing by the data user application. true requirements object Requirements from the system true","title":"M4DApplication.spec.data[index]"},{"location":"reference/crds.html#m4dapplicationspecdataindexrequirements","text":"\u21a9 Parent Requirements from the system Name Type Description Required copy object CopyRequrements include the requirements for copying the data false interface object Interface indicates the protocol and format expected by the data user true","title":"M4DApplication.spec.data[index].requirements"},{"location":"reference/crds.html#m4dapplicationspecdataindexrequirementscopy","text":"\u21a9 Parent CopyRequrements include the requirements for copying the data Name Type Description Required catalog object Catalog indicates that the data asset must be cataloged. false required boolean Required indicates that the data must be copied. false","title":"M4DApplication.spec.data[index].requirements.copy"},{"location":"reference/crds.html#m4dapplicationspecdataindexrequirementscopycatalog","text":"\u21a9 Parent Catalog indicates that the data asset must be cataloged. Name Type Description Required catalogID string CatalogID specifies the catalog where the data will be cataloged. false service string CatalogService specifies the datacatalog service that will be used for catalogging the data into. false","title":"M4DApplication.spec.data[index].requirements.copy.catalog"},{"location":"reference/crds.html#m4dapplicationspecdataindexrequirementsinterface","text":"\u21a9 Parent Interface indicates the protocol and format expected by the data user Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true","title":"M4DApplication.spec.data[index].requirements.interface"},{"location":"reference/crds.html#m4dapplicationstatus","text":"\u21a9 Parent M4DApplicationStatus defines the observed state of M4DApplication. Name Type Description Required catalogedAssets map[string]string CatalogedAssets provide the new asset identifiers after being registered in the enterprise catalog It maps the original asset id to the cataloged asset id. false conditions []object Conditions represent the possible error and failure conditions false dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false generated object Generated resource identifier false observedGeneration integer ObservedGeneration is taken from the M4DApplication metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether the Blueprint status changed. false provisionedStorage map[string]object ProvisionedStorage maps a dataset (identified by AssetID) to the new provisioned bucket. It allows M4DApplication controller to manage buckets in case the spec has been modified, an error has occurred, or a delete event has been received. ProvisionedStorage has the information required to register the dataset once the owned plotter resource is ready false readEndpointsMap map[string]object ReadEndpointsMap maps an datasetID (after parsing from json to a string with dashes) to the endpoint spec from which the asset will be served to the application false ready boolean Ready is true if a blueprint has been successfully orchestrated false","title":"M4DApplication.status"},{"location":"reference/crds.html#m4dapplicationstatusconditionsindex","text":"\u21a9 Parent Condition describes the state of a M4DApplication at a certain point. Name Type Description Required message string Message contains the details of the current condition false status string Status of the condition: true or false true type string Type of the condition true","title":"M4DApplication.status.conditions[index]"},{"location":"reference/crds.html#m4dapplicationstatusgenerated","text":"\u21a9 Parent Generated resource identifier Name Type Description Required appVersion integer Version of M4DApplication that has generated this resource true kind string Kind of the resource (Blueprint, Plotter) true name string Name of the resource true namespace string Namespace of the resource true","title":"M4DApplication.status.generated"},{"location":"reference/crds.html#m4dapplicationstatusprovisionedstoragekey","text":"\u21a9 Parent DatasetDetails contain dataset connection and metadata required to register this dataset in the enterprise catalog Name Type Description Required datasetRef string Reference to a Dataset resource containing the request to provision storage false details object Dataset information false secretRef string Reference to a secret where the credentials are stored false","title":"M4DApplication.status.provisionedStorage[key]"},{"location":"reference/crds.html#m4dapplicationstatusreadendpointsmapkey","text":"\u21a9 Parent EndpointSpec is used both by the module creator and by the status of the m4dapplication Name Type Description Required hostname string Always equals the release name. Can be omitted. false port integer true scheme string For example: http, https, grpc, grpc+tls, jdbc:oracle:thin:@ etc true","title":"M4DApplication.status.readEndpointsMap[key]"},{"location":"reference/crds.html#m4dmodule","text":"\u21a9 Parent M4DModule is a description of an injectable component. the parameters it requires, as well as the specification of how to instantiate such a component. It is used as metadata only. There is no status nor reconciliation. Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string M4DModule true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object M4DModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. true","title":"M4DModule"},{"location":"reference/crds.html#m4dmodulespec","text":"\u21a9 Parent M4DModuleSpec contains the info common to all modules, which are one of the components that process, load, write, audit, monitor the data used by the data scientist's application. Name Type Description Required dependencies []object Other components that must be installed in order for this module to work false statusIndicators []object StatusIndicators allow to check status of a non-standard resource that can not be computed by helm/kstatus false capabilities object Capabilities declares what this module knows how to do and the types of data it knows how to handle true chart object Reference to a Helm chart that allows deployment of the resources required for this module true flows []enum Flows is a list of the types of capabilities supported by the module - copy, read, write true","title":"M4DModule.spec"},{"location":"reference/crds.html#m4dmodulespecdependenciesindex","text":"\u21a9 Parent Dependency details another component on which this module relies - i.e. a pre-requisit Name Type Description Required name string Name is the name of the dependent component true type enum Type provides information used in determining how to instantiate the component [module connector feature] true","title":"M4DModule.spec.dependencies[index]"},{"location":"reference/crds.html#m4dmodulespecstatusindicatorsindex","text":"\u21a9 Parent ResourceStatusIndicator is used to determine the status of an orchestrated resource Name Type Description Required errorMessage string ErrorMessage specifies the resource field to check for an error, e.g. status.errorMsg false failureCondition string FailureCondition specifies a condition that indicates the resource failure It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) false kind string Kind provides information about the resource kind true successCondition string SuccessCondition specifies a condition that indicates that the resource is ready It uses kubernetes label selection syntax (https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) true","title":"M4DModule.spec.statusIndicators[index]"},{"location":"reference/crds.html#m4dmodulespeccapabilities","text":"\u21a9 Parent Capabilities declares what this module knows how to do and the types of data it knows how to handle Name Type Description Required actions []object Actions are the data transformations that the module supports false api object API indicates to the application how to access/write the data false supportedInterfaces []object Copy should have one or more instances in the list, and its content should have source and sink Read should have one or more instances in the list, each with source populated Write should have one or more instances in the list, each with sink populated TODO - In the future if we have a module type that doesn't interface directly with data then this list could be empty true","title":"M4DModule.spec.capabilities"},{"location":"reference/crds.html#m4dmodulespeccapabilitiesactionsindex","text":"\u21a9 Parent SupportedAction declares an action that the module supports (action identifier and its scope) Name Type Description Required id string false level integer false","title":"M4DModule.spec.capabilities.actions[index]"},{"location":"reference/crds.html#m4dmodulespeccapabilitiesapi","text":"\u21a9 Parent API indicates to the application how to access/write the data Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false endpoint object EndpointSpec is used both by the module creator and by the status of the m4dapplication true protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true","title":"M4DModule.spec.capabilities.api"},{"location":"reference/crds.html#m4dmodulespeccapabilitiesapiendpoint","text":"\u21a9 Parent EndpointSpec is used both by the module creator and by the status of the m4dapplication Name Type Description Required hostname string Always equals the release name. Can be omitted. false port integer true scheme string For example: http, https, grpc, grpc+tls, jdbc:oracle:thin:@ etc true","title":"M4DModule.spec.capabilities.api.endpoint"},{"location":"reference/crds.html#m4dmodulespeccapabilitiessupportedinterfacesindex","text":"\u21a9 Parent ModuleInOut specifies the protocol and format of the data input and output by the module - if any Name Type Description Required sink object Sink specifies the output data protocol and format false source object Source specifies the input data protocol and format false flow enum Flow for which this interface is supported [copy read write] true","title":"M4DModule.spec.capabilities.supportedInterfaces[index]"},{"location":"reference/crds.html#m4dmodulespeccapabilitiessupportedinterfacesindexsink","text":"\u21a9 Parent Sink specifies the output data protocol and format Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true","title":"M4DModule.spec.capabilities.supportedInterfaces[index].sink"},{"location":"reference/crds.html#m4dmodulespeccapabilitiessupportedinterfacesindexsource","text":"\u21a9 Parent Source specifies the input data protocol and format Name Type Description Required dataformat enum DataFormatType defines data format type [parquet table csv json avro orc binary arrow] false protocol enum IFProtocol defines interface protocol for data transactions [s3 kafka jdbc-db2 m4d-arrow-flight] true","title":"M4DModule.spec.capabilities.supportedInterfaces[index].source"},{"location":"reference/crds.html#m4dmodulespecchart","text":"\u21a9 Parent Reference to a Helm chart that allows deployment of the resources required for this module Name Type Description Required values map[string]string Values to pass to helm chart installation false name string Name of helm chart true","title":"M4DModule.spec.chart"},{"location":"reference/crds.html#m4dstorageaccount","text":"\u21a9 Parent M4DStorageAccount defines a storage account used for copying data. Only S3 based storage is supported. It contains endpoint, region and a reference to the credentials a Owner of the asset is responsible to store the credentials Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string M4DStorageAccount true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object M4DStorageAccountSpec defines the desired state of M4DStorageAccount false status object M4DStorageAccountStatus defines the observed state of M4DStorageAccount false","title":"M4DStorageAccount"},{"location":"reference/crds.html#m4dstorageaccountspec","text":"\u21a9 Parent M4DStorageAccountSpec defines the desired state of M4DStorageAccount Name Type Description Required endpoint string Endpoint true regions []string Regions true secretRef string A name of k8s secret deployed in the control plane. This secret includes secretKey and accessKey credentials for S3 bucket true","title":"M4DStorageAccount.spec"},{"location":"reference/crds.html#plotter","text":"\u21a9 Parent Plotter is the Schema for the plotters API Name Type Description Required apiVersion string app.m4d.ibm.com/v1alpha1 true kind string Plotter true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter installs the runtime environment (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. false status object PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring false","title":"Plotter"},{"location":"reference/crds.html#plotterspec","text":"\u21a9 Parent PlotterSpec defines the desired state of Plotter, which is applied in a multi-clustered environment. Plotter installs the runtime environment (as blueprints running on remote clusters) which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. Name Type Description Required blueprints map[string]object Blueprints structure represents remote blueprints mapped by the identifier of a cluster in which they will be running true selector object Selector enables to connect the resource to the application Should match the selector of the owner - M4DApplication CRD. true","title":"Plotter.spec"},{"location":"reference/crds.html#plotterspecblueprintskey","text":"\u21a9 Parent BlueprintSpec defines the desired state of Blueprint, which is the runtime environment which provides the Data Scientist's application with secure and governed access to the data requested in the M4DApplication. The blueprint uses an \"argo like\" syntax which indicates the components and the flow of data between them as steps TODO: Add an indication of the communication relationships between the components Name Type Description Required entrypoint string true flow object DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow true templates []object true","title":"Plotter.spec.blueprints[key]"},{"location":"reference/crds.html#plotterspecblueprintskeyflow","text":"\u21a9 Parent DataFlow indicates the flow of the data between the components Currently we assume this is linear and thus use steps, but other more complex graphs could be defined as per how it is done in argo workflow Name Type Description Required name string true steps []object true","title":"Plotter.spec.blueprints[key].flow"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindex","text":"\u21a9 Parent FlowStep is one step indicates an instance of a module in the blueprint, It includes the name of the module template (spec) and the parameters received by the component instance that is initiated by the orchestrator. Name Type Description Required arguments object Arguments are the input parameters for a specific instance of a module. false name string Name is the name of the instance of the module. For example, if the application is named \"notebook\" and an implicitcopy module is deemed necessary. The FlowStep name would be notebook-implicitcopy. true template string Template is the name of the specification in the Blueprint describing how to instantiate a component indicated by the module. It is the name of a M4DModule CRD. For example: implicit-copy-db2wh-to-s3-latest true","title":"Plotter.spec.blueprints[key].flow.steps[index]"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexarguments","text":"\u21a9 Parent Arguments are the input parameters for a specific instance of a module. Name Type Description Required copy object CopyArgs are parameters specific to modules that copy data from one data store to another. false read []object ReadArgs are parameters that are specific to modules that enable an application to read data false write []object WriteArgs are parameters that are specific to modules that enable an application to write data false","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentscopy","text":"\u21a9 Parent CopyArgs are parameters specific to modules that copy data from one data store to another. Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is copied. false destination object Destination is the data store to which the data will be copied true source object Source is the where the data currently resides true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.copy"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentscopydestination","text":"\u21a9 Parent Destination is the data store to which the data will be copied Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.destination"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentscopydestinationvault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.destination.vault"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentscopysource","text":"\u21a9 Parent Source is the where the data currently resides Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.source"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentscopysourcevault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.copy.source.vault"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentsreadindex","text":"\u21a9 Parent ReadModuleArgs define the input parameters for modules that read data from location A Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data false assetID string AssetID identifies the asset to be used for accessing the data when it is ready It is copied from the M4DApplication resource true source object Source of the read path module true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.read[index]"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentsreadindexsource","text":"\u21a9 Parent Source of the read path module Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.read[index].source"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentsreadindexsourcevault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.read[index].source.vault"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentswriteindex","text":"\u21a9 Parent WriteModuleArgs define the input parameters for modules that write data to location B Name Type Description Required transformations []object Transformations are different types of processing that may be done to the data as it is written. false destination object Destination is the data store to which the data will be written true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.write[index]"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentswriteindexdestination","text":"\u21a9 Parent Destination is the data store to which the data will be written Name Type Description Required connection object Connection has the relevant details for accesing the data (url, table, ssl, etc.) true format string Format represents data format (e.g. parquet) as received from catalog connectors true vault object Holds details for retrieving credentials by the modules from Vault store. true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.write[index].destination"},{"location":"reference/crds.html#plotterspecblueprintskeyflowstepsindexargumentswriteindexdestinationvault","text":"\u21a9 Parent Holds details for retrieving credentials by the modules from Vault store. Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"Plotter.spec.blueprints[key].flow.steps[index].arguments.write[index].destination.vault"},{"location":"reference/crds.html#plotterspecblueprintskeytemplatesindex","text":"\u21a9 Parent ComponentTemplate is a copy of a M4DModule Custom Resource. It contains the information necessary to instantiate a component in a FlowStep, which provides the functionality described by the module. There are 3 different module types. Name Type Description Required chart object Chart contains the location of the helm chart with info detailing how to deploy true kind string Kind of k8s resource true name string Name of the template true","title":"Plotter.spec.blueprints[key].templates[index]"},{"location":"reference/crds.html#plotterspecblueprintskeytemplatesindexchart","text":"\u21a9 Parent Chart contains the location of the helm chart with info detailing how to deploy Name Type Description Required values map[string]string Values to pass to helm chart installation false name string Name of helm chart true","title":"Plotter.spec.blueprints[key].templates[index].chart"},{"location":"reference/crds.html#plotterspecselector","text":"\u21a9 Parent Selector enables to connect the resource to the application Should match the selector of the owner - M4DApplication CRD. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"Plotter.spec.selector"},{"location":"reference/crds.html#plotterspecselectormatchexpressionsindex","text":"\u21a9 Parent A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true","title":"Plotter.spec.selector.matchExpressions[index]"},{"location":"reference/crds.html#plotterstatus","text":"\u21a9 Parent PlotterStatus defines the observed state of Plotter This includes readiness, error message, and indicators received from blueprint resources owned by the Plotter for cleanup and status monitoring Name Type Description Required blueprints map[string]object false observedGeneration integer ObservedGeneration is taken from the Plotter metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated blueprints should be checked. false observedState object ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions false readyTimestamp string false","title":"Plotter.status"},{"location":"reference/crds.html#plotterstatusblueprintskey","text":"\u21a9 Parent MetaBlueprint defines blueprint metadata (name, namespace) and status Name Type Description Required name string true namespace string true status object BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring true","title":"Plotter.status.blueprints[key]"},{"location":"reference/crds.html#plotterstatusblueprintskeystatus","text":"\u21a9 Parent BlueprintStatus defines the observed state of Blueprint This includes readiness, error message, and indicators forthe Kubernetes resources owned by the Blueprint for cleanup and status monitoring Name Type Description Required observedGeneration integer ObservedGeneration is taken from the Blueprint metadata. This is used to determine during reconcile whether reconcile was called because the desired state changed, or whether status of the allocated resources should be checked. false observedState object ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions false releases map[string]integer Releases map each release to the observed generation of the blueprint containing this release. At the end of reconcile, each release should be mapped to the latest blueprint version or be uninstalled. false","title":"Plotter.status.blueprints[key].status"},{"location":"reference/crds.html#plotterstatusblueprintskeystatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.blueprints[key].status.observedState"},{"location":"reference/crds.html#plotterstatusobservedstate","text":"\u21a9 Parent ObservedState includes information to be reported back to the M4DApplication resource It includes readiness and error indications, as well as user instructions Name Type Description Required dataAccessInstructions string DataAccessInstructions indicate how the data user or his application may access the data. Instructions are available upon successful orchestration. false error string Error indicates that there has been an error to orchestrate the modules and provides the error message false ready boolean Ready represents that the modules have been orchestrated successfully and the data is ready for usage false","title":"Plotter.status.observedState"},{"location":"reference/crds.html#katalogm4dibmcomv1alpha1","text":"Resource Types: Asset","title":"katalog.m4d.ibm.com/v1alpha1"},{"location":"reference/crds.html#asset","text":"\u21a9 Parent Name Type Description Required apiVersion string katalog.m4d.ibm.com/v1alpha1 true kind string Asset true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object true","title":"Asset"},{"location":"reference/crds.html#assetspec","text":"\u21a9 Parent Name Type Description Required assetDetails object Asset details true assetMetadata object true secretRef object Reference to a Secret resource holding credentials for this asset true","title":"Asset.spec"},{"location":"reference/crds.html#assetspecassetdetails","text":"\u21a9 Parent Asset details Name Type Description Required dataFormat string false connection object Connection information true","title":"Asset.spec.assetDetails"},{"location":"reference/crds.html#assetspecassetdetailsconnection","text":"\u21a9 Parent Connection information Name Type Description Required db2 object false kafka object false s3 object Connection information for S3 compatible object store false type enum [s3 db2 kafka] true","title":"Asset.spec.assetDetails.connection"},{"location":"reference/crds.html#assetspecassetdetailsconnectiondb2","text":"\u21a9 Parent Name Type Description Required database string false port string false ssl string false table string false url string false","title":"Asset.spec.assetDetails.connection.db2"},{"location":"reference/crds.html#assetspecassetdetailsconnectionkafka","text":"\u21a9 Parent Name Type Description Required bootstrap_servers string false key_deserializer string false sasl_mechanism string false schema_registry string false security_protocol string false ssl_truststore string false ssl_truststore_password string false topic_name string false value_deserializer string false","title":"Asset.spec.assetDetails.connection.kafka"},{"location":"reference/crds.html#assetspecassetdetailsconnections3","text":"\u21a9 Parent Connection information for S3 compatible object store Name Type Description Required region string false bucket string true endpoint string true objectKey string true","title":"Asset.spec.assetDetails.connection.s3"},{"location":"reference/crds.html#assetspecassetmetadata","text":"\u21a9 Parent Name Type Description Required componentsMetadata map[string]object metadata for each component in asset (e.g., column) false geography string false namedMetadata map[string]string false owner string false tags []string Tags associated with the asset false","title":"Asset.spec.assetMetadata"},{"location":"reference/crds.html#assetspecassetmetadatacomponentsmetadatakey","text":"\u21a9 Parent Name Type Description Required componentType string false namedMetadata map[string]string Named terms, that exist in Catalog toxonomy and the values for these terms for columns we will have \"SchemaDetails\" key, that will include technical schema details for this column TODO: Consider create special field for schema outside of metadata false tags []string Tags - can be any free text added to a component (no taxonomy) false","title":"Asset.spec.assetMetadata.componentsMetadata[key]"},{"location":"reference/crds.html#assetspecsecretref","text":"\u21a9 Parent Reference to a Secret resource holding credentials for this asset Name Type Description Required name string Name of the Secret resource (must exist in the same namespace) true","title":"Asset.spec.secretRef"},{"location":"reference/crds.html#motionm4dibmcomv1alpha1","text":"Resource Types: BatchTransfer StreamTransfer","title":"motion.m4d.ibm.com/v1alpha1"},{"location":"reference/crds.html#batchtransfer","text":"\u21a9 Parent BatchTransfer is the Schema for the batchtransfers API Name Type Description Required apiVersion string motion.m4d.ibm.com/v1alpha1 true kind string BatchTransfer true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object BatchTransferSpec defines the state of a BatchTransfer. The state includes source/destination specification, a schedule and the means by which data movement is to be conducted. The means is given as a kubernetes job description. In addition, the state also contains a sketch of a transformation instruction. In future releases, the transformation description should be specified in a separate CRD. false status object BatchTransferStatus defines the observed state of BatchTransfer This includes a reference to the job that implements the movement as well as the last schedule time. What is missing: Extended status information such as: - number of records moved - technical meta-data false","title":"BatchTransfer"},{"location":"reference/crds.html#batchtransferspec","text":"\u21a9 Parent BatchTransferSpec defines the state of a BatchTransfer. The state includes source/destination specification, a schedule and the means by which data movement is to be conducted. The means is given as a kubernetes job description. In addition, the state also contains a sketch of a transformation instruction. In future releases, the transformation description should be specified in a separate CRD. Name Type Description Required failedJobHistoryLimit integer Maximal number of failed Kubernetes job objects that should be kept. This property will be defaulted by the webhook if not set. false flowType enum Data flow type that specifies if this is a stream or a batch workflow [Batch Stream] false image string Image that should be used for the actual batch job. This is usually a datamover image. This property will be defaulted by the webhook if not set. false imagePullPolicy string Image pull policy that should be used for the actual job. This property will be defaulted by the webhook if not set. false maxFailedRetries integer Maximal number of failed retries until the batch job should stop trying. This property will be defaulted by the webhook if not set. false noFinalizer boolean If this batch job instance should have a finalizer or not. This property will be defaulted by the webhook if not set. false readDataType enum Data type of the data that is read from source (log data or change data) [LogData ChangeData] false schedule string Cron schedule if this BatchTransfer job should run on a regular schedule. Values are specified like cron job schedules. A good translation to human language can be found here https://crontab.guru/ false secretProviderRole string Secret provider role that should be used for the actual job. This property will be defaulted by the webhook if not set. false secretProviderURL string Secret provider url that should be used for the actual job. This property will be defaulted by the webhook if not set. false spark object Optional Spark configuration for tuning false successfulJobHistoryLimit integer Maximal number of successful Kubernetes job objects that should be kept. This property will be defaulted by the webhook if not set. false suspend boolean If this batch job instance is run on a schedule the regular schedule can be suspended with this property. This property will be defaulted by the webhook if not set. false transformation []object Transformations to be applied to the source data before writing to destination false writeDataType enum Data type of how the data should be written to the target (log data or change data) [LogData ChangeData] false writeOperation enum Write operation that should be performed when writing (overwrite,append,update) Caution: Some write operations are only available for batch and some only for stream. [Overwrite Append Update] false destination object Destination data store for this batch job true source object Source data store for this batch job true","title":"BatchTransfer.spec"},{"location":"reference/crds.html#batchtransferspecspark","text":"\u21a9 Parent Optional Spark configuration for tuning Name Type Description Required appName string Name of the transaction. Mainly used for debugging and lineage tracking. false driverCores integer Number of cores that the driver should use false driverMemory integer Memory that the driver should have false executorCores integer Number of cores that each executor should have false executorMemory string Memory that each executor should have false image string Image to be used for executors false imagePullPolicy string Image pull policy to be used for executor false numExecutors integer Number of executors to be started false options map[string]string Additional options for Spark configuration. false shufflePartitions integer Number of shuffle partitions for Spark false","title":"BatchTransfer.spec.spark"},{"location":"reference/crds.html#batchtransferspectransformationindex","text":"\u21a9 Parent to be refined... Name Type Description Required action enum Transformation action that should be performed. [RemoveColumns EncryptColumns DigestColumns RedactColumns SampleRows FilterRows] false columns []string Columns that are involved in this action. This property is optional as for some actions no columns have to be specified. E.g. filter is a row based transformation. false name string Name of the transaction. Mainly used for debugging and lineage tracking. false options map[string]string Additional options for this transformation. false","title":"BatchTransfer.spec.transformation[index]"},{"location":"reference/crds.html#batchtransferspecdestination","text":"\u21a9 Parent Destination data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false","title":"BatchTransfer.spec.destination"},{"location":"reference/crds.html#batchtransferspecdestinationcloudant","text":"\u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true","title":"BatchTransfer.spec.destination.cloudant"},{"location":"reference/crds.html#batchtransferspecdestinationcloudantvault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.destination.cloudant.vault"},{"location":"reference/crds.html#batchtransferspecdestinationdatabase","text":"\u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true","title":"BatchTransfer.spec.destination.database"},{"location":"reference/crds.html#batchtransferspecdestinationdatabasevault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.destination.database.vault"},{"location":"reference/crds.html#batchtransferspecdestinationkafka","text":"\u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true","title":"BatchTransfer.spec.destination.kafka"},{"location":"reference/crds.html#batchtransferspecdestinationkafkavault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.destination.kafka.vault"},{"location":"reference/crds.html#batchtransferspecdestinations3","text":"\u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true","title":"BatchTransfer.spec.destination.s3"},{"location":"reference/crds.html#batchtransferspecdestinations3vault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.destination.s3.vault"},{"location":"reference/crds.html#batchtransferspecsource","text":"\u21a9 Parent Source data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false","title":"BatchTransfer.spec.source"},{"location":"reference/crds.html#batchtransferspecsourcecloudant","text":"\u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true","title":"BatchTransfer.spec.source.cloudant"},{"location":"reference/crds.html#batchtransferspecsourcecloudantvault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.source.cloudant.vault"},{"location":"reference/crds.html#batchtransferspecsourcedatabase","text":"\u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true","title":"BatchTransfer.spec.source.database"},{"location":"reference/crds.html#batchtransferspecsourcedatabasevault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.source.database.vault"},{"location":"reference/crds.html#batchtransferspecsourcekafka","text":"\u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true","title":"BatchTransfer.spec.source.kafka"},{"location":"reference/crds.html#batchtransferspecsourcekafkavault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.source.kafka.vault"},{"location":"reference/crds.html#batchtransferspecsources3","text":"\u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true","title":"BatchTransfer.spec.source.s3"},{"location":"reference/crds.html#batchtransferspecsources3vault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"BatchTransfer.spec.source.s3.vault"},{"location":"reference/crds.html#batchtransferstatus","text":"\u21a9 Parent BatchTransferStatus defines the observed state of BatchTransfer This includes a reference to the job that implements the movement as well as the last schedule time. What is missing: Extended status information such as: - number of records moved - technical meta-data Name Type Description Required active object A pointer to the currently running job (or nil) false error string false lastCompleted object ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . false lastFailed object ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . false lastRecordTime string false lastScheduleTime string Information when was the last time the job was successfully scheduled. false lastSuccessTime string false numRecords integer false status enum [STARTING RUNNING SUCCEEDED FAILED] false","title":"BatchTransfer.status"},{"location":"reference/crds.html#batchtransferstatusactive","text":"\u21a9 Parent A pointer to the currently running job (or nil) Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false","title":"BatchTransfer.status.active"},{"location":"reference/crds.html#batchtransferstatuslastcompleted","text":"\u21a9 Parent ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false","title":"BatchTransfer.status.lastCompleted"},{"location":"reference/crds.html#batchtransferstatuslastfailed","text":"\u21a9 Parent ObjectReference contains enough information to let you inspect or modify the referred object. --- New uses of this type are discouraged because of difficulty describing its usage when embedded in APIs. 1. Ignored fields. It includes many fields which are not generally honored. For instance, ResourceVersion and FieldPath are both very rarely valid in actual usage. 2. Invalid usage help. It is impossible to add specific help for individual usage. In most embedded usages, there are particular restrictions like, \"must refer only to types A and B\" or \"UID not honored\" or \"name must be restricted\". Those cannot be well described when embedded. 3. Inconsistent validation. Because the usages are different, the validation rules are different by usage, which makes it hard for users to predict what will happen. 4. The fields are both imprecise and overly precise. Kind is not a precise mapping to a URL. This can produce ambiguity during interpretation and require a REST mapping. In most cases, the dependency is on the group,resource tuple and the version of the actual struct is irrelevant. 5. We cannot easily change it. Because this type is embedded in many locations, updates to this type will affect numerous schemas. Don't make new APIs embed an underspecified API type they do not control. Instead of using this type, create a locally provided and used type that is well-focused on your reference. For example, ServiceReferences for admission registration: https://github.com/kubernetes/api/blob/release-1.17/admissionregistration/v1/types.go#L533 . Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false","title":"BatchTransfer.status.lastFailed"},{"location":"reference/crds.html#streamtransfer","text":"\u21a9 Parent StreamTransfer is the Schema for the streamtransfers API Name Type Description Required apiVersion string motion.m4d.ibm.com/v1alpha1 true kind string StreamTransfer true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object StreamTransferSpec defines the desired state of StreamTransfer false status object StreamTransferStatus defines the observed state of StreamTransfer false","title":"StreamTransfer"},{"location":"reference/crds.html#streamtransferspec","text":"\u21a9 Parent StreamTransferSpec defines the desired state of StreamTransfer Name Type Description Required flowType enum Data flow type that specifies if this is a stream or a batch workflow [Batch Stream] false image string Image that should be used for the actual batch job. This is usually a datamover image. This property will be defaulted by the webhook if not set. false imagePullPolicy string Image pull policy that should be used for the actual job. This property will be defaulted by the webhook if not set. false noFinalizer boolean If this batch job instance should have a finalizer or not. This property will be defaulted by the webhook if not set. false readDataType enum Data type of the data that is read from source (log data or change data) [LogData ChangeData] false secretProviderRole string Secret provider role that should be used for the actual job. This property will be defaulted by the webhook if not set. false secretProviderURL string Secret provider url that should be used for the actual job. This property will be defaulted by the webhook if not set. false suspend boolean If this batch job instance is run on a schedule the regular schedule can be suspended with this property. This property will be defaulted by the webhook if not set. false transformation []object Transformations to be applied to the source data before writing to destination false triggerInterval string Interval in which the Micro batches of this stream should be triggered The default is '5 seconds'. false writeDataType enum Data type of how the data should be written to the target (log data or change data) [LogData ChangeData] false writeOperation enum Write operation that should be performed when writing (overwrite,append,update) Caution: Some write operations are only available for batch and some only for stream. [Overwrite Append Update] false destination object Destination data store for this batch job true source object Source data store for this batch job true","title":"StreamTransfer.spec"},{"location":"reference/crds.html#streamtransferspectransformationindex","text":"\u21a9 Parent to be refined... Name Type Description Required action enum Transformation action that should be performed. [RemoveColumns EncryptColumns DigestColumns RedactColumns SampleRows FilterRows] false columns []string Columns that are involved in this action. This property is optional as for some actions no columns have to be specified. E.g. filter is a row based transformation. false name string Name of the transaction. Mainly used for debugging and lineage tracking. false options map[string]string Additional options for this transformation. false","title":"StreamTransfer.spec.transformation[index]"},{"location":"reference/crds.html#streamtransferspecdestination","text":"\u21a9 Parent Destination data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false","title":"StreamTransfer.spec.destination"},{"location":"reference/crds.html#streamtransferspecdestinationcloudant","text":"\u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true","title":"StreamTransfer.spec.destination.cloudant"},{"location":"reference/crds.html#streamtransferspecdestinationcloudantvault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.destination.cloudant.vault"},{"location":"reference/crds.html#streamtransferspecdestinationdatabase","text":"\u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true","title":"StreamTransfer.spec.destination.database"},{"location":"reference/crds.html#streamtransferspecdestinationdatabasevault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.destination.database.vault"},{"location":"reference/crds.html#streamtransferspecdestinationkafka","text":"\u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true","title":"StreamTransfer.spec.destination.kafka"},{"location":"reference/crds.html#streamtransferspecdestinationkafkavault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.destination.kafka.vault"},{"location":"reference/crds.html#streamtransferspecdestinations3","text":"\u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true","title":"StreamTransfer.spec.destination.s3"},{"location":"reference/crds.html#streamtransferspecdestinations3vault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.destination.s3.vault"},{"location":"reference/crds.html#streamtransferspecsource","text":"\u21a9 Parent Source data store for this batch job Name Type Description Required cloudant object IBM Cloudant. Needs cloudant legacy credentials. false database object Database data store. For the moment only Db2 is supported. false description string Description of the transfer in human readable form that is displayed in the kubectl get If not provided this will be filled in depending on the datastore that is specified. false kafka object Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. false s3 object An object store data store that is compatible with S3. This can be a COS bucket. false","title":"StreamTransfer.spec.source"},{"location":"reference/crds.html#streamtransferspecsourcecloudant","text":"\u21a9 Parent IBM Cloudant. Needs cloudant legacy credentials. Name Type Description Required password string Cloudant password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false username string Cloudant user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false database string Database to be read from/written to true host string Host of cloudant instance true","title":"StreamTransfer.spec.source.cloudant"},{"location":"reference/crds.html#streamtransferspecsourcecloudantvault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.source.cloudant.vault"},{"location":"reference/crds.html#streamtransferspecsourcedatabase","text":"\u21a9 Parent Database data store. For the moment only Db2 is supported. Name Type Description Required password string Database password. Can be retrieved from vault if specified in vault parameter and is thus optional. false secretImport string Define a secret import definition. false user string Database user. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false db2URL string URL to Db2 instance in JDBC format Supported SSL certificates are currently certificates signed with IBM Intermediate CA or cloud signed certificates. true table string Table to be read true","title":"StreamTransfer.spec.source.database"},{"location":"reference/crds.html#streamtransferspecsourcedatabasevault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.source.database.vault"},{"location":"reference/crds.html#streamtransferspecsourcekafka","text":"\u21a9 Parent Kafka data store. The supposed format within the given Kafka topic is a Confluent compatible format stored as Avro. A schema registry needs to be specified as well. Name Type Description Required createSnapshot boolean If a snapshot should be created of the topic. Records in Kafka are stored as key-value pairs. Updates/Deletes for the same key are appended to the Kafka topic and the last value for a given key is the valid key in a Snapshot. When this property is true only the last value will be written. If the property is false all values will be written out. As a CDC example: If the property is true a valid snapshot of the log stream will be created. If the property is false the CDC stream will be dumped as is like a change log. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false keyDeserializer string Deserializer to be used for the keys of the topic false password string Kafka user password Can be retrieved from vault if specified in vault parameter and is thus optional. false saslMechanism string SASL Mechanism to be used (e.g. PLAIN or SCRAM-SHA-512) Default SCRAM-SHA-512 will be assumed if not specified false secretImport string Define a secret import definition. false securityProtocol string Kafka security protocol one of (PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, SSL) Default SASL_SSL will be assumed if not specified false sslTruststore string A truststore or certificate encoded as base64. The format can be JKS or PKCS12. A truststore can be specified like this or in a predefined Kubernetes secret false sslTruststoreLocation string SSL truststore location. false sslTruststorePassword string SSL truststore password. false sslTruststoreSecret string Kubernetes secret that contains the SSL truststore. The format can be JKS or PKCS12. A truststore can be specified like this or as false user string Kafka user name. Can be retrieved from vault if specified in vault parameter and is thus optional. false valueDeserializer string Deserializer to be used for the values of the topic false vault object Define secrets that are fetched from a Vault instance false kafkaBrokers string Kafka broker URLs as a comma separated list. true kafkaTopic string Kafka topic true schemaRegistryURL string URL to the schema registry. The registry has to be Confluent schema registry compatible. true","title":"StreamTransfer.spec.source.kafka"},{"location":"reference/crds.html#streamtransferspecsourcekafkavault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.source.kafka.vault"},{"location":"reference/crds.html#streamtransferspecsources3","text":"\u21a9 Parent An object store data store that is compatible with S3. This can be a COS bucket. Name Type Description Required accessKey string Access key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false dataFormat string Data format of the objects in S3. e.g. parquet or csv. Please refer to struct for allowed values. false partitionBy []string Partition by partition (for target data stores) Defines the columns to partition the output by for a target data store. false region string Region of S3 service false secretImport string Define a secret import definition. false secretKey string Secret key of the HMAC credentials that can access the given bucket. Can be retrieved from vault if specified in vault parameter and is thus optional. false vault object Define secrets that are fetched from a Vault instance false bucket string Bucket of S3 service true endpoint string Endpoint of S3 service true objectKey string Object key of the object in S3. This is used as a prefix! Thus all objects that have the given objectKey as prefix will be used as input! true","title":"StreamTransfer.spec.source.s3"},{"location":"reference/crds.html#streamtransferspecsources3vault","text":"\u21a9 Parent Define secrets that are fetched from a Vault instance Name Type Description Required address string Address is Vault address true authPath string AuthPath is the path to auth method i.e. kubernetes true role string Role is the Vault role used for retrieving the credentials true secretPath string SecretPath is the path of the secret holding the Credentials in Vault true","title":"StreamTransfer.spec.source.s3.vault"},{"location":"reference/crds.html#streamtransferstatus","text":"\u21a9 Parent StreamTransferStatus defines the observed state of StreamTransfer Name Type Description Required active object A pointer to the currently running job (or nil) false error string false status enum [STARTING RUNNING STOPPED FAILING] false","title":"StreamTransfer.status"},{"location":"reference/crds.html#streamtransferstatusactive","text":"\u21a9 Parent A pointer to the currently running job (or nil) Name Type Description Required apiVersion string API version of the referent. false fieldPath string If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: \"spec.containers{name}\" (where \"name\" refers to the name of the container that triggered the event) or if no container name is specified \"spec.containers[2]\" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future. false kind string Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false name string Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names false namespace string Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ false resourceVersion string Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency false uid string UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids false","title":"StreamTransfer.status.active"},{"location":"reference/ddc.html","text":"Data Distribution Controller Overview of Requirements and Functionality The Data Distribution Controller (DDC) handles the movement of data between data stores. Mesh for Data uses the DDC to perform an action called \"implicit copy\", i.e. the movement of a data set from one data store to another with possibly some unitary transform applied to that data set. It corresponds to Step 8 in the Architecture Document (Add a link here) Data can be copied from data store to data store in a large variety of different ways, depending on the types of the data store (e.g. COS, Relational DB) and nature of the data capture (Streamed, Snapshot). This document defines the functionality as well as the boundary conditions of the data distribution controller. Goals This document introduces fundamental concepts of the data distribution component and describes a high-level API for invoking data distributions. The initial focus is on structured (tabular) data. One goal of the data distribution component is to maximize congruence across different data stores and formats by preserving not only the data content but also the structure of the data as faithfully as possible. Fully unstructured data such (e.g. \"binary content\") will also be supported but that is not the focus of the initial version. Semi-structured data will be supported on a case-by-case basis. Non-Goals The focus is on how to invoke data distribution and not the if and when. This document doesn't describe the control component that is required to decide whether, when and how often data should be copied across storage systems. Neither does the data distribution perform any policy enforcement. This is done by the component that controls the data distribution system. High-Level Design Before providing an outline of the API functionality, some fundamental concepts are defined. Data Sets and Data Assets The following definition is aligned with the terminology used in the Watson Knowledge Catalog. Data is organized into data sets and data assets. A data set is a collection of data assets that is administered by a single body using a set of policies. Both data sets and data assets are uniquely identifiable. A data set is a collection of data assets with the same structure. Some examples: A data set is data that resides in a relational database where the database tables or views form the data asset. A data set consisting of objects that reside in a COS bucket where object prefix paths that have a common format are data assets. E.g. a set of partitioned parquet files with the same schema. A data set may be formed by a set of Kafka topics where each topic contains messages in compatible format. A data asset is represented by the content of the topic. The unit of data distribution is the data asset. Data Stores A data store allows access to data sets and data assets. Each store allows to individually access data through a data store specific API, e.g. S3 API or JDBC. Additional properties that are relevant for data distribution: Granularity of data access for reading: Some systems provide access to entire data assets only. (e.g. single unpartitioned files on COS). Other storage systems support queries to retrieve a sub-set (a selection and/or projection) of an individual data assets. (e.g. queries on Db2 or partitioned/bucketed prefixes on COS) Granularity of data access for writing: Fine-granular write access is required to apply delta-updates of individual data assets, i.e. update and insert ( upsert ) operations as well as deletes on record level are needed to process streams of changes. Systems that support fine-granular updates are relational database systems, elastic search indexes, and in-memory caches. Other systems such as traditional parquet files stored on COS or HDFS only allow data assets to be updated in their entirety. More sophisticated storage formats such as Delta Lake , Apache Iceberg or Hudi extend the capabilities of parquet. Fidelity of the type system: Data stores use various different typing systems and have different data models that require type conversions as data is distributed between these systems. For example, when moving the content of an elastic search index into a relational database we are moving between two entirely different data models. In order to minimize loss of information, type specific metadata (technical metadata) may need to be preserved as separate entities. In addition, schema inference might be needed to support certain data distributions. The invoker of the DDC is assumed to have knowledge of the technical metadata present at the source data asset and of the desired technical metadata of that data asset at the target. If the invoker does not specify this the DDC will attempt to infer it where possible. In both cases the source and target technical metadata are returned as part of the result of the data distribution. If the passed source or target technical metadata is inconsistent with the data asset at the source, then the data distribution fails. The version 1.0 of the DDC supports the following data stores: Db2 LUW v10.5 or newer Apache Kafka v2.2 or newer (Raw + Confluent KTable format serialized in JSON or Avro) IBM COS with Parquet, JSON and ORC (using a Stocator based approach) Transformations The data distribution supports simple transformations and filtering such as: - Filtering of individual rows or columns based on condition clauses. - Masking of specific columns. - Encrypting/hashing of specific columns. - Sampling of a subset of rows. This is specifically for creating a derived version of a specific data asset and is NOT to enrich or combine data assets, i.e. this is a not a general purpose computation environment. Data Life-cycle The DDC moves a data asset from a source to a target data store. The copy of the data asset will be retained at the target until explicitly removed by the invoker via the DDC API. API High-level Description The API follows the custom resource definition approach (CRD) for Kubernetes. The following basic CRD types exist: - BatchTransfer : One-time or periodic transfer of a single data asset from a source data store to a destination data store. This is also called snapshotting. This is similar to a job in K8s and will inherit many features from it, e.g. the state is kept in K8s after the batch transfer has completed and must be deleted manually. - SyncTransfer : Continuous synchronization of a single data asset from a source data store to a destination data store. The main use-case is to continuously update a destination data asset as it is typically used in a streaming or change-data-capture scenario. This CRD is similar to a stateful set in K8s. Both transfer types will have the same API concerning the core transfer definitions such as: - The source data store including connection details and data asset. - The path (in Vault) to the credentials required to access the source data store. - The destination data store including connection details and data asset. - The path (in Vault) to the credentials required to access the destination data store. - Transfer properties that define parameters such as schedule, retries, transformations etc. The difference is that SyncTransfer is running continuously, BatchTransfer requires a schedule or is a one-time transfer. Initially we will limit SyncTransfer to the movement of data from Kafka to COS or from Kafka to Db2. The status of the CRD is continuously updated with the state of the data distribution. It is used to detect both success or error situations as well as freshness. It also provides transfer statistics. Using the status of the CRD a user may examine: - where data assets have been moved - when this was last successfully completed (for _BatchTransfer_s) - statistics, i.e. how long this took, how many bytes, rows etc. were transferred - what technical metadata about the data was used at the source/destination Other K8s controllers can watch the objects and subscribe to statistics or technical metadata updates and forward these changes e.g. in dashboards or WKT. Secret management The data distribution API should not define any secrets in the CRD spec in a production environment. For development and testing direct definitions can be used but in a production environment credentials shall be retrieved from the secret provider. The secret provider can be accessed via a REST API using a role and a secret name. This secret name refers to a path in vault. At the movement operator shall not create any secrets in Kubernetes that contain any credentials and credentials shall only be maintained in memory. The fetching of secrets will be executed by the datamover component. The datamover component retrieves configuration from a JSON file that is passed on as a Kubernetes secret. The goal is that vault paths can be specified in this JSON configuration file and will be substituted by values retrieved from the secret provider. The following example illustrates this mechanism: Given the example configuration file: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"vaultPath\": \"/myvault/db2Password\" } and the following string in vault: {\"password\": \"mypassword\"} The substitution in the datamover will find a JSON field called vaultPath and look up the value using the secret provider. The substitution happens at the same level as the vaultPath field was found. This works whenever the data that is stored in vault is a JSON object itself. The advantage is that the in-memory configuration will be the same as in a dev/test environment after the substitution. The result of the given example after substitution will be: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"password\": \"mypassword\" } This credential substitution can also be used in the options field of transformations. Error handling The data distribution API is using validation hooks to do simple checks when a CRD is created or updated. This is a first kind of error that will result in an error when creating/updating the CRD. It will specify an error message about which fields are not valid. (e.g. an invalid cron pattern for the schedule property) As validation errors are checked before objects are created they return an error via the Kubernetes API. If an error occurred during a BatchTransfer the status of the CRD will be set to FAILED and a possible error reason will show in the error field. The error messages will differ depending on the type of exception that is thrown in the internal datamover process. The internal datamover process will communicate errors to Kubernetes via a termination message . The content of the termination message will be written into the error field of the BatchTransfer . The error message shall describe the error as good as possible without any stack traces to keep it readable and displayable in a short form. Actions for possible error states: * Pending - Nothing to do. Normal process * Running - Nothing to do. Normal process * Succeeded - Possibly execute on succeeded actions (e.g. updating a catalog, ...) * Failed - Operator will try to recover. * Fatal - Operator could not recover. Possibly recreate CRD to resolve and investigate error further. Events In addition to errors the datamover application that is called by the data distribution api will publish Kubernetes events for the CRD in order to give feedback for errors and successes. Errors will contain the error message. Successful messages will contain additional metrics such as number of transferred rows or technical metadata information. API Specification The formalism to use to describe this is to be decided, possibilities are Go using kubebuilder OR CRD directly. As the definition of transfer specific parameters is the same for BatchTransfer kind and SyncTransfer kind the definition below focusses on the BatchTransfer kind. (Think of it like a pod template definition that is the same for a job or a deployment) A possible but not complete list of Go structs using kubebuilder is: // BatchTransferSpec defines the desired state of BatchTransfer type BatchTransferSpec struct { Source DataStore `json:\"source\"` Destination DataStore `json:\"destination\"` Transformation []Transformation `json:\"transformation,omitempty\"` Schedule string `json:\"schedule,omitempty\"` Image string `json:\"image\"` // Has default value from webhook ImagePullPolicy corev1.PullPolicy `json:\"imagePullPolicy\"` // Has default value from webhook SecretProviderURL string `json:\"secretProviderURL\"` // Has default value from webhook SecretProviderRole string `json:\"secretProviderRole\"` // Has default value from webhook Suspend bool `json:\"suspend,omitempty\"` // Has default value from webhook MaxFailedRetries int `json:\"maxFailedRetries,omitempty\"` // Has default value from webhook SuccessfulJobHistoryLimit int `json:\"successfulJobHistoryLimit,omitempty\"` // Has default value from webhook FailedJobHistoryLimit int `json:\"failedJobHistoryLimit,omitempty\"` // Has default value from webhook } type DataStore struct { DataAsset string `json:\"dataAsset\"` Database *Database `json:\"database,omitempty\"` S3 *S3 `json:\"s3,omitempty\"` Kafka *Kafka `json:\"kafka,omitempty\"` } type Database struct { Db2URL string `json:\"db2URL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` } type S3 struct { Endpoint string `json:\"endpoint\"` Region string `json:\"region,omitempty\"` Bucket string `json:\"bucket\"` AccessKey *string `json:\"accessKey,omitempty\"` // Please use for dev/test only! SecretKey *string `json:\"secretKey,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` ObjectKey string `json:\"objectKey\"` DataFormat string `json:\"dataFormat,omitempty\"` } type Kafka struct { KafkaBrokers string `json:\"kafkaBrokers\"` SchemaRegistryURL string `json:\"schemaRegistryURL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` SslTruststoreLocation string `json:\"sslTruststoreLocation,omitempty\"` SslTruststorePassword string `json:\"sslTruststorePassword,omitempty\"` KafkaTopic string `json:\"kafkaTopic\"` CreateSnapshot bool `json:\"createSnapshot,omitempty\"` } type Transformation struct { Name string `json:\"name,omitempty\"` Action Action `json:\"action,omitempty\"` Columns []string `json:\"columns,omitempty\"` Options map[string]string `json:\"options,omitempty\"` } type Action string const ( RemoveColumn Action = \"RemoveColumn\" Filter Action = \"Filter\" Encrypt Action = \"Encrypt\" Sample Action = \"Sample\" Digest Action = \"Digest\" // md5, sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32 Redact Action = \"Redact\" // random, fixed, formatted, etc ) // BatchTransferStatus defines the observed state of BatchTransfer type BatchTransferStatus struct { Active *corev1.ObjectReference `json:\"active,omitempty\"` Status Status `json:\"status,omitempty\"` Error string `json:\"status,omitempty\"` LastCompleted *corev1.ObjectReference `json:\"lastCompleted,omitempty\"` LastFailed *corev1.ObjectReference `json:\"lastFailed,omitempty\"` LastSuccessTime *metav1.Time `json:\"lastSuccessTime,omitempty\"` LastRecordTime *metav1.Time `json:\"lastRecordTime,omitempty\"` NumRecords int64 `json:\"numRecords,omitempty\"` LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\"` } // +kubebuilder:validation:Enum=Pending;Running;Succeeded;Failed;Fatal;ConfigurationError type Status string const ( Pending Status = \"Pending\" // Starting up transfers Running Status = \"Running\" // Transfers are running Succeeded Status = \"Succeeded\" // Transfers succeeded Failed Status = \"Failed\" // Transfers failed (Maybe recoverable (e.g. temporary connection issues)) Fatal Status = \"Fatal\" // Fatal. Cannot recover. Manual intervention needed ) // +kubebuilder:object:root=true // BatchTransfer is the Schema for the batchtransfers API type BatchTransfer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec BatchTransferSpec `json:\"spec,omitempty\"` Status BatchTransferStatus `json:\"status,omitempty\"` } // +kubebuilder:object:root=true // BatchTransferList contains a list of BatchTransfer type BatchTransferList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Assets []BatchTransfer `json:\"assets\"` } Examples --- apiVersion: \"m4d.ibm.com/v1\" kind: BatchTransfer metadata: name: copy1 namespace: myNamespace spec: source: db: db2URL: \"jdbc:db2://db1.psvc-dev.zc2.ibm.com:50602/DHUBS:sslConnection=true;\" user: myuser password: \"mypassword\" destination: cos: endpoint: s3... bucket: myBucket accessKey: 0123 secretKey: 0123 transformation: - name: \"Remove column A\" action: \"RemoveColumn\" columns: [\"A\"] - name: \"Digest column B\" action: \"Digest\" columns: [\"B\"] options: algo: \"md5\" schedule: null # Cron schedule definition if needed maxFailedRetries: 3 # Maximum retries if failed suspend: false successfulJobsHistoryLimit: 2 failedJobsHistoryLimit: 5 status: lastCompleted: corev1.ObjectReference # Reference to child K8s objects lastScheduledTime: 2018-01-01T00:00:00Z lastSuccessTime: 2018-01-01T00:00:00Z lastRecordTime: 2018-01-01T00:00:00Z # inspect data? numRecords: 23113 External Dependencies Data distribution will be implemented in different ways, depending on the distribution kind, on the source and destination data store technologies as well as depending on the requested transformations. The control layer of the data distribution is implemented following the operator pattern of Kubernetes. In addition, the following technologies are relevant for specific distribution scenarios: - Redhat Debezium for Change Data Capture - IBM Event Streams (Apache Kafka) for SyncTransfer - Apache Spark - Db2 client - COS client - Reference to IBM Specific JDBC driver for streaming into a relation database. Relevant Code Repositories The data distribution core libraries that are Scala/Spark based The data distribution operator has been integrated into Mesh for Data code and is part of the manager. Roadmap Integration with Parquet Encryption + KeyProtect (As Target) Integration with Iceberg (As Target) Integration with Relational Databases (As Target) Integration with KTables (As Source)","title":"Data Distribution Controller"},{"location":"reference/ddc.html#data-distribution-controller","text":"","title":"Data Distribution Controller"},{"location":"reference/ddc.html#overview-of-requirements-and-functionality","text":"The Data Distribution Controller (DDC) handles the movement of data between data stores. Mesh for Data uses the DDC to perform an action called \"implicit copy\", i.e. the movement of a data set from one data store to another with possibly some unitary transform applied to that data set. It corresponds to Step 8 in the Architecture Document (Add a link here) Data can be copied from data store to data store in a large variety of different ways, depending on the types of the data store (e.g. COS, Relational DB) and nature of the data capture (Streamed, Snapshot). This document defines the functionality as well as the boundary conditions of the data distribution controller.","title":"Overview of Requirements and Functionality"},{"location":"reference/ddc.html#goals","text":"This document introduces fundamental concepts of the data distribution component and describes a high-level API for invoking data distributions. The initial focus is on structured (tabular) data. One goal of the data distribution component is to maximize congruence across different data stores and formats by preserving not only the data content but also the structure of the data as faithfully as possible. Fully unstructured data such (e.g. \"binary content\") will also be supported but that is not the focus of the initial version. Semi-structured data will be supported on a case-by-case basis.","title":"Goals"},{"location":"reference/ddc.html#non-goals","text":"The focus is on how to invoke data distribution and not the if and when. This document doesn't describe the control component that is required to decide whether, when and how often data should be copied across storage systems. Neither does the data distribution perform any policy enforcement. This is done by the component that controls the data distribution system.","title":"Non-Goals"},{"location":"reference/ddc.html#high-level-design","text":"Before providing an outline of the API functionality, some fundamental concepts are defined.","title":"High-Level Design"},{"location":"reference/ddc.html#data-sets-and-data-assets","text":"The following definition is aligned with the terminology used in the Watson Knowledge Catalog. Data is organized into data sets and data assets. A data set is a collection of data assets that is administered by a single body using a set of policies. Both data sets and data assets are uniquely identifiable. A data set is a collection of data assets with the same structure. Some examples: A data set is data that resides in a relational database where the database tables or views form the data asset. A data set consisting of objects that reside in a COS bucket where object prefix paths that have a common format are data assets. E.g. a set of partitioned parquet files with the same schema. A data set may be formed by a set of Kafka topics where each topic contains messages in compatible format. A data asset is represented by the content of the topic. The unit of data distribution is the data asset.","title":"Data Sets and Data Assets"},{"location":"reference/ddc.html#data-stores","text":"A data store allows access to data sets and data assets. Each store allows to individually access data through a data store specific API, e.g. S3 API or JDBC. Additional properties that are relevant for data distribution: Granularity of data access for reading: Some systems provide access to entire data assets only. (e.g. single unpartitioned files on COS). Other storage systems support queries to retrieve a sub-set (a selection and/or projection) of an individual data assets. (e.g. queries on Db2 or partitioned/bucketed prefixes on COS) Granularity of data access for writing: Fine-granular write access is required to apply delta-updates of individual data assets, i.e. update and insert ( upsert ) operations as well as deletes on record level are needed to process streams of changes. Systems that support fine-granular updates are relational database systems, elastic search indexes, and in-memory caches. Other systems such as traditional parquet files stored on COS or HDFS only allow data assets to be updated in their entirety. More sophisticated storage formats such as Delta Lake , Apache Iceberg or Hudi extend the capabilities of parquet. Fidelity of the type system: Data stores use various different typing systems and have different data models that require type conversions as data is distributed between these systems. For example, when moving the content of an elastic search index into a relational database we are moving between two entirely different data models. In order to minimize loss of information, type specific metadata (technical metadata) may need to be preserved as separate entities. In addition, schema inference might be needed to support certain data distributions. The invoker of the DDC is assumed to have knowledge of the technical metadata present at the source data asset and of the desired technical metadata of that data asset at the target. If the invoker does not specify this the DDC will attempt to infer it where possible. In both cases the source and target technical metadata are returned as part of the result of the data distribution. If the passed source or target technical metadata is inconsistent with the data asset at the source, then the data distribution fails. The version 1.0 of the DDC supports the following data stores: Db2 LUW v10.5 or newer Apache Kafka v2.2 or newer (Raw + Confluent KTable format serialized in JSON or Avro) IBM COS with Parquet, JSON and ORC (using a Stocator based approach)","title":"Data Stores"},{"location":"reference/ddc.html#transformations","text":"The data distribution supports simple transformations and filtering such as: - Filtering of individual rows or columns based on condition clauses. - Masking of specific columns. - Encrypting/hashing of specific columns. - Sampling of a subset of rows. This is specifically for creating a derived version of a specific data asset and is NOT to enrich or combine data assets, i.e. this is a not a general purpose computation environment.","title":"Transformations"},{"location":"reference/ddc.html#data-life-cycle","text":"The DDC moves a data asset from a source to a target data store. The copy of the data asset will be retained at the target until explicitly removed by the invoker via the DDC API.","title":"Data Life-cycle"},{"location":"reference/ddc.html#api-high-level-description","text":"The API follows the custom resource definition approach (CRD) for Kubernetes. The following basic CRD types exist: - BatchTransfer : One-time or periodic transfer of a single data asset from a source data store to a destination data store. This is also called snapshotting. This is similar to a job in K8s and will inherit many features from it, e.g. the state is kept in K8s after the batch transfer has completed and must be deleted manually. - SyncTransfer : Continuous synchronization of a single data asset from a source data store to a destination data store. The main use-case is to continuously update a destination data asset as it is typically used in a streaming or change-data-capture scenario. This CRD is similar to a stateful set in K8s. Both transfer types will have the same API concerning the core transfer definitions such as: - The source data store including connection details and data asset. - The path (in Vault) to the credentials required to access the source data store. - The destination data store including connection details and data asset. - The path (in Vault) to the credentials required to access the destination data store. - Transfer properties that define parameters such as schedule, retries, transformations etc. The difference is that SyncTransfer is running continuously, BatchTransfer requires a schedule or is a one-time transfer. Initially we will limit SyncTransfer to the movement of data from Kafka to COS or from Kafka to Db2. The status of the CRD is continuously updated with the state of the data distribution. It is used to detect both success or error situations as well as freshness. It also provides transfer statistics. Using the status of the CRD a user may examine: - where data assets have been moved - when this was last successfully completed (for _BatchTransfer_s) - statistics, i.e. how long this took, how many bytes, rows etc. were transferred - what technical metadata about the data was used at the source/destination Other K8s controllers can watch the objects and subscribe to statistics or technical metadata updates and forward these changes e.g. in dashboards or WKT.","title":"API High-level Description"},{"location":"reference/ddc.html#secret-management","text":"The data distribution API should not define any secrets in the CRD spec in a production environment. For development and testing direct definitions can be used but in a production environment credentials shall be retrieved from the secret provider. The secret provider can be accessed via a REST API using a role and a secret name. This secret name refers to a path in vault. At the movement operator shall not create any secrets in Kubernetes that contain any credentials and credentials shall only be maintained in memory. The fetching of secrets will be executed by the datamover component. The datamover component retrieves configuration from a JSON file that is passed on as a Kubernetes secret. The goal is that vault paths can be specified in this JSON configuration file and will be substituted by values retrieved from the secret provider. The following example illustrates this mechanism: Given the example configuration file: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"vaultPath\": \"/myvault/db2Password\" } and the following string in vault: {\"password\": \"mypassword\"} The substitution in the datamover will find a JSON field called vaultPath and look up the value using the secret provider. The substitution happens at the same level as the vaultPath field was found. This works whenever the data that is stored in vault is a JSON object itself. The advantage is that the in-memory configuration will be the same as in a dev/test environment after the substitution. The result of the given example after substitution will be: { \"db2URL\": \"jdbc:db2//host1:1234/MYDB\", \"user\": \"myuser\" \"password\": \"mypassword\" } This credential substitution can also be used in the options field of transformations.","title":"Secret management"},{"location":"reference/ddc.html#error-handling","text":"The data distribution API is using validation hooks to do simple checks when a CRD is created or updated. This is a first kind of error that will result in an error when creating/updating the CRD. It will specify an error message about which fields are not valid. (e.g. an invalid cron pattern for the schedule property) As validation errors are checked before objects are created they return an error via the Kubernetes API. If an error occurred during a BatchTransfer the status of the CRD will be set to FAILED and a possible error reason will show in the error field. The error messages will differ depending on the type of exception that is thrown in the internal datamover process. The internal datamover process will communicate errors to Kubernetes via a termination message . The content of the termination message will be written into the error field of the BatchTransfer . The error message shall describe the error as good as possible without any stack traces to keep it readable and displayable in a short form. Actions for possible error states: * Pending - Nothing to do. Normal process * Running - Nothing to do. Normal process * Succeeded - Possibly execute on succeeded actions (e.g. updating a catalog, ...) * Failed - Operator will try to recover. * Fatal - Operator could not recover. Possibly recreate CRD to resolve and investigate error further.","title":"Error handling"},{"location":"reference/ddc.html#events","text":"In addition to errors the datamover application that is called by the data distribution api will publish Kubernetes events for the CRD in order to give feedback for errors and successes. Errors will contain the error message. Successful messages will contain additional metrics such as number of transferred rows or technical metadata information.","title":"Events"},{"location":"reference/ddc.html#api-specification","text":"The formalism to use to describe this is to be decided, possibilities are Go using kubebuilder OR CRD directly. As the definition of transfer specific parameters is the same for BatchTransfer kind and SyncTransfer kind the definition below focusses on the BatchTransfer kind. (Think of it like a pod template definition that is the same for a job or a deployment) A possible but not complete list of Go structs using kubebuilder is: // BatchTransferSpec defines the desired state of BatchTransfer type BatchTransferSpec struct { Source DataStore `json:\"source\"` Destination DataStore `json:\"destination\"` Transformation []Transformation `json:\"transformation,omitempty\"` Schedule string `json:\"schedule,omitempty\"` Image string `json:\"image\"` // Has default value from webhook ImagePullPolicy corev1.PullPolicy `json:\"imagePullPolicy\"` // Has default value from webhook SecretProviderURL string `json:\"secretProviderURL\"` // Has default value from webhook SecretProviderRole string `json:\"secretProviderRole\"` // Has default value from webhook Suspend bool `json:\"suspend,omitempty\"` // Has default value from webhook MaxFailedRetries int `json:\"maxFailedRetries,omitempty\"` // Has default value from webhook SuccessfulJobHistoryLimit int `json:\"successfulJobHistoryLimit,omitempty\"` // Has default value from webhook FailedJobHistoryLimit int `json:\"failedJobHistoryLimit,omitempty\"` // Has default value from webhook } type DataStore struct { DataAsset string `json:\"dataAsset\"` Database *Database `json:\"database,omitempty\"` S3 *S3 `json:\"s3,omitempty\"` Kafka *Kafka `json:\"kafka,omitempty\"` } type Database struct { Db2URL string `json:\"db2URL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` } type S3 struct { Endpoint string `json:\"endpoint\"` Region string `json:\"region,omitempty\"` Bucket string `json:\"bucket\"` AccessKey *string `json:\"accessKey,omitempty\"` // Please use for dev/test only! SecretKey *string `json:\"secretKey,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` ObjectKey string `json:\"objectKey\"` DataFormat string `json:\"dataFormat,omitempty\"` } type Kafka struct { KafkaBrokers string `json:\"kafkaBrokers\"` SchemaRegistryURL string `json:\"schemaRegistryURL\"` User string `json:\"user\"` Password *string `json:\"password,omitempty\"` // Please use for dev/test only! VaultPath *string `json:\"vaultPath,omitempty\"` SslTruststoreLocation string `json:\"sslTruststoreLocation,omitempty\"` SslTruststorePassword string `json:\"sslTruststorePassword,omitempty\"` KafkaTopic string `json:\"kafkaTopic\"` CreateSnapshot bool `json:\"createSnapshot,omitempty\"` } type Transformation struct { Name string `json:\"name,omitempty\"` Action Action `json:\"action,omitempty\"` Columns []string `json:\"columns,omitempty\"` Options map[string]string `json:\"options,omitempty\"` } type Action string const ( RemoveColumn Action = \"RemoveColumn\" Filter Action = \"Filter\" Encrypt Action = \"Encrypt\" Sample Action = \"Sample\" Digest Action = \"Digest\" // md5, sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32 Redact Action = \"Redact\" // random, fixed, formatted, etc ) // BatchTransferStatus defines the observed state of BatchTransfer type BatchTransferStatus struct { Active *corev1.ObjectReference `json:\"active,omitempty\"` Status Status `json:\"status,omitempty\"` Error string `json:\"status,omitempty\"` LastCompleted *corev1.ObjectReference `json:\"lastCompleted,omitempty\"` LastFailed *corev1.ObjectReference `json:\"lastFailed,omitempty\"` LastSuccessTime *metav1.Time `json:\"lastSuccessTime,omitempty\"` LastRecordTime *metav1.Time `json:\"lastRecordTime,omitempty\"` NumRecords int64 `json:\"numRecords,omitempty\"` LastScheduleTime *metav1.Time `json:\"lastScheduleTime,omitempty\"` } // +kubebuilder:validation:Enum=Pending;Running;Succeeded;Failed;Fatal;ConfigurationError type Status string const ( Pending Status = \"Pending\" // Starting up transfers Running Status = \"Running\" // Transfers are running Succeeded Status = \"Succeeded\" // Transfers succeeded Failed Status = \"Failed\" // Transfers failed (Maybe recoverable (e.g. temporary connection issues)) Fatal Status = \"Fatal\" // Fatal. Cannot recover. Manual intervention needed ) // +kubebuilder:object:root=true // BatchTransfer is the Schema for the batchtransfers API type BatchTransfer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec BatchTransferSpec `json:\"spec,omitempty\"` Status BatchTransferStatus `json:\"status,omitempty\"` } // +kubebuilder:object:root=true // BatchTransferList contains a list of BatchTransfer type BatchTransferList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Assets []BatchTransfer `json:\"assets\"` }","title":"API Specification"},{"location":"reference/ddc.html#examples","text":"--- apiVersion: \"m4d.ibm.com/v1\" kind: BatchTransfer metadata: name: copy1 namespace: myNamespace spec: source: db: db2URL: \"jdbc:db2://db1.psvc-dev.zc2.ibm.com:50602/DHUBS:sslConnection=true;\" user: myuser password: \"mypassword\" destination: cos: endpoint: s3... bucket: myBucket accessKey: 0123 secretKey: 0123 transformation: - name: \"Remove column A\" action: \"RemoveColumn\" columns: [\"A\"] - name: \"Digest column B\" action: \"Digest\" columns: [\"B\"] options: algo: \"md5\" schedule: null # Cron schedule definition if needed maxFailedRetries: 3 # Maximum retries if failed suspend: false successfulJobsHistoryLimit: 2 failedJobsHistoryLimit: 5 status: lastCompleted: corev1.ObjectReference # Reference to child K8s objects lastScheduledTime: 2018-01-01T00:00:00Z lastSuccessTime: 2018-01-01T00:00:00Z lastRecordTime: 2018-01-01T00:00:00Z # inspect data? numRecords: 23113","title":"Examples"},{"location":"reference/ddc.html#external-dependencies","text":"Data distribution will be implemented in different ways, depending on the distribution kind, on the source and destination data store technologies as well as depending on the requested transformations. The control layer of the data distribution is implemented following the operator pattern of Kubernetes. In addition, the following technologies are relevant for specific distribution scenarios: - Redhat Debezium for Change Data Capture - IBM Event Streams (Apache Kafka) for SyncTransfer - Apache Spark - Db2 client - COS client - Reference to IBM Specific JDBC driver for streaming into a relation database.","title":"External Dependencies"},{"location":"reference/ddc.html#relevant-code-repositories","text":"The data distribution core libraries that are Scala/Spark based The data distribution operator has been integrated into Mesh for Data code and is part of the manager.","title":"Relevant Code Repositories"},{"location":"reference/ddc.html#roadmap","text":"Integration with Parquet Encryption + KeyProtect (As Target) Integration with Iceberg (As Target) Integration with Relational Databases (As Target) Integration with KTables (As Source)","title":"Roadmap"},{"location":"reference/katalog.html","text":"Katalog Katalog is a data catalog that is included in Mesh for Data for evaluation purposes. It is powered by Kubernetes resources: Asset CRD for managing data assets Secret resources for managing data access credentials Usage An Asset CRD includes a reference to a credentials Secret , connection information, and other metadata such as columns and associated security tags. Apply it like any other Kubernetes resource. Access credenditals are stored in Kubernetes Secret resources. You can use Basic authentication secrets or Opaque secrets with the following keys: Name Type Description Required access_key string Access key also known as AccessKeyId false secret_key string Secret key also known as SecretAccessKey false api_key string API key used in various IAM enabled services false password string Password for basic authentication false username string Username for basic authentication false Manage users Kubernetes RBAC is used for user management: To view Asset resources a Kubernetes user must be granted the katalog-viewer cluster role. To manage Asset resources a Kubernetes user must be granted the katalog-editor cluster role. As always, create a RoleBinding to grant these permissions to assets in a specific namespace and a ClusterRoleBinding to grant these premissions cluster wide.","title":"Katalog"},{"location":"reference/katalog.html#katalog","text":"Katalog is a data catalog that is included in Mesh for Data for evaluation purposes. It is powered by Kubernetes resources: Asset CRD for managing data assets Secret resources for managing data access credentials","title":"Katalog"},{"location":"reference/katalog.html#usage","text":"An Asset CRD includes a reference to a credentials Secret , connection information, and other metadata such as columns and associated security tags. Apply it like any other Kubernetes resource. Access credenditals are stored in Kubernetes Secret resources. You can use Basic authentication secrets or Opaque secrets with the following keys: Name Type Description Required access_key string Access key also known as AccessKeyId false secret_key string Secret key also known as SecretAccessKey false api_key string API key used in various IAM enabled services false password string Password for basic authentication false username string Username for basic authentication false","title":"Usage"},{"location":"reference/katalog.html#manage-users","text":"Kubernetes RBAC is used for user management: To view Asset resources a Kubernetes user must be granted the katalog-viewer cluster role. To manage Asset resources a Kubernetes user must be granted the katalog-editor cluster role. As always, create a RoleBinding to grant these permissions to assets in a specific namespace and a ClusterRoleBinding to grant these premissions cluster wide.","title":"Manage users"},{"location":"samples/notebook.html","text":"Notebook sample This sample shows how Mesh for Data enables a Jupyter notebook workload to access a dataset. It demonstrates how policies are seamlessly applied when accessing the dataset classified as financial data. In this sample you play multiple roles: As a data owner you upload a dataset and register it in a data catalog As a data steward you setup data governance policies As a data user you specify your data usage requirements and use a notebook to consume the data Before you begin Install Mesh for Data using the Quick Start guide. This sample assumes the use of the built-in catalog, Open Policy Agent (OPA) and flight module. A web browser. Create a namespace for the sample Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace m4d-notebook-sample kubectl config set-context --current --namespace = m4d-notebook-sample This enables easy cleanup once you're done experimenting with the sample. Prepare a dataset to be accessed by the notebook This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n m4d-notebook-sample --timeout = 120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } && aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } && aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH } Register the dataset in a data catalog Register the credentials required for accessing the dataset. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register the data asset itself in the catalog. Replace the values for endpoint , bucket and objectKey with values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : katalog.m4d.ibm.com/v1alpha1 kind : Asset metadata : name : paysim-csv spec : secretRef : name : paysim-csv assetDetails : dataFormat : csv connection : type : s3 s3 : endpoint : \"http://localstack.m4d-notebook-sample.svc.cluster.local:4566\" bucket : \"demo\" objectKey : \"PS_20174392719_1491204439457_log.csv\" assetMetadata : geography : theshire tags : - finance componentsMetadata : nameOrig : tags : - PII oldbalanceOrg : tags : - sensitive newbalanceOrig : tags : - sensitive EOF The asset is now registered in the catalog. The identifier of the asset is m4d-notebook-sample/paysim-csv (i.e. <namespace>/<name> ). You will use that name in the M4DApplication later. Notice the assetMetadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies. Define data access policies Define an OpenPolicyAgent policy to redact the nameOrig column for datasets tagged as finance . Below is the policy (written in Rego language): package dataapi.authz import data.data_policies as dp transform[action] { description := \"Redact sensitive columns in finance datasets\" dp.AccessType() == \"READ\" dp.dataset_has_tag(\"finance\") column_names := dp.column_with_any_name({\"nameOrig\"}) action = dp.build_redact_column_action(column_names[_], dp.build_policy_from_description(description)) } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n m4d-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n m4d-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n m4d-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files. Deploy a Jupyter notebook In this sample a Jupyter notebook is used as the user workload and its business logic requires reading the asset that we registered (e.g., for creating a fraud detection model). Deploy a notebook to your cluster: JupyterLab Deploy JupyterLab: kubectl create deployment my-notebook --image = jupyter/base-notebook --port = 8888 -- start.sh jupyter lab --LabApp.token = '' kubectl set env deployment my-notebook JUPYTER_ENABLE_LAB = yes kubectl label deployment my-notebook app.kubernetes.io/name = my-notebook kubectl wait --for = condition = available --timeout = 120s deployment/my-notebook kubectl expose deployment my-notebook --port = 80 --target-port = 8888 Create a port-forward to communicate with JupyterLab: kubectl port-forward svc/my-notebook 8080 :80 & Open your browser and go to http://localhost:8080/ . Create a new notebook in the server Kubeflow Ensure that Kubeflow is installed in your cluster Create a port-forward to communicate with Kubeflow: kubectl port-forward svc/istio-ingressgateway -n istio-system 8080 :80 & Open your browser and go to http://localhost:8080/ . Click Start Setup and then Finish (use the anonymous namespace). Click Notebook Servers (in the left). In the notebooks page select in the top left the anonymous namespace and then click New Server . In the notebook server creation page, set my-notebook in the Name box and then click Launch . Wait for the server to become ready. Click Connect and create a new notebook in the server. Create a M4DApplication resource for the notebook Create a M4DApplication resource to register the notebook workload to the control plane of Mesh for Data: cat <<EOF | kubectl apply -f - apiVersion : app.m4d.ibm.com/v1alpha1 kind : M4DApplication metadata : name : my-notebook labels : app : my-notebook spec : selector : workloadSelector : matchLabels : app : my-notebook appInfo : intent : fraud-detection data : - dataSetID : \"m4d-notebook-sample/paysim-csv\" requirements : interface : protocol : m4d-arrow-flight dataformat : arrow EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol and dataformat indicate that the developer wants to consume the data using Apache Arrow Flight. Run the following command to wait until the M4DApplication is ready: while [[ $( kubectl get m4dapplication my-notebook -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for M4DApplication\" && sleep 5 ; done Read the dataset from the notebook In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the M4DApplication resource: ENDPOINT_SCHEME = $( kubectl get m4dapplication my-notebook -o jsonpath ={ .status.readEndpointsMap.m4d-notebook-sample/paysim-csv.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get m4dapplication my-notebook -o jsonpath ={ .status.readEndpointsMap.m4d-notebook-sample/paysim-csv.hostname } ) ENDPOINT_PORT = $( kubectl get m4dapplication my-notebook -o jsonpath ={ .status.readEndpointsMap.m4d-notebook-sample/paysim-csv.port } ) printf \" ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \" The next steps use the endpoint to read the data in a python notebook Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow Insert a new notebook cell to read the data using the endpoint value extracted from the M4DApplication in the previous step: %pip install pandas pyarrow import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"m4d-notebook-sample/paysim-csv\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df Execute all notebook cells and notice that the nameOrig column appears redacted. Cleanup When you\u2019re finished experimenting with the notebook sample, clean it up: Stop kubectl port-forward processes (e.g., using pkill kubectl ) Delete the namespace created for this sample: kubectl delete namespace m4d-notebook-sample Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Notebook sample"},{"location":"samples/notebook.html#notebook-sample","text":"This sample shows how Mesh for Data enables a Jupyter notebook workload to access a dataset. It demonstrates how policies are seamlessly applied when accessing the dataset classified as financial data. In this sample you play multiple roles: As a data owner you upload a dataset and register it in a data catalog As a data steward you setup data governance policies As a data user you specify your data usage requirements and use a notebook to consume the data","title":"Notebook sample"},{"location":"samples/notebook.html#before-you-begin","text":"Install Mesh for Data using the Quick Start guide. This sample assumes the use of the built-in catalog, Open Policy Agent (OPA) and flight module. A web browser.","title":"Before you begin"},{"location":"samples/notebook.html#create-a-namespace-for-the-sample","text":"Create a new Kubernetes namespace and set it as the active namespace: kubectl create namespace m4d-notebook-sample kubectl config set-context --current --namespace = m4d-notebook-sample This enables easy cleanup once you're done experimenting with the sample.","title":"Create a namespace for the sample"},{"location":"samples/notebook.html#prepare-a-dataset-to-be-accessed-by-the-notebook","text":"This sample uses the Synthetic Financial Datasets For Fraud Detection dataset 1 as the data that the notebook needs to read. Download and extract the file to your machine. You should now see a file named PS_20174392719_1491204439457_log.csv . Alternatively, use a sample of 100 lines of the same dataset by downloading PS_20174392719_1491204439457_log.csv from GitHub. Upload the CSV file to an object storage of your choice such as AWS S3, IBM Cloud Object Storage or Ceph. Make a note of the service endpoint, bucket name, and access credentials. You will need them later. Setup and upload to localstack For experimentation you can install localstack to your cluster instead of using a cloud service. Define variables for access key and secret key export ACCESS_KEY = \"myaccesskey\" export SECRET_KEY = \"mysecretkey\" Install localstack to the currently active namespace and wait for it to be ready: helm repo add localstack-charts https://localstack.github.io/helm-charts helm install localstack localstack-charts/localstack --set startServices = \"s3\" --set service.type = ClusterIP kubectl wait --for = condition = ready --all pod -n m4d-notebook-sample --timeout = 120s Create a port-forward to communicate with localstack server: kubectl port-forward svc/localstack 4566 :4566 & Use AWS CLI to upload the dataset to a new created bucket in the localstack server: export ENDPOINT = \"http://127.0.0.1:4566\" export BUCKET = \"demo\" export OBJECT_KEY = \"PS_20174392719_1491204439457_log.csv\" export FILEPATH = \"/path/to/PS_20174392719_1491204439457_log.csv\" aws configure set aws_access_key_id ${ ACCESS_KEY } && aws configure set aws_secret_access_key ${ SECRET_KEY } && aws --endpoint-url = ${ ENDPOINT } s3api create-bucket --bucket ${ BUCKET } && aws --endpoint-url = ${ ENDPOINT } s3api put-object --bucket ${ BUCKET } --key ${ OBJECT_KEY } --body ${ FILEPATH }","title":"Prepare a dataset to be accessed by the notebook"},{"location":"samples/notebook.html#register-the-dataset-in-a-data-catalog","text":"Register the credentials required for accessing the dataset. Replace the values for access_key and secret_key with the values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : v1 kind : Secret metadata : name : paysim-csv type : Opaque stringData : access_key : \"${ACCESS_KEY}\" secret_key : \"${SECRET_KEY}\" EOF Then, register the data asset itself in the catalog. Replace the values for endpoint , bucket and objectKey with values from the object storage service that you used and run: cat << EOF | kubectl apply -f - apiVersion : katalog.m4d.ibm.com/v1alpha1 kind : Asset metadata : name : paysim-csv spec : secretRef : name : paysim-csv assetDetails : dataFormat : csv connection : type : s3 s3 : endpoint : \"http://localstack.m4d-notebook-sample.svc.cluster.local:4566\" bucket : \"demo\" objectKey : \"PS_20174392719_1491204439457_log.csv\" assetMetadata : geography : theshire tags : - finance componentsMetadata : nameOrig : tags : - PII oldbalanceOrg : tags : - sensitive newbalanceOrig : tags : - sensitive EOF The asset is now registered in the catalog. The identifier of the asset is m4d-notebook-sample/paysim-csv (i.e. <namespace>/<name> ). You will use that name in the M4DApplication later. Notice the assetMetadata field above. It specifies the dataset geography and tags. These attributes can later be used in policies.","title":"Register the dataset in a data catalog"},{"location":"samples/notebook.html#define-data-access-policies","text":"Define an OpenPolicyAgent policy to redact the nameOrig column for datasets tagged as finance . Below is the policy (written in Rego language): package dataapi.authz import data.data_policies as dp transform[action] { description := \"Redact sensitive columns in finance datasets\" dp.AccessType() == \"READ\" dp.dataset_has_tag(\"finance\") column_names := dp.column_with_any_name({\"nameOrig\"}) action = dp.build_redact_column_action(column_names[_], dp.build_policy_from_description(description)) } In this sample only the policy above is applied. Copy the policy to a file named sample-policy.rego and then run: kubectl -n m4d-system create configmap sample-policy --from-file = sample-policy.rego kubectl -n m4d-system label configmap sample-policy openpolicyagent.org/policy = rego while [[ $( kubectl get cm sample-policy -n m4d-system -o 'jsonpath={.metadata.annotations.openpolicyagent\\.org/policy-status}' ) ! = '{\"status\":\"ok\"}' ]] ; do echo \"waiting for policy to be applied\" && sleep 5 ; done You can similarly apply a directory holding multiple rego files.","title":"Define data access policies"},{"location":"samples/notebook.html#deploy-a-jupyter-notebook","text":"In this sample a Jupyter notebook is used as the user workload and its business logic requires reading the asset that we registered (e.g., for creating a fraud detection model). Deploy a notebook to your cluster: JupyterLab Deploy JupyterLab: kubectl create deployment my-notebook --image = jupyter/base-notebook --port = 8888 -- start.sh jupyter lab --LabApp.token = '' kubectl set env deployment my-notebook JUPYTER_ENABLE_LAB = yes kubectl label deployment my-notebook app.kubernetes.io/name = my-notebook kubectl wait --for = condition = available --timeout = 120s deployment/my-notebook kubectl expose deployment my-notebook --port = 80 --target-port = 8888 Create a port-forward to communicate with JupyterLab: kubectl port-forward svc/my-notebook 8080 :80 & Open your browser and go to http://localhost:8080/ . Create a new notebook in the server Kubeflow Ensure that Kubeflow is installed in your cluster Create a port-forward to communicate with Kubeflow: kubectl port-forward svc/istio-ingressgateway -n istio-system 8080 :80 & Open your browser and go to http://localhost:8080/ . Click Start Setup and then Finish (use the anonymous namespace). Click Notebook Servers (in the left). In the notebooks page select in the top left the anonymous namespace and then click New Server . In the notebook server creation page, set my-notebook in the Name box and then click Launch . Wait for the server to become ready. Click Connect and create a new notebook in the server.","title":"Deploy a Jupyter notebook"},{"location":"samples/notebook.html#create-a-m4dapplication-resource-for-the-notebook","text":"Create a M4DApplication resource to register the notebook workload to the control plane of Mesh for Data: cat <<EOF | kubectl apply -f - apiVersion : app.m4d.ibm.com/v1alpha1 kind : M4DApplication metadata : name : my-notebook labels : app : my-notebook spec : selector : workloadSelector : matchLabels : app : my-notebook appInfo : intent : fraud-detection data : - dataSetID : \"m4d-notebook-sample/paysim-csv\" requirements : interface : protocol : m4d-arrow-flight dataformat : arrow EOF Notice that: The selector field matches the labels of our Jupyter notebook workload. The data field includes a dataSetID that matches the asset identifier in the catalog. The protocol and dataformat indicate that the developer wants to consume the data using Apache Arrow Flight. Run the following command to wait until the M4DApplication is ready: while [[ $( kubectl get m4dapplication my-notebook -o 'jsonpath={.status.ready}' ) ! = \"true\" ]] ; do echo \"waiting for M4DApplication\" && sleep 5 ; done","title":"Create a M4DApplication resource for the notebook"},{"location":"samples/notebook.html#read-the-dataset-from-the-notebook","text":"In your terminal , run the following command to print the endpoint to use for reading the data. It fetches the code from the M4DApplication resource: ENDPOINT_SCHEME = $( kubectl get m4dapplication my-notebook -o jsonpath ={ .status.readEndpointsMap.m4d-notebook-sample/paysim-csv.scheme } ) ENDPOINT_HOSTNAME = $( kubectl get m4dapplication my-notebook -o jsonpath ={ .status.readEndpointsMap.m4d-notebook-sample/paysim-csv.hostname } ) ENDPOINT_PORT = $( kubectl get m4dapplication my-notebook -o jsonpath ={ .status.readEndpointsMap.m4d-notebook-sample/paysim-csv.port } ) printf \" ${ ENDPOINT_SCHEME } :// ${ ENDPOINT_HOSTNAME } : ${ ENDPOINT_PORT } \" The next steps use the endpoint to read the data in a python notebook Insert a new notebook cell to install pandas and pyarrow packages: % pip install pandas pyarrow Insert a new notebook cell to read the data using the endpoint value extracted from the M4DApplication in the previous step: %pip install pandas pyarrow import json import pyarrow.flight as fl import pandas as pd # Create a Flight client client = fl.connect ( '<ENDPOINT>' ) # Prepare the request request = { \"asset\" : \"m4d-notebook-sample/paysim-csv\" , # To request specific columns add to the request a \"columns\" key with a list of column names # \"columns\": [...] } # Send request and fetch result as a pandas DataFrame info = client.get_flight_info ( fl.FlightDescriptor.for_command ( json.dumps ( request ))) reader: fl.FlightStreamReader = client.do_get ( info.endpoints [ 0 ] .ticket ) df: pd.DataFrame = reader.read_pandas () Insert a new notebook cell with the following command to visualize the result: df Execute all notebook cells and notice that the nameOrig column appears redacted.","title":"Read the dataset from the notebook"},{"location":"samples/notebook.html#cleanup","text":"When you\u2019re finished experimenting with the notebook sample, clean it up: Stop kubectl port-forward processes (e.g., using pkill kubectl ) Delete the namespace created for this sample: kubectl delete namespace m4d-notebook-sample Created by NTNU and shared under the CC BY-SA 4.0 license. \u21a9","title":"Cleanup"},{"location":"tasks/control-plane-security.html","text":"Enable Control Plane Security Kubernetes NetworkPolicies and optionally Istio are used to protect components of the control plane. Specifically, traffic to connectors that run as part of the control plane must be secured. Follow this page to enable control plane security. Ingress traffic policy The installation of Mesh for Data applies a Kubernetes NetworkPolicy resource to the m4d-system namespace. This resource ensures that ingress traffic to connectors is only allowed from workloads that run in the m4d-system namespace and thus disallow access to connectors from other namespaces or external parties. The NetworkPolicy is always created. However, your Kubernetes cluster must have a Network Plugin with NetworkPolicy support. Otherwise, NetworkPolicy resources will have no affect. While most Kubernetes distributions include a network plugin that enfoces network policies, some like Kind do not and require you to install a separate network plugin instead. Mutual TLS If Istio is installed in the cluster then you can use automatic mutual TLS to encrypt the traffic to the connectors. Follow these steps to enable mutual TLS: - Ensure that Istio 1.6 or above is installed. - Enable Istio sidecar injection in the m4d-system namespace: kubectl label namespace m4d-system istio-injection = enabled - Create Istio PeerAuthentication resource to enable mutual TLS between contains with Istio sidecars: cat << EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"premissive-mtls-in-control-plane\" namespace: m4d-system spec: mtls: mode: PERMISSIVE EOF - Create Istio Sidecar resource to allow any egress traffic from the control plane containers: cat << EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: sidecar-default namespace: m4d-system spec: egress: - hosts: - \"*/*\" outboundTrafficPolicy: mode: ALLOW_ANY EOF - Restart the control plane pods: kubectl delete pod --all -n m4d-system","title":"Enable Control Plane Security"},{"location":"tasks/control-plane-security.html#enable-control-plane-security","text":"Kubernetes NetworkPolicies and optionally Istio are used to protect components of the control plane. Specifically, traffic to connectors that run as part of the control plane must be secured. Follow this page to enable control plane security.","title":"Enable Control Plane Security"},{"location":"tasks/control-plane-security.html#ingress-traffic-policy","text":"The installation of Mesh for Data applies a Kubernetes NetworkPolicy resource to the m4d-system namespace. This resource ensures that ingress traffic to connectors is only allowed from workloads that run in the m4d-system namespace and thus disallow access to connectors from other namespaces or external parties. The NetworkPolicy is always created. However, your Kubernetes cluster must have a Network Plugin with NetworkPolicy support. Otherwise, NetworkPolicy resources will have no affect. While most Kubernetes distributions include a network plugin that enfoces network policies, some like Kind do not and require you to install a separate network plugin instead.","title":"Ingress traffic policy"},{"location":"tasks/control-plane-security.html#mutual-tls","text":"If Istio is installed in the cluster then you can use automatic mutual TLS to encrypt the traffic to the connectors. Follow these steps to enable mutual TLS: - Ensure that Istio 1.6 or above is installed. - Enable Istio sidecar injection in the m4d-system namespace: kubectl label namespace m4d-system istio-injection = enabled - Create Istio PeerAuthentication resource to enable mutual TLS between contains with Istio sidecars: cat << EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"premissive-mtls-in-control-plane\" namespace: m4d-system spec: mtls: mode: PERMISSIVE EOF - Create Istio Sidecar resource to allow any egress traffic from the control plane containers: cat << EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: sidecar-default namespace: m4d-system spec: egress: - hosts: - \"*/*\" outboundTrafficPolicy: mode: ALLOW_ANY EOF - Restart the control plane pods: kubectl delete pod --all -n m4d-system","title":"Mutual TLS"},{"location":"tasks/multicluster.html","text":"Multicluster setup Mesh for data is dynamic in its multi cluster capabilities in that it has abstractions to support multiple different cross-cluster orchestration mechanisms. Currently only one multi cluster orchestration mechanism is implemented and is using Razee for the orchestration. Multicluster operation with Razee Razee is a multi-cluster continuous delivery tool for Kubernetes that can deploy software on remote clusters and track the deployment status of such deployments. There are multiple ways to run Razee. The two described here are a vanilla open source deployment on your own Kubernetes or as a managed service from a cloud provider. Due to the complex nature of installing Razee a managed service from a cloud provider is recommended. It's possible to define a multicluster group name that groups clusters that are used in a mesh instance. This will restrict the clusters that are usable in the mesh instance to the ones that are registered in the specified Razee group. This is especially helpful if Razee is also used for different purposes than Mesh for data or multiple mesh instances should be used under the same Razee installation. In general there is a need for the following Razee components to be installed: Razee watch keeper (installed on all clusters) Razee cluster subscription manager (installed on all clusters) RazeeDash API (installed on coordinator cluster/as cloud service) Both methods below describe how the above components can be installed depending on what RazeeDash deployment method is used. Installing Razee on Kubernetes Coordinator cluster An installation of the open source components is described here . Please follow the instructions in the Razee documentation to install RazeeDash , Watch keeper and the cluster subscription agent . At the moment Razee supports GitHub, GitHub Enterprise and BitBucket for the OAUTH Authentication of this installation. Please be aware that the RazeeDash API needs to be reachable from all clusters. Thus there may be the need for routes, ingresses or node ports in order to expose it to other networks and clusters. Once RazeeDash is installed the UI can be used to group registered clusters in a multicluster group that can be configured below. The API Key can also be retrieved from the UI following these two steps. From the RazeeDash console, click the arrow icon in the upper right corner. Then, select Profile. Copy the API key value. If no API key exists, click Generate to generate one. In order to configure Mesh for data to use the installed Razee on Kubernetes the values of the helm charts have to be adapted to the following: coordinator: # URL for Razee deployment url: \"https://your-razee-service:3333/graphql\" # Razee deployment with oauth API key authentication requires the apiKey parameter apiKey: \"<your Razee X_API_KEY>\" multiclusterGroup: \"<your group name>\" Remote cluster The remote clusters only need the watch keeper and cluster subscription agents installed. The remote clusters do not need the coordinator component of mesh for data. It's enough to follow this guide to install the agents and configure a group via the RazeeDash UI if needed. The coordinator configuration would look like the following: coordinator: enabled: false Installing using IBM Satellite Config When using IBM Satellite Config the RazeeDash API is running as a service in the cloud and all CRD distribution is handled by the cloud. The process here describes how an already existing Kubernetes cluster can be registered and configured. Prerequisites: An IBM Cloud Account IBM Cloud Satellite service IAM API Keys with access to IBM Cloud Satellite service The step below has to be executed for each cluster that should be added to the Mesh for data instance. This step is the same for coordinator and remote clusters. In the IBM Satellite Cloud service under the Clusters tab click on Attach cluster . Enter a cluster name in the popup dialog and click Register cluster . (Please don't use spaces in the name) The next dialog will offer you a kubectl command that can be executed on the cluster that should be attached. After executing the kubectl command the Razee services will be installed in the razeedeploy namespace and the cluster will show up in your cluster list (like in the picture above). This installs the watch keeper and cluster subscription components. The next step is to configure Mesh for data to use IBM Satellite config as multicluster orchestrator. This configuration is done via a Kubernetes secret that is created by the helm chart. Overwriting the coordinator.razee values in your deployment will make use of the multicluster tooling. A configuration using IBM Satellite Config would look like the following for the coordinator cluster: coordinator: # Configures the Razee instance to be used by the coordinator manager in a multicluster setup razee: # IBM Cloud IAM API Key of a user or service account that have access to IBM Cloud Satellite Config iamKey: \"<your IAM API KEY key>\" multiclusterGroup: \"<your group name>\" For the remote cluster the coordinator will be disabled: coordinator: enabled: false","title":"Multicluster setup"},{"location":"tasks/multicluster.html#multicluster-setup","text":"Mesh for data is dynamic in its multi cluster capabilities in that it has abstractions to support multiple different cross-cluster orchestration mechanisms. Currently only one multi cluster orchestration mechanism is implemented and is using Razee for the orchestration.","title":"Multicluster setup"},{"location":"tasks/multicluster.html#multicluster-operation-with-razee","text":"Razee is a multi-cluster continuous delivery tool for Kubernetes that can deploy software on remote clusters and track the deployment status of such deployments. There are multiple ways to run Razee. The two described here are a vanilla open source deployment on your own Kubernetes or as a managed service from a cloud provider. Due to the complex nature of installing Razee a managed service from a cloud provider is recommended. It's possible to define a multicluster group name that groups clusters that are used in a mesh instance. This will restrict the clusters that are usable in the mesh instance to the ones that are registered in the specified Razee group. This is especially helpful if Razee is also used for different purposes than Mesh for data or multiple mesh instances should be used under the same Razee installation. In general there is a need for the following Razee components to be installed: Razee watch keeper (installed on all clusters) Razee cluster subscription manager (installed on all clusters) RazeeDash API (installed on coordinator cluster/as cloud service) Both methods below describe how the above components can be installed depending on what RazeeDash deployment method is used.","title":"Multicluster operation with Razee"},{"location":"tasks/multicluster.html#installing-razee-on-kubernetes","text":"","title":"Installing Razee on Kubernetes"},{"location":"tasks/multicluster.html#coordinator-cluster","text":"An installation of the open source components is described here . Please follow the instructions in the Razee documentation to install RazeeDash , Watch keeper and the cluster subscription agent . At the moment Razee supports GitHub, GitHub Enterprise and BitBucket for the OAUTH Authentication of this installation. Please be aware that the RazeeDash API needs to be reachable from all clusters. Thus there may be the need for routes, ingresses or node ports in order to expose it to other networks and clusters. Once RazeeDash is installed the UI can be used to group registered clusters in a multicluster group that can be configured below. The API Key can also be retrieved from the UI following these two steps. From the RazeeDash console, click the arrow icon in the upper right corner. Then, select Profile. Copy the API key value. If no API key exists, click Generate to generate one. In order to configure Mesh for data to use the installed Razee on Kubernetes the values of the helm charts have to be adapted to the following: coordinator: # URL for Razee deployment url: \"https://your-razee-service:3333/graphql\" # Razee deployment with oauth API key authentication requires the apiKey parameter apiKey: \"<your Razee X_API_KEY>\" multiclusterGroup: \"<your group name>\"","title":"Coordinator cluster"},{"location":"tasks/multicluster.html#remote-cluster","text":"The remote clusters only need the watch keeper and cluster subscription agents installed. The remote clusters do not need the coordinator component of mesh for data. It's enough to follow this guide to install the agents and configure a group via the RazeeDash UI if needed. The coordinator configuration would look like the following: coordinator: enabled: false","title":"Remote cluster"},{"location":"tasks/multicluster.html#installing-using-ibm-satellite-config","text":"When using IBM Satellite Config the RazeeDash API is running as a service in the cloud and all CRD distribution is handled by the cloud. The process here describes how an already existing Kubernetes cluster can be registered and configured. Prerequisites: An IBM Cloud Account IBM Cloud Satellite service IAM API Keys with access to IBM Cloud Satellite service The step below has to be executed for each cluster that should be added to the Mesh for data instance. This step is the same for coordinator and remote clusters. In the IBM Satellite Cloud service under the Clusters tab click on Attach cluster . Enter a cluster name in the popup dialog and click Register cluster . (Please don't use spaces in the name) The next dialog will offer you a kubectl command that can be executed on the cluster that should be attached. After executing the kubectl command the Razee services will be installed in the razeedeploy namespace and the cluster will show up in your cluster list (like in the picture above). This installs the watch keeper and cluster subscription components. The next step is to configure Mesh for data to use IBM Satellite config as multicluster orchestrator. This configuration is done via a Kubernetes secret that is created by the helm chart. Overwriting the coordinator.razee values in your deployment will make use of the multicluster tooling. A configuration using IBM Satellite Config would look like the following for the coordinator cluster: coordinator: # Configures the Razee instance to be used by the coordinator manager in a multicluster setup razee: # IBM Cloud IAM API Key of a user or service account that have access to IBM Cloud Satellite Config iamKey: \"<your IAM API KEY key>\" multiclusterGroup: \"<your group name>\" For the remote cluster the coordinator will be disabled: coordinator: enabled: false","title":"Installing using IBM Satellite Config"},{"location":"tasks/using-opa.html","text":"Using OPA There are several ways to manage policies and data of the OPA service. One simple approach is to use OPA kube-mgmt and manage Rego policies in Kubernetes Configmap resources. By default Mesh for Data installs OPA with kube-mgmt enabled. This task shows how to use OPA with kube-mgmt. Warning Due to size limits you must ensure that each configmap is smaller than 1MB when base64 encoded. Using a configmap YAML Create a configmap with a Rego policy and a openpolicyagent.org/policy=rego label in the m4d-system namespace: apiVersion : v1 kind : ConfigMap metadata : name : <policy-name> namespace : m4d-system labels : openpolicyagent.org/policy : rego data : main : | <you rego policy here> Apply the configmap: kubectl apply -f <policy-name>.yaml To remove the policy just remove the configmap: kubectl delete -f <policy-name>.yaml Using a Rego file You can use kubectl to create a configmap from a Rego file. To create a configmap named <policy-name> from a Rego file in path <policy-name.rego> : kubectl create configmap <policy-name> --from-file = main = <policy-name.rego> -n m4d-system kubectl label configmap <policy-name> openpolicyagent.org/policy = rego -n m4d-system Delete the policy with kubectl delete configmap <policy-name> -n m4d-system .","title":"Using OPA"},{"location":"tasks/using-opa.html#using-opa","text":"There are several ways to manage policies and data of the OPA service. One simple approach is to use OPA kube-mgmt and manage Rego policies in Kubernetes Configmap resources. By default Mesh for Data installs OPA with kube-mgmt enabled. This task shows how to use OPA with kube-mgmt. Warning Due to size limits you must ensure that each configmap is smaller than 1MB when base64 encoded.","title":"Using OPA"},{"location":"tasks/using-opa.html#using-a-configmap-yaml","text":"Create a configmap with a Rego policy and a openpolicyagent.org/policy=rego label in the m4d-system namespace: apiVersion : v1 kind : ConfigMap metadata : name : <policy-name> namespace : m4d-system labels : openpolicyagent.org/policy : rego data : main : | <you rego policy here> Apply the configmap: kubectl apply -f <policy-name>.yaml To remove the policy just remove the configmap: kubectl delete -f <policy-name>.yaml","title":"Using a configmap YAML"},{"location":"tasks/using-opa.html#using-a-rego-file","text":"You can use kubectl to create a configmap from a Rego file. To create a configmap named <policy-name> from a Rego file in path <policy-name.rego> : kubectl create configmap <policy-name> --from-file = main = <policy-name.rego> -n m4d-system kubectl label configmap <policy-name> openpolicyagent.org/policy = rego -n m4d-system Delete the policy with kubectl delete configmap <policy-name> -n m4d-system .","title":"Using a Rego file"}]}